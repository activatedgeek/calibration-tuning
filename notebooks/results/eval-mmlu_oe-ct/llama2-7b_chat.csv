N,acc,unc_acc,unc_auroc,unc_ece,dataset,split,ts
12,0.1666666716337204,0.4166666865348816,0.55,0.45301934083302814,mmlu:econometrics,validation,21.38984616473317
114,0.11403508484363556,0.6754385828971863,0.6111957349581112,0.04900697762506049,mmlu:econometrics,test,268.56325329840183
16,0.375,0.625,0.5416666666666666,0.1851002648472786,mmlu:astronomy,validation,28.250134881585836
152,0.45394736528396606,0.6578947305679321,0.5739479657761482,0.09800243730607787,mmlu:astronomy,test,197.05719001963735
11,0.7272727489471436,0.7272727489471436,0.625,0.12152998555790294,mmlu:medical_genetics,validation,32.029131351038814
100,0.3799999952316284,0.699999988079071,0.48832767402376914,0.04639107346534731,mmlu:medical_genetics,test,253.67108075134456
9,0.3333333432674408,0.7777777910232544,0.3888888888888889,0.2767202059427897,mmlu:high_school_computer_science,validation,15.385651465505362
100,0.35999998450279236,0.6299999952316284,0.3882378472222222,0.10568157255649568,mmlu:high_school_computer_science,test,196.40344766154885
43,0.3720930218696594,0.6511628031730652,0.42476851851851855,0.14425334542296653,mmlu:high_school_macroeconomics,validation,101.1524935811758
390,0.30512821674346924,0.7384615540504456,0.37571707649849606,0.02897380070808607,mmlu:high_school_macroeconomics,test,1006.3235904499888
86,0.41860464215278625,0.7325581312179565,0.4638888888888889,0.0532786541206892,mmlu:miscellaneous,validation,89.38086989335716
783,0.449552983045578,0.7305235862731934,0.5058630826829782,0.0345044948284349,mmlu:miscellaneous,test,942.0023628044873
21,0.523809552192688,0.5714285969734192,0.3136363636363636,0.1914678102447873,mmlu:high_school_government_and_politics,validation,53.51048784516752
193,0.559585452079773,0.6891191601753235,0.5027777777777778,0.045768515124839836,mmlu:high_school_government_and_politics,test,464.52869424410164
19,0.5789473652839661,0.6315789222717285,0.25,0.22806317241568314,mmlu:world_religions,validation,17.466644888743758
171,0.5438596606254578,0.5964912176132202,0.30321202095395644,0.13579386960693274,mmlu:world_religions,test,152.1409404873848
11,0.5454545617103577,0.5454545617103577,0.5833333333333333,0.21090544895692306,mmlu:computer_security,validation,16.711834758520126
100,0.44999998807907104,0.6200000047683716,0.5135353535353536,0.08561345815658572,mmlu:computer_security,test,136.2615403626114
14,0.2142857313156128,0.8571429252624512,0.33333333333333326,0.18710691162518092,mmlu:anatomy,validation,20.371192902326584
135,0.3629629611968994,0.6814814805984497,0.4501661129568106,0.08955153535913536,mmlu:anatomy,test,186.44634624943137
38,0.34210526943206787,0.7105263471603394,0.3846153846153846,0.07076481612105119,mmlu:moral_disputes,validation,72.72384012490511
346,0.45375722646713257,0.6358381509780884,0.4004987699255215,0.09503238187359933,mmlu:moral_disputes,test,609.1357943657786
11,0.0,0.9090909361839294,,0.07802612673152576,mmlu:college_mathematics,validation,11.35549926571548
100,0.22999998927116394,0.7699999809265137,0.4257481648785996,0.07968297779560088,mmlu:college_mathematics,test,86.53806010447443
31,0.22580644488334656,0.6774193048477173,0.47916666666666663,0.0893637595638152,mmlu:professional_accounting,validation,28.800566704943776
282,0.1489361673593521,0.7730495929718018,0.4903769841269841,0.05558847281949741,mmlu:professional_accounting,test,261.6188397705555
11,0.3636363744735718,0.7272727489471436,0.6071428571428572,0.0970548608086326,mmlu:business_ethics,validation,15.122260373085737
100,0.29999998211860657,0.7199999690055847,0.4726190476190477,0.06576776325702668,mmlu:business_ethics,test,130.9072180390358
11,0.5454545617103577,0.7272727489471436,0.5833333333333334,0.17787606607783923,mmlu:us_foreign_policy,validation,20.243870195001364
100,0.5899999737739563,0.6899999976158142,0.4565936337329475,0.12636022448539735,mmlu:us_foreign_policy,test,150.53623603656888
10,0.30000001192092896,0.699999988079071,0.5,0.11765542626380923,mmlu:global_facts,validation,18.746042439714074
100,0.08999999612569809,0.8899999856948853,0.63003663003663,0.17710947513580325,mmlu:global_facts,test,149.44090394303203
11,0.1818181872367859,1.0,0.6111111111111112,0.2737390236421065,mmlu:management,validation,27.260607458651066
103,0.3300970792770386,0.7961165308952332,0.3062659846547314,0.06111578570986258,mmlu:management,test,180.85059466585517
12,0.1666666716337204,0.9166666865348816,0.4,0.23661939799785614,mmlu:human_sexuality,validation,29.41334241628647
131,0.4122137427330017,0.6870229244232178,0.5149110149110149,0.06959608219962085,mmlu:human_sexuality,test,313.95166852511466
18,0.5555555820465088,0.6666666865348816,0.575,0.19921525319417316,mmlu:logical_fallacies,validation,34.75430995412171
163,0.39263802766799927,0.6932514905929565,0.5641571969696969,0.04627957738981658,mmlu:logical_fallacies,test,271.8125779759139
22,0.09090909361839294,0.8636363744735718,0.7374999999999999,0.1588493531400507,mmlu:high_school_chemistry,validation,26.435720236971974
203,0.13300491869449615,0.842364490032196,0.373526936026936,0.06366543112129999,mmlu:high_school_chemistry,test,231.26674285903573
14,0.4285714626312256,0.5714285969734192,0.5833333333333333,0.18178787401744298,mmlu:formal_logic,validation,32.77524055540562
126,0.2142857313156128,0.7380952835083008,0.5415263748597081,0.10672835272455977,mmlu:formal_logic,test,216.60409750044346
17,0.05882352963089943,0.529411792755127,0.375,0.1301072520368239,mmlu:high_school_physics,validation,19.013091589789838
151,0.1589404046535492,0.6887417435646057,0.4703083989501313,0.030357045448379007,mmlu:high_school_physics,test,167.07444862183183
18,0.7222222089767456,0.7222222089767456,0.7000000000000001,0.08907092942131886,mmlu:high_school_european_history,validation,52.30279732495546
165,0.7575757503509521,0.8121212124824524,0.7163999999999998,0.03044948939121133,mmlu:high_school_european_history,test,415.027373066172
23,0.260869562625885,0.739130437374115,0.3088235294117647,0.12671244403590326,mmlu:high_school_statistics,validation,26.3517072587274
216,0.20370370149612427,0.7083333134651184,0.47251585623678644,0.02155313558048673,mmlu:high_school_statistics,test,259.01916957460344
29,0.13793103396892548,0.8275861740112305,0.18500000000000003,0.10664442078820592,mmlu:clinical_knowledge,validation,32.74376333504915
265,0.28679245710372925,0.7622641324996948,0.4546087440824283,0.05643405217044758,mmlu:clinical_knowledge,test,314.7247554231435
35,0.2857142984867096,0.800000011920929,0.54,0.14442315442221504,mmlu:prehistory,validation,110.66231408342719
324,0.354938268661499,0.7191358208656311,0.46238818389848135,0.04356972431695018,mmlu:prehistory,test,998.0711839944124
69,0.37681159377098083,0.6376811861991882,0.39177101967799643,0.12371616864549939,mmlu:professional_psychology,validation,97.46770653873682
612,0.30882352590560913,0.6797385811805725,0.42360563873566237,0.035410569871173185,mmlu:professional_psychology,test,848.2610145229846
33,0.3636363744735718,0.696969747543335,0.44642857142857145,0.10882525191162572,mmlu:nutrition,validation,46.52109838090837
306,0.42810457944869995,0.673202633857727,0.37810250817884405,0.08212843673681121,mmlu:nutrition,test,442.38968177326024
26,0.7307692766189575,0.6538462042808533,0.6654135338345865,0.11512740987997788,mmlu:high_school_world_history,validation,39.181835058145225
237,0.5696202516555786,0.6624472141265869,0.6351851851851852,0.09854029276199983,mmlu:high_school_world_history,test,406.8651014212519
11,0.27272728085517883,0.6363636255264282,0.41666666666666663,0.22725012085654517,mmlu:college_computer_science,validation,18.641382217407227
100,0.19999998807907104,0.7899999618530273,0.44156249999999997,0.09246164023876188,mmlu:college_computer_science,test,161.1374995931983
22,0.40909093618392944,0.6818181872367859,0.3333333333333333,0.11533252488483081,mmlu:sociology,validation,50.35243652574718
201,0.46268656849861145,0.7164179086685181,0.3638490641178813,0.03387708658009618,mmlu:sociology,test,450.3013980332762
25,0.19999998807907104,0.6800000071525574,0.405,0.06787744522094727,mmlu:marketing,validation,42.255294401198626
234,0.38034188747406006,0.7094017267227173,0.3932584269662921,0.03664184202495801,mmlu:marketing,test,376.8567993454635
8,0.0,0.875,,0.18594451248645782,mmlu:college_chemistry,validation,12.157911768183112
100,0.14000000059604645,0.8399999737739563,0.3558970099667774,0.04762121379375457,mmlu:college_chemistry,test,110.44144630804658
22,0.7272727489471436,0.8181818723678589,0.8229166666666666,0.14343686537309128,mmlu:high_school_us_history,validation,35.72412570938468
204,0.6225490570068359,0.7254902124404907,0.6725636568156252,0.07893372400134216,mmlu:high_school_us_history,test,326.6643782798201
13,0.38461539149284363,0.6153846383094788,0.675,0.3431980105546805,mmlu:international_law,validation,21.323240408673882
121,0.5619834661483765,0.652892529964447,0.5857380688124306,0.12514835940904853,mmlu:international_law,test,188.4248207937926
26,0.3461538553237915,0.692307710647583,0.5392156862745098,0.10984143156271714,mmlu:high_school_microeconomics,validation,47.935258032754064
238,0.3739495873451233,0.6974790096282959,0.44864640675665485,0.037445225385056805,mmlu:high_school_microeconomics,test,436.1647459156811
11,0.4545454680919647,0.5454545617103577,0.4666666666666667,0.1801718527620489,mmlu:machine_learning,validation,79.69793210551143
112,0.2232142984867096,0.7500000596046448,0.4464367816091954,0.10784185518111501,mmlu:machine_learning,test,153.2300817258656
26,0.3461538553237915,0.6538462042808533,0.4444444444444445,0.10558493779255797,mmlu:conceptual_physics,validation,55.344410210847855
235,0.3617021143436432,0.6340425610542297,0.37870588235294117,0.09092984275614964,mmlu:conceptual_physics,test,490.73449726589024
22,0.3181818127632141,0.6818181872367859,0.2095238095238095,0.20885626565326343,mmlu:college_medicine,validation,24.928617908619344
173,0.32947975397109985,0.7630057334899902,0.3913339382940109,0.06138885469105892,mmlu:college_medicine,test,200.08090385049582
100,0.14999999105930328,0.8499999642372131,0.4643137254901961,0.09052877306938174,mmlu:moral_scenarios,validation,123.82981861382723
895,0.14189943671226501,0.8513966202735901,0.5225762795275591,0.10604169828265742,mmlu:moral_scenarios,test,1017.9008174072951
18,0.2777777910232544,0.6111111044883728,0.7384615384615385,0.3415233426623874,mmlu:virology,validation,24.022158969193697
166,0.33734938502311707,0.6746987700462341,0.4675324675324676,0.0733991039086537,mmlu:virology,test,279.42239032592624
23,0.30434784293174744,0.739130437374115,0.34375,0.17510170781094092,mmlu:human_aging,validation,29.727912850677967
223,0.34529149532318115,0.726457417011261,0.3873421099448497,0.0450036886561612,mmlu:human_aging,test,351.7834972962737
170,0.4647058844566345,0.5941176414489746,0.4534705800528585,0.11203003911411062,mmlu:professional_law,validation,509.13045185245574
1534,0.45176008343696594,0.6036505699157715,0.5543296048646822,0.0857054541832154,mmlu:professional_law,test,4573.726818175986
11,0.09090909361839294,0.9090909361839294,0.14999999999999997,0.10414366830479013,mmlu:college_physics,validation,15.2763905916363
102,0.10784313827753067,0.843137264251709,0.5204795204795205,0.06301584313897526,mmlu:college_physics,test,119.71675683930516
31,0.4193548262119293,0.6774193048477173,0.5833333333333334,0.16012882224975092,mmlu:professional_medicine,validation,69.28832619264722
272,0.30514705181121826,0.7242646813392639,0.3237394020526551,0.044554427704390366,mmlu:professional_medicine,test,504.95250564441085
32,0.3125,0.625,0.6318181818181818,0.11618485301733018,mmlu:high_school_biology,validation,46.5333445109427
310,0.4032258093357086,0.6774193644523621,0.5089081081081082,0.055010805014641076,mmlu:high_school_biology,test,446.71159831248224
11,0.09090909361839294,0.8181818723678589,0.4,0.14530413259159433,mmlu:abstract_algebra,validation,7.178831897675991
100,0.2199999988079071,0.75,0.2896270396270396,0.07428100287914276,mmlu:abstract_algebra,test,78.47641719691455
29,0.03448275849223137,0.9655172228813171,0.7321428571428572,0.1169518643412097,mmlu:high_school_mathematics,validation,19.143038471229374
270,0.0555555559694767,0.940740704536438,0.4066666666666667,0.1049618473759404,mmlu:high_school_mathematics,test,177.5486112330109
12,0.3333333432674408,0.9166666865348816,0.6875,0.24845105906327566,mmlu:public_relations,validation,17.46667418256402
110,0.3272727131843567,0.7454545497894287,0.4844219219219219,0.04804378043521536,mmlu:public_relations,test,105.48829302936792
16,0.375,0.75,0.3416666666666666,0.10167334228754044,mmlu:college_biology,validation,18.912458142265677
144,0.3333333432674408,0.6666666865348816,0.4363064236111111,0.054564187924067184,mmlu:college_biology,test,178.29674409423023
27,0.7407407760620117,0.6666666865348816,0.43928571428571433,0.22116837457374292,mmlu:security_studies,validation,50.02325297705829
245,0.6734693646430969,0.7102040648460388,0.3759090909090909,0.02941044739314489,mmlu:security_studies,test,462.5402987189591
60,0.45000001788139343,0.6833333969116211,0.648709315375982,0.16283658643563592,mmlu:high_school_psychology,validation,74.69412405230105
545,0.4990825653076172,0.6587156057357788,0.6395038784744668,0.06284202687237239,mmlu:high_school_psychology,test,752.3800752367824
16,0.1875,0.875,0.3974358974358974,0.10025984048843384,mmlu:electrical_engineering,validation,17.988425681367517
145,0.22758620977401733,0.7724137902259827,0.3226461038961039,0.04165200570534013,mmlu:electrical_engineering,test,166.1713802628219
22,0.5454545617103577,0.6818181872367859,0.17083333333333334,0.14809697324579416,mmlu:high_school_geography,validation,33.44108732789755
198,0.3737373650074005,0.7171717286109924,0.3474825632083697,0.07998170063953203,mmlu:high_school_geography,test,250.91138183139265
41,0.3658536374568939,0.6341463327407837,0.33076923076923076,0.14930021326716356,mmlu:elementary_mathematics,validation,34.880640679970384
378,0.31481480598449707,0.7116401791572571,0.3559585996560787,0.112969281496825,mmlu:elementary_mathematics,test,418.3516948632896
34,0.3529411852359772,0.529411792755127,0.5606060606060606,0.23741285415256724,mmlu:philosophy,validation,57.79857900738716
311,0.34405145049095154,0.6237941980361938,0.5851887483965548,0.05495435957739974,mmlu:philosophy,test,541.5811617877334
11,0.4545454680919647,0.7272727489471436,0.06666666666666668,0.08822318640622226,mmlu:jurisprudence,validation,23.077819738537073
108,0.5,0.6111111044883728,0.3835733882030178,0.0717743949757682,mmlu:jurisprudence,test,165.87515867501497
