N,acc,unc_acc,unc_auroc,unc_ece,dataset,split,ts
22,0.5454545617103577,0.5454545617103577,0.20833333333333331,0.26260270313783124,mmlu:sociology,validation,87.09900687204208
201,0.38308456540107727,0.7164179086685181,0.3189149560117302,0.1013483680895905,mmlu:sociology,test,769.3348949661013
26,0.6538462042808533,0.692307710647583,0.6666666666666667,0.1517645372794225,mmlu:high_school_world_history,validation,108.69308419153094
237,0.5485231876373291,0.6540083885192871,0.7432063263838964,0.19641897019454696,mmlu:high_school_world_history,test,986.6741544175893
31,0.35483869910240173,0.5483870506286621,0.575,0.18086432449279294,mmlu:professional_medicine,validation,125.49806535057724
272,0.30514705181121826,0.6764705777168274,0.340727991330401,0.07913275282172594,mmlu:professional_medicine,test,1115.2498684395105
8,0.125,1.0,0.1428571428571429,0.17824426293373108,mmlu:college_chemistry,validation,31.14711032807827
100,0.1599999964237213,0.8899999856948853,0.375,0.12311424136161803,mmlu:college_chemistry,test,368.94899188913405
16,0.375,0.6875,0.725,0.2131424993276596,mmlu:astronomy,validation,60.8931344691664
152,0.4934210479259491,0.8092105388641357,0.5076190476190476,0.08767414877289222,mmlu:astronomy,test,564.7133201509714
16,0.25,0.75,0.6458333333333334,0.19933107867836952,mmlu:college_biology,validation,60.297362454235554
144,0.4027777910232544,0.75,0.5851042502004811,0.11911446932289335,mmlu:college_biology,test,538.8241623416543
11,0.3636363744735718,0.7272727489471436,0.5178571428571429,0.18243482979861173,mmlu:jurisprudence,validation,39.432098384946585
108,0.3888888955116272,0.7037037014961243,0.4666305916305916,0.08299931663054005,mmlu:jurisprudence,test,377.15949937468395
23,0.17391304671764374,0.782608687877655,0.8223684210526315,0.13364657370940497,mmlu:high_school_statistics,validation,88.76931316778064
216,0.24074074625968933,0.6388888955116272,0.5774507504690433,0.12521220164166558,mmlu:high_school_statistics,test,793.5977627336979
11,0.09090909361839294,0.9090909361839294,0.19999999999999996,0.08611352877183397,mmlu:college_mathematics,validation,39.88877422269434
100,0.09999999403953552,0.8999999761581421,0.25722222222222224,0.05192397654056549,mmlu:college_mathematics,test,480.07560402387753
22,0.4545454680919647,0.6363636255264282,0.4708333333333333,0.2035080140287226,mmlu:high_school_geography,validation,146.3917523138225
198,0.38383838534355164,0.7121211886405945,0.3653472821397757,0.08081507893523783,mmlu:high_school_geography,test,1313.2432826571167
35,0.4000000059604645,0.7428571581840515,0.38095238095238093,0.08963045052119664,mmlu:prehistory,validation,135.3019480444491
324,0.48765432834625244,0.645061731338501,0.45567713893548883,0.16681322786543107,mmlu:prehistory,test,1222.7715993598104
22,0.13636364042758942,0.9090909361839294,0.29824561403508776,0.10421750220385466,mmlu:high_school_chemistry,validation,147.91482804343104
203,0.1822660118341446,0.8325123190879822,0.3133344187561055,0.05351258528056401,mmlu:high_school_chemistry,test,1358.6649527177215
33,0.3030303120613098,0.696969747543335,0.4282608695652174,0.2032184492457997,mmlu:nutrition,validation,112.60703978687525
306,0.35947713255882263,0.6895424723625183,0.598678107606679,0.10586698966867783,mmlu:nutrition,test,1038.665133697912
27,0.5185185074806213,0.6296296119689941,0.5906593406593407,0.1547272315731755,mmlu:security_studies,validation,95.56701134517789
245,0.5510203838348389,0.6734693646430969,0.462020202020202,0.06236512660980224,mmlu:security_studies,test,854.8394235447049
16,0.25,0.8125,0.7291666666666666,0.21871709823608398,mmlu:electrical_engineering,validation,107.51947477832437
145,0.24137930572032928,0.7655172348022461,0.41987012987012984,0.05105676075507857,mmlu:electrical_engineering,test,960.9761447273195
12,0.1666666716337204,0.8333333730697632,0.9500000000000001,0.15723787744839987,mmlu:econometrics,validation,47.00057256035507
114,0.18421052396297455,0.780701756477356,0.47875064004096257,0.08659168555025469,mmlu:econometrics,test,429.6663095783442
18,0.3333333432674408,0.7222222089767456,0.19444444444444445,0.22371151049931845,mmlu:virology,validation,75.73918518982828
166,0.3072288930416107,0.7228915691375732,0.3205456095481671,0.1314173218715622,mmlu:virology,test,634.3480216208845
12,0.25,0.75,0.2962962962962963,0.16374700268109638,mmlu:human_sexuality,validation,48.098308101296425
131,0.42748090624809265,0.6717557311058044,0.41261904761904766,0.15172279836567307,mmlu:human_sexuality,test,483.28813979774714
11,0.1818181872367859,0.7272727489471436,0.33333333333333337,0.21752405166625977,mmlu:college_computer_science,validation,79.88597317785025
100,0.1599999964237213,0.8199999928474426,0.40401785714285715,0.12010188698768615,mmlu:college_computer_science,test,722.6790710613132
23,0.30434784293174744,0.695652186870575,0.30803571428571425,0.13033823604169104,mmlu:human_aging,validation,85.64618703350425
223,0.3632287085056305,0.7578475475311279,0.36602330029560076,0.044786518999279344,mmlu:human_aging,test,822.0308829378337
26,0.38461539149284363,0.5769230723381042,0.32499999999999996,0.2321818287555988,mmlu:high_school_microeconomics,validation,171.6224971599877
238,0.38235294818878174,0.7310924530029297,0.48474994393361737,0.0664345549435175,mmlu:high_school_microeconomics,test,1567.931573189795
17,0.11764705926179886,0.7058823704719543,1.0,0.27530264503815594,mmlu:high_school_physics,validation,116.67994449287653
151,0.21192052960395813,0.5960264801979065,0.49816176470588236,0.1394574705338636,mmlu:high_school_physics,test,1012.041961401701
22,0.40909093618392944,0.7727273106575012,0.7222222222222222,0.16105559468269348,mmlu:college_medicine,validation,83.34089288301766
173,0.323699414730072,0.7341040372848511,0.6274420024420024,0.07231092349642271,mmlu:college_medicine,test,658.5015940796584
38,0.3947368562221527,0.7368420958518982,0.6434782608695653,0.15872972733096075,mmlu:moral_disputes,validation,131.41068379208446
346,0.42485547065734863,0.6647398471832275,0.41476087922606225,0.04470672945066685,mmlu:moral_disputes,test,1234.8444352191873
34,0.23529411852359772,0.7352941036224365,0.6923076923076923,0.18148338794708255,mmlu:philosophy,validation,125.09523515030742
311,0.31832796335220337,0.6527330875396729,0.5124833238040785,0.0710368028024385,mmlu:philosophy,test,1131.433066714555
25,0.35999998450279236,0.4399999976158142,0.4861111111111111,0.21802021503448485,mmlu:marketing,validation,93.5022832043469
234,0.4700855016708374,0.696581244468689,0.4951979472140763,0.03816740354921064,mmlu:marketing,test,870.3304562661797
43,0.39534884691238403,0.6744186282157898,0.4106334841628959,0.13002843635026798,mmlu:high_school_macroeconomics,validation,160.29717278294265
390,0.33076924085617065,0.7179487347602844,0.31028542576257095,0.06594596719130492,mmlu:high_school_macroeconomics,test,1426.4207057356834
29,0.10344827175140381,0.8965517282485962,0.7115384615384616,0.0762352182947356,mmlu:high_school_mathematics,validation,117.74381623417139
270,0.09259258955717087,0.9148147702217102,0.2840000000000001,0.028403983292756278,mmlu:high_school_mathematics,test,1018.49839146249
11,0.09090909361839294,0.7272727489471436,0.09999999999999998,0.31067846579985187,mmlu:abstract_algebra,validation,43.247824216261506
100,0.22999998927116394,0.7199999690055847,0.34782608695652173,0.06730394661426545,mmlu:abstract_algebra,test,382.07891195453703
32,0.375,0.625,0.5208333333333334,0.1941703259944916,mmlu:high_school_biology,validation,111.51453225314617
310,0.47096773982048035,0.5935483574867249,0.3437186769127965,0.21504508045411885,mmlu:high_school_biology,test,1064.0418885014951
18,0.5555555820465088,0.8888888955116272,0.95,0.09758203559451631,mmlu:logical_fallacies,validation,125.90349885448813
163,0.453987717628479,0.7607361674308777,0.696781050713635,0.08889699603882305,mmlu:logical_fallacies,test,1113.3079682961106
41,0.24390242993831635,0.707317054271698,0.36451612903225805,0.1805933713912964,mmlu:elementary_mathematics,validation,153.76416615769267
378,0.37566137313842773,0.637566089630127,0.39112855096681787,0.20179596905985836,mmlu:elementary_mathematics,test,1446.6818693876266
11,0.5454545617103577,0.9090909361839294,0.2333333333333333,0.31525679068131884,mmlu:business_ethics,validation,42.09264308400452
100,0.3499999940395355,0.6499999761581421,0.501978021978022,0.07066429257392884,mmlu:business_ethics,test,375.27468842081726
11,0.4545454680919647,0.7272727489471436,0.4,0.2914539467204701,mmlu:computer_security,validation,40.28269392065704
100,0.4899999797344208,0.6200000047683716,0.3719487795118047,0.10833162546157839,mmlu:computer_security,test,333.6045856382698
86,0.604651153087616,0.6744186282157898,0.6001131221719458,0.167774930249813,mmlu:miscellaneous,validation,350.9627802092582
783,0.6321839094161987,0.7203065156936646,0.5242459315375982,0.10577105676534075,mmlu:miscellaneous,test,3354.399236837402
29,0.24137930572032928,0.8275861740112305,0.538961038961039,0.09773740686219314,mmlu:clinical_knowledge,validation,109.16947169788182
265,0.32830190658569336,0.7547169923782349,0.4840823970037453,0.0509892501921024,mmlu:clinical_knowledge,test,980.2660415638238
11,0.7272727489471436,0.27272728085517883,0.5,0.49026885357770056,mmlu:management,validation,45.38029018044472
103,0.43689319491386414,0.6601941585540771,0.45727969348659003,0.11748272064820076,mmlu:management,test,327.1614777557552
13,0.3076923191547394,0.692307710647583,0.47222222222222227,0.286371281513801,mmlu:international_law,validation,52.23144992440939
121,0.5454545021057129,0.5454545021057129,0.39779614325068874,0.10674580562213239,mmlu:international_law,test,460.8089631255716
26,0.38461539149284363,0.6538462042808533,0.19687500000000002,0.19476755307270932,mmlu:conceptual_physics,validation,92.85415536165237
235,0.4723404049873352,0.570212721824646,0.4206625980819529,0.25316717751482704,mmlu:conceptual_physics,test,805.7136132698506
60,0.5833333730697632,0.7666667103767395,0.6760000000000002,0.12464126050472259,mmlu:high_school_psychology,validation,206.39502778928727
545,0.5486238598823547,0.6605504751205444,0.6413927182750087,0.11350865746856831,mmlu:high_school_psychology,test,1870.8538876930252
100,0.28999999165534973,0.7099999785423279,0.5225837785332685,0.20340470731258387,mmlu:moral_scenarios,validation,370.16685120761395
895,0.3027932941913605,0.6972066760063171,0.4507906377140694,0.2142904175060421,mmlu:moral_scenarios,test,3321.045126825571
11,0.5454545617103577,0.8181818723678589,0.6166666666666667,0.12652857737107714,mmlu:us_foreign_policy,validation,44.719886066392064
100,0.6499999761581421,0.7099999785423279,0.6024175824175824,0.037730113863944995,mmlu:us_foreign_policy,test,367.8087540231645
69,0.3913043439388275,0.49275362491607666,0.5692239858906525,0.2340934613476629,mmlu:professional_psychology,validation,260.47650083713233
612,0.3235294222831726,0.6258170008659363,0.5587395696091348,0.11450676405741501,mmlu:professional_psychology,test,2379.9000171329826
12,0.25,0.8333333730697632,0.2777777777777778,0.13276682297388714,mmlu:public_relations,validation,44.65597992390394
110,0.2818181812763214,0.6909090876579285,0.47386688444262964,0.08552914749492299,mmlu:public_relations,test,488.9903493523598
14,0.2142857313156128,0.9285714626312256,0.6666666666666666,0.12249918920653206,mmlu:anatomy,validation,51.06656183861196
135,0.4888888895511627,0.6592592597007751,0.5171277997364954,0.12246460693853872,mmlu:anatomy,test,502.74502189457417
31,0.09677419066429138,0.6451612710952759,0.738095238095238,0.13361996412277224,mmlu:professional_accounting,validation,122.5788035877049
282,0.1702127605676651,0.7446808218955994,0.5128205128205128,0.014380299453194275,mmlu:professional_accounting,test,1060.5634131878614
21,0.5714285969734192,0.523809552192688,0.5601851851851851,0.3164786299069723,mmlu:high_school_government_and_politics,validation,78.09313670545816
193,0.5129533410072327,0.6735751032829285,0.6519987105093489,0.1165648569097173,mmlu:high_school_government_and_politics,test,680.2158688493073
170,0.42352941632270813,0.6647058725357056,0.4730725623582766,0.14067695526515736,mmlu:professional_law,validation,584.712205145508
1534,0.39634940028190613,0.6271186470985413,0.4902141710242129,0.13034498505051478,mmlu:professional_law,test,5229.3075263071805
11,0.27272728085517883,0.7272727489471436,0.5,0.1807312911207026,mmlu:college_physics,validation,38.60911552794278
102,0.18627451360225677,0.686274528503418,0.6686746987951807,0.06658287843068443,mmlu:college_physics,test,347.45863416139036
14,0.5,0.5714285969734192,0.10204081632653064,0.22131354893956862,mmlu:formal_logic,validation,52.26741788163781
126,0.2142857313156128,0.7619048357009888,0.5989524878413767,0.04708549522218249,mmlu:formal_logic,test,432.81661798246205
10,0.20000000298023224,0.699999988079071,0.59375,0.26007076501846316,mmlu:global_facts,validation,38.673077680170536
100,0.2199999988079071,0.7299999594688416,0.44493006993006995,0.08163354039192203,mmlu:global_facts,test,366.929800381884
11,0.7272727489471436,0.8181818723678589,0.75,0.1770114302635193,mmlu:medical_genetics,validation,45.0743488445878
100,0.4699999988079071,0.7199999690055847,0.5058209554395825,0.06668940901756287,mmlu:medical_genetics,test,388.63417364284396
19,0.6842105388641357,0.8421052694320679,0.5897435897435898,0.14914445814333463,mmlu:world_religions,validation,69.56185710243881
171,0.6549707651138306,0.7543859481811523,0.7163286924939468,0.023136508395100185,mmlu:world_religions,test,628.2199292797595
11,0.3636363744735718,0.8181818723678589,0.6428571428571428,0.1421267119320956,mmlu:machine_learning,validation,44.01642496697605
112,0.2053571492433548,0.7321428656578064,0.4284318514899853,0.07772670579808097,mmlu:machine_learning,test,439.33011791296303
9,0.2222222238779068,0.7777777910232544,0.7142857142857143,0.22994275887807208,mmlu:high_school_computer_science,validation,36.093554532155395
100,0.4399999976158142,0.6800000071525574,0.5284090909090909,0.12395196497440338,mmlu:high_school_computer_science,test,374.9056668244302
