N,acc,unc_acc,unc_auroc,unc_ece,dataset,split,ts
11,0.4545454680919647,0.5454545617103577,0.5,0.022017045454545414,mmlu:abstract_algebra,validation,4.013644051970914
100,0.28999999165534973,0.7099999785423279,0.5,0.18656249999999996,mmlu:abstract_algebra,test,3.485496518202126
14,0.4285714328289032,0.5714285969734192,0.5,0.0479910714285714,mmlu:anatomy,validation,0.7025337878149003
135,0.5703703761100769,0.4296296238899231,0.5,0.09380787037037036,mmlu:anatomy,test,3.578425014158711
16,0.375,0.625,0.5,0.1015625,mmlu:astronomy,validation,0.7859437519218773
152,0.41447368264198303,0.5855262875556946,0.5,0.06208881578947367,mmlu:astronomy,test,4.875914742005989
11,0.6363636255264282,0.3636363744735718,0.5,0.15980113636363635,mmlu:business_ethics,validation,0.7101348049473017
100,0.3499999940395355,0.6499999761581421,0.5,0.12656250000000002,mmlu:business_ethics,test,3.9787001200020313
29,0.24137930572032928,0.7586206793785095,0.5,0.23518318965517238,mmlu:clinical_knowledge,validation,1.0705160561483353
265,0.35849055647850037,0.6415094137191772,0.5,0.11807193396226412,mmlu:clinical_knowledge,test,7.315984851215035
16,0.25,0.75,0.5,0.2265625,mmlu:college_biology,validation,0.7454380970448256
144,0.3958333432674408,0.6041666865348816,0.5,0.08072916666666663,mmlu:college_biology,test,5.039447963004932
8,0.125,0.875,0.5,0.3515625,mmlu:college_chemistry,validation,0.5120788370259106
100,0.23000000417232513,0.7699999809265137,0.5,0.24656250000000002,mmlu:college_chemistry,test,4.325590563938022
11,0.09090909361839294,0.9090909361839294,0.5,0.38565340909090906,mmlu:college_computer_science,validation,0.9589098310098052
100,0.20999999344348907,0.7900000214576721,0.5,0.26656250000000004,mmlu:college_computer_science,test,7.371894563082606
11,0.09090909361839294,0.9090909361839294,0.5,0.38565340909090906,mmlu:college_mathematics,validation,0.8426138628274202
100,0.20000000298023224,0.800000011920929,0.5,0.27656250000000004,mmlu:college_mathematics,test,5.315513957990333
22,0.5,0.5,0.5,0.0234375,mmlu:college_medicine,validation,1.0546313980594277
173,0.39306357502937317,0.6069363951683044,0.5,0.08349891618497107,mmlu:college_medicine,test,9.36591527913697
11,0.4545454680919647,0.5454545617103577,0.5,0.022017045454545414,mmlu:college_physics,validation,0.7402688851580024
102,0.23529411852359772,0.7647058963775635,0.5,0.24126838235294112,mmlu:college_physics,test,4.563237379072234
11,0.6363636255264282,0.3636363744735718,0.5,0.15980113636363635,mmlu:computer_security,validation,0.6853062249720097
100,0.49000000953674316,0.5099999904632568,0.5,0.013437499999999991,mmlu:computer_security,test,3.01345923403278
26,0.3076923191547394,0.692307710647583,0.5,0.1688701923076923,mmlu:conceptual_physics,validation,1.084544361103326
235,0.48085105419158936,0.5191489458084106,0.5,0.004288563829787262,mmlu:conceptual_physics,test,5.878678956069052
12,0.5,0.5,0.5,0.0234375,mmlu:econometrics,validation,0.8382080208975822
114,0.21929824352264404,0.780701756477356,0.5,0.2572642543859649,mmlu:econometrics,test,6.003415270941332
16,0.3125,0.6875,0.5,0.1640625,mmlu:electrical_engineering,validation,0.7556576319038868
145,0.2896551787853241,0.7103448510169983,0.5,0.18690732758620687,mmlu:electrical_engineering,test,5.225425832206383
41,0.3658536672592163,0.6341463327407837,0.5,0.11070884146341464,mmlu:elementary_mathematics,validation,1.7555094310082495
378,0.44708994030952454,0.5529100298881531,0.5,0.029472552910052907,mmlu:elementary_mathematics,test,13.924376495182514
14,0.4285714328289032,0.5714285969734192,0.5,0.0479910714285714,mmlu:formal_logic,validation,0.7731081240344793
126,0.3095238208770752,0.6904761791229248,0.5,0.16703869047619047,mmlu:formal_logic,test,5.7168614799156785
10,0.20000000298023224,0.800000011920929,0.5,0.27656250000000004,mmlu:global_facts,validation,0.579744340153411
100,0.1899999976158142,0.8100000023841858,0.5,0.28656250000000005,mmlu:global_facts,test,3.2035788549110293
32,0.46875,0.53125,0.5,0.0078125,mmlu:high_school_biology,validation,1.2347805588506162
310,0.5064516067504883,0.4935483932495117,0.5,0.029889112903225823,mmlu:high_school_biology,test,10.646560418885201
22,0.13636364042758942,0.8636363744735718,0.5,0.34019886363636365,mmlu:high_school_chemistry,validation,1.0218969569541514
203,0.2019704431295395,0.7980295419692993,0.5,0.27459205665024633,mmlu:high_school_chemistry,test,8.293483385117725
9,0.4444444477558136,0.5555555820465088,0.5,0.03211805555555558,mmlu:high_school_computer_science,validation,0.851413789903745
100,0.5699999928474426,0.4300000071525574,0.5,0.0934375,mmlu:high_school_computer_science,test,6.6284289818722755
18,0.7222222089767456,0.2777777910232544,0.5,0.2456597222222222,mmlu:high_school_european_history,validation,5.276336417067796
165,0.678787887096405,0.32121211290359497,0.5,0.20222537878787877,mmlu:high_school_european_history,test,47.488551661837846
22,0.3636363744735718,0.6363636255264282,0.5,0.11292613636363635,mmlu:high_school_geography,validation,0.8761758068576455
198,0.4444444477558136,0.5555555820465088,0.5,0.03211805555555558,mmlu:high_school_geography,test,5.055403104051948
21,0.5714285969734192,0.4285714328289032,0.5,0.09486607142857145,mmlu:high_school_government_and_politics,validation,0.9053931701928377
193,0.5544041395187378,0.4455958604812622,0.5,0.07784164507772023,mmlu:high_school_government_and_politics,test,5.687744743889198
43,0.44186046719551086,0.5581395626068115,0.5,0.03470203488372092,mmlu:high_school_macroeconomics,validation,1.4476067058276385
390,0.3692307770252228,0.6307692527770996,0.5,0.10733173076923075,mmlu:high_school_macroeconomics,test,10.161324117099866
29,0.06896551698446274,0.931034505367279,0.5,0.40759698275862066,mmlu:high_school_mathematics,validation,1.5183988891076297
270,0.12962962687015533,0.8703703880310059,0.5,0.34693287037037035,mmlu:high_school_mathematics,test,12.43068234110251
26,0.42307692766189575,0.5769230723381042,0.5,0.05348557692307687,mmlu:high_school_microeconomics,validation,0.980876452056691
238,0.3781512677669525,0.6218487620353699,0.5,0.09841123949579833,mmlu:high_school_microeconomics,test,6.3617971141356975
17,0.1764705926179886,0.8235294222831726,0.5,0.30009191176470584,mmlu:high_school_physics,validation,1.0379294708836824
151,0.20529800653457642,0.7947019934654236,0.5,0.27126448675496684,mmlu:high_school_physics,test,6.499511257978156
60,0.6333333253860474,0.36666667461395264,0.5,0.15677083333333336,mmlu:high_school_psychology,validation,2.3044314438011497
545,0.5596330165863037,0.4403669834136963,0.5,0.08307052752293576,mmlu:high_school_psychology,test,18.645263162907213
23,0.30434781312942505,0.695652186870575,0.5,0.17221467391304346,mmlu:high_school_statistics,validation,1.4955172501504421
216,0.32870370149612427,0.6712962985038757,0.5,0.14785879629629628,mmlu:high_school_statistics,test,12.59956870903261
22,0.5909090638160706,0.40909090638160706,0.5,0.11434659090909088,mmlu:high_school_us_history,validation,4.980751320021227
204,0.6421568393707275,0.3578431308269501,0.5,0.16559436274509803,mmlu:high_school_us_history,test,45.096860848134384
26,0.5769230723381042,0.42307692766189575,0.5,0.10036057692307693,mmlu:high_school_world_history,validation,4.223789359908551
237,0.4388185739517212,0.5611814260482788,0.5,0.03774393459915615,mmlu:high_school_world_history,test,34.090534107992426
23,0.30434781312942505,0.695652186870575,0.5,0.17221467391304346,mmlu:human_aging,validation,0.8777787680737674
223,0.34529146552085876,0.6547085046768188,0.5,0.1312710201793722,mmlu:human_aging,test,5.608853694051504
12,0.3333333432674408,0.6666666865348816,0.5,0.14322916666666663,mmlu:human_sexuality,validation,0.6114223038312048
131,0.49618321657180786,0.5038167834281921,0.5,0.019620706106870278,mmlu:human_sexuality,test,3.6565322128590196
13,0.4615384638309479,0.5384615659713745,0.5,0.015024038461538436,mmlu:international_law,validation,0.6553000560961664
121,0.5619834661483765,0.43801653385162354,0.5,0.08542097107438018,mmlu:international_law,test,4.260432234965265
11,0.4545454680919647,0.5454545617103577,0.5,0.022017045454545414,mmlu:jurisprudence,validation,0.6860759858973324
108,0.49074074625968933,0.5092592835426331,0.5,0.0141782407407407,mmlu:jurisprudence,test,3.2445197941269726
18,0.5,0.5,0.5,0.0234375,mmlu:logical_fallacies,validation,0.8447834320832044
163,0.47852760553359985,0.5214723944664001,0.5,0.001965107361963203,mmlu:logical_fallacies,test,5.094623513985425
11,0.1818181872367859,0.8181818127632141,0.5,0.29474431818181823,mmlu:machine_learning,validation,0.7864773841574788
112,0.2767857015132904,0.7232142686843872,0.5,0.1997767857142857,mmlu:machine_learning,test,5.354225419927388
11,0.6363636255264282,0.3636363744735718,0.5,0.15980113636363635,mmlu:management,validation,0.6131652598269284
103,0.3980582654476166,0.6019417643547058,0.5,0.07850424757281549,mmlu:management,test,2.440928999101743
25,0.2800000011920929,0.7200000286102295,0.5,0.19656249999999997,mmlu:marketing,validation,1.0086161219514906
234,0.47863247990608215,0.5213675498962402,0.5,0.0020699786324785974,mmlu:marketing,test,6.593081812839955
11,0.7272727489471436,0.27272728085517883,0.5,0.2507102272727273,mmlu:medical_genetics,validation,0.599006568081677
100,0.550000011920929,0.44999998807907104,0.5,0.07343749999999999,mmlu:medical_genetics,test,2.5296787649858743
86,0.5930232405662537,0.40697672963142395,0.5,0.11646075581395349,mmlu:miscellaneous,validation,2.364540355047211
783,0.6245210766792297,0.37547892332077026,0.5,0.14795857279693486,mmlu:miscellaneous,test,19.12634173105471
38,0.3684210479259491,0.6315789222717285,0.5,0.10814144736842102,mmlu:moral_disputes,validation,1.3784160101786256
346,0.40751445293426514,0.5924855470657349,0.5,0.06904804913294793,mmlu:moral_disputes,test,10.733694757102057
100,0.5899999737739563,0.4099999964237213,0.5,0.11343750000000002,mmlu:moral_scenarios,validation,5.54374990100041
895,0.5329608917236328,0.4670391082763672,0.5,0.056398393854748596,mmlu:moral_scenarios,test,47.76333517301828
33,0.39393940567970276,0.6060606241226196,0.5,0.08262310606060608,mmlu:nutrition,validation,1.5029565910808742
306,0.4346405267715454,0.5653594732284546,0.5,0.041921977124182996,mmlu:nutrition,test,11.050776422023773
34,0.38235294818878174,0.6176470518112183,0.5,0.09420955882352944,mmlu:philosophy,validation,1.3605635268613696
311,0.34405145049095154,0.6559485793113708,0.5,0.13251105305466238,mmlu:philosophy,test,8.3292719819583
35,0.37142857909202576,0.6285714507102966,0.5,0.10513392857142856,mmlu:prehistory,validation,1.3860314290504903
324,0.45987653732299805,0.540123462677002,0.5,0.016685956790123413,mmlu:prehistory,test,9.420215081889182
31,0.19354838132858276,0.8064516186714172,0.5,0.28301411290322576,mmlu:professional_accounting,validation,2.0719130248762667
282,0.173758864402771,0.826241135597229,0.5,0.3028036347517731,mmlu:professional_accounting,test,16.978139135055244
170,0.4000000059604645,0.6000000238418579,0.5,0.07656249999999998,mmlu:professional_law,validation,22.953460841905326
1534,0.35071706771850586,0.6492829322814941,0.5,0.1258454204693612,mmlu:professional_law,test,211.78614243888296
31,0.4516128897666931,0.5483871102333069,0.5294117647058824,0.009895833333333326,mmlu:professional_medicine,validation,3.4925035987980664
272,0.35661765933036804,0.6433823704719543,0.5,0.11994485294117652,mmlu:professional_medicine,test,29.29562302096747
69,0.43478259444236755,0.5652173757553101,0.5,0.04177989130434778,mmlu:professional_psychology,validation,2.7915703989565372
612,0.3660130798816681,0.6339869499206543,0.5,0.11054942810457513,mmlu:professional_psychology,test,22.027258621994406
12,0.3333333432674408,0.6666666865348816,0.5,0.14322916666666663,mmlu:public_relations,validation,0.59427260607481
110,0.30909091234207153,0.6909090876579285,0.5,0.16747159090909092,mmlu:public_relations,test,3.416537584969774
27,0.5925925970077515,0.40740740299224854,0.5,0.11603009259259262,mmlu:security_studies,validation,1.4677548750769347
245,0.5469387769699097,0.45306122303009033,0.5,0.07037627551020409,mmlu:security_studies,test,11.042047573952004
22,0.5,0.5,0.5,0.0234375,mmlu:sociology,validation,0.9086014898493886
201,0.45771142840385437,0.5422885417938232,0.5,0.018851057213930322,mmlu:sociology,test,5.4210928729735315
11,0.7272727489471436,0.27272728085517883,0.5,0.2507102272727273,mmlu:us_foreign_policy,validation,0.548651544842869
100,0.6100000143051147,0.38999998569488525,0.5,0.1334375,mmlu:us_foreign_policy,test,2.853358004009351
18,0.3888888955116272,0.6111111044883728,0.5,0.08767361111111116,mmlu:virology,validation,0.9219099150504917
166,0.34939759969711304,0.650602400302887,0.5,0.1271649096385542,mmlu:virology,test,4.648825310869142
19,0.6842105388641357,0.31578946113586426,0.5,0.2076480263157895,mmlu:world_religions,validation,0.8047479938250035
171,0.6725146174430847,0.3274853825569153,0.5,0.19595211988304095,mmlu:world_religions,test,3.9373375601135194
