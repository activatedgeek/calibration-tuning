N,acc,unc_acc,unc_auroc,unc_ece,dataset,split,ts
11,0.1818181872367859,0.1818181872367859,0.5,0.3220880681818182,mmlu_offline:llama2-7b:abstract_algebra,validation,5.86587848700583
100,0.2199999988079071,0.2199999988079071,0.5,0.28390625,mmlu_offline:llama2-7b:abstract_algebra,test,3.49300808692351
14,0.2857142984867096,0.2857142984867096,0.5,0.2181919642857143,mmlu_offline:llama2-7b:anatomy,validation,0.6330005610361695
135,0.4148148000240326,0.4148148000240326,0.5,0.0890914351851852,mmlu_offline:llama2-7b:anatomy,test,4.165591675788164
16,0.4375,0.4375,0.5,0.06640625,mmlu_offline:llama2-7b:astronomy,validation,0.818304989952594
152,0.5065789222717285,0.5065789222717285,0.5,0.0026726973684210176,mmlu_offline:llama2-7b:astronomy,test,5.703841242939234
11,0.6363636255264282,0.6363636255264282,0.5,0.13245738636363635,mmlu_offline:llama2-7b:business_ethics,validation,0.7623371588997543
100,0.30000001192092896,0.30000001192092896,0.5,0.20390625,mmlu_offline:llama2-7b:business_ethics,test,4.91111340560019
29,0.24137930572032928,0.24137930572032928,0.5,0.2625269396551724,mmlu_offline:llama2-7b:clinical_knowledge,validation,1.149123793002218
265,0.35849055647850037,0.35849055647850037,0.5,0.14541568396226418,mmlu_offline:llama2-7b:clinical_knowledge,test,8.871707019861788
16,0.3125,0.3125,0.5,0.19140625,mmlu_offline:llama2-7b:college_biology,validation,0.787783085834235
144,0.2986111044883728,0.2986111044883728,0.5,0.2052951388888889,mmlu_offline:llama2-7b:college_biology,test,5.91016159625724
8,0.125,0.125,0.5,0.37890625,mmlu_offline:llama2-7b:college_chemistry,validation,0.5562648880295455
100,0.14000000059604645,0.14000000059604645,0.5,0.36390625,mmlu_offline:llama2-7b:college_chemistry,test,4.8223005081526935
11,0.0,0.0,,0.50390625,mmlu_offline:llama2-7b:college_computer_science,validation,1.2893236940726638
100,0.17000000178813934,0.18000000715255737,0.5060240963855422,0.33273437499999997,mmlu_offline:llama2-7b:college_computer_science,test,8.385929228272289
11,0.0,0.0,,0.50390625,mmlu_offline:llama2-7b:college_mathematics,validation,0.8432810790836811
100,0.1599999964237213,0.1599999964237213,0.5,0.34390624999999997,mmlu_offline:llama2-7b:college_mathematics,test,5.806174150668085
22,0.3636363744735718,0.3636363744735718,0.5,0.14026988636363635,mmlu_offline:llama2-7b:college_medicine,validation,1.1048872508108616
173,0.36994218826293945,0.36994218826293945,0.5,0.1339640534682081,mmlu_offline:llama2-7b:college_medicine,test,10.594936289824545
11,0.27272728085517883,0.27272728085517883,0.5,0.2311789772727273,mmlu_offline:llama2-7b:college_physics,validation,0.8147663292475045
102,0.14705882966518402,0.14705882966518402,0.5,0.3568474264705882,mmlu_offline:llama2-7b:college_physics,test,5.328052626922727
11,0.7272727489471436,0.7272727489471436,0.5,0.2233664772727273,mmlu_offline:llama2-7b:computer_security,validation,0.6572260218672454
100,0.4300000071525574,0.4300000071525574,0.5,0.07390625,mmlu_offline:llama2-7b:computer_security,test,3.5281619802117348
26,0.3076923191547394,0.3076923191547394,0.5,0.1962139423076923,mmlu_offline:llama2-7b:conceptual_physics,validation,1.0549492188729346
235,0.4127659499645233,0.4127659499645233,0.5,0.0911402925531915,mmlu_offline:llama2-7b:conceptual_physics,test,8.746196351014078
12,0.0833333358168602,0.0833333358168602,0.5,0.4205729166666667,mmlu_offline:llama2-7b:econometrics,validation,0.898110797163099
114,0.14912280440330505,0.14912280440330505,0.5,0.3547834429824561,mmlu_offline:llama2-7b:econometrics,test,6.776641678065062
16,0.125,0.125,0.5,0.37890625,mmlu_offline:llama2-7b:electrical_engineering,validation,0.8461862481199205
145,0.1862068921327591,0.1862068921327591,0.5,0.3176993534482758,mmlu_offline:llama2-7b:electrical_engineering,test,6.063909023068845
41,0.26829269528388977,0.26829269528388977,0.5,0.23561356707317072,mmlu_offline:llama2-7b:elementary_mathematics,validation,2.094593388028443
378,0.32275131344795227,0.32275131344795227,0.5,0.18115492724867727,mmlu_offline:llama2-7b:elementary_mathematics,test,16.709105824120343
14,0.5,0.5,0.4285714285714286,0.034555288461538436,mmlu_offline:llama2-7b:formal_logic,validation,0.8478605933487415
126,0.30158731341362,0.2936508059501648,0.4813098086124402,0.2164248511904762,mmlu_offline:llama2-7b:formal_logic,test,6.672638392075896
10,0.30000001192092896,0.30000001192092896,0.5,0.20390625,mmlu_offline:llama2-7b:global_facts,validation,0.6655177809298038
100,0.17000000178813934,0.17000000178813934,0.5,0.33390624999999996,mmlu_offline:llama2-7b:global_facts,test,3.8911418737843633
32,0.15625,0.15625,0.5,0.34765625,mmlu_offline:llama2-7b:high_school_biology,validation,1.4484731499105692
310,0.41290321946144104,0.41290321946144104,0.5,0.09100302419354839,mmlu_offline:llama2-7b:high_school_biology,test,12.594215549062937
22,0.13636364042758942,0.13636364042758942,0.5,0.36754261363636365,mmlu_offline:llama2-7b:high_school_chemistry,validation,1.1793392598628998
203,0.16748768091201782,0.16748768091201782,0.5,0.33641856527093594,mmlu_offline:llama2-7b:high_school_chemistry,test,9.375544470734894
9,0.2222222238779068,0.2222222238779068,0.5,0.2816840277777778,mmlu_offline:llama2-7b:high_school_computer_science,validation,0.9079120708629489
100,0.3700000047683716,0.3700000047683716,0.5135135135135135,0.1392578125,mmlu_offline:llama2-7b:high_school_computer_science,test,7.368378814775497
18,0.7222222089767456,0.7222222089767456,0.5,0.2183159722222222,mmlu_offline:llama2-7b:high_school_european_history,validation,5.95175969414413
165,0.6727272868156433,0.6727272868156433,0.5,0.16882102272727273,mmlu_offline:llama2-7b:high_school_european_history,test,53.918795579113066
22,0.4545454680919647,0.4545454680919647,0.5,0.04936079545454547,mmlu_offline:llama2-7b:high_school_geography,validation,0.8482314250431955
198,0.3636363744735718,0.3636363744735718,0.5,0.14026988636363635,mmlu_offline:llama2-7b:high_school_geography,test,6.484884689096361
21,0.4285714328289032,0.4285714328289032,0.5,0.07533482142857145,mmlu_offline:llama2-7b:high_school_government_and_politics,validation,0.9723514369688928
193,0.48704662919044495,0.48704662919044495,0.5,0.01685961787564766,mmlu_offline:llama2-7b:high_school_government_and_politics,test,6.703231068328023
43,0.41860464215278625,0.41860464215278625,0.5,0.08530159883720928,mmlu_offline:llama2-7b:high_school_macroeconomics,validation,1.6385003980249166
390,0.34358975291252136,0.34358975291252136,0.5,0.1603165064102564,mmlu_offline:llama2-7b:high_school_macroeconomics,test,12.587819644715637
29,0.03448275849223137,0.03448275849223137,0.5,0.46942349137931033,mmlu_offline:llama2-7b:high_school_mathematics,validation,1.7416321719065309
270,0.08148147910833359,0.08148147910833359,0.5,0.4224247685185185,mmlu_offline:llama2-7b:high_school_mathematics,test,13.799967887811363
26,0.3076923191547394,0.3076923191547394,0.5,0.1962139423076923,mmlu_offline:llama2-7b:high_school_microeconomics,validation,1.0765945529565215
238,0.32773110270500183,0.32773110270500183,0.5,0.1761751575630252,mmlu_offline:llama2-7b:high_school_microeconomics,test,7.776157300919294
17,0.23529411852359772,0.23529411852359772,0.5,0.2686121323529412,mmlu_offline:llama2-7b:high_school_physics,validation,1.1544296322390437
151,0.17880794405937195,0.17880794405937195,0.5,0.3250983029801324,mmlu_offline:llama2-7b:high_school_physics,test,7.495309999212623
60,0.5,0.5,0.5,0.00390625,mmlu_offline:llama2-7b:high_school_psychology,validation,2.710277865640819
545,0.5064220428466797,0.5064220428466797,0.5,0.0025157683486238813,mmlu_offline:llama2-7b:high_school_psychology,test,22.73462192201987
23,0.1304347813129425,0.1304347813129425,0.5,0.3734714673913043,mmlu_offline:llama2-7b:high_school_statistics,validation,1.6694151740521193
216,0.24074074625968933,0.24074074625968933,0.5,0.2631655092592593,mmlu_offline:llama2-7b:high_school_statistics,test,14.26795220002532
22,0.5909090638160706,0.5909090638160706,0.5,0.08700284090909094,mmlu_offline:llama2-7b:high_school_us_history,validation,5.70302011910826
204,0.5980392098426819,0.5980392098426819,0.5,0.0941329656862745,mmlu_offline:llama2-7b:high_school_us_history,test,51.53477444918826
26,0.5384615659713745,0.5384615659713745,0.5,0.034555288461538436,mmlu_offline:llama2-7b:high_school_world_history,validation,4.762240001000464
237,0.4345991611480713,0.4345991611480713,0.5097087378640777,0.07433412447257386,mmlu_offline:llama2-7b:high_school_world_history,test,39.187093941960484
23,0.21739129722118378,0.21739129722118378,0.5,0.28651494565217395,mmlu_offline:llama2-7b:human_aging,validation,0.9763562520965934
223,0.340807169675827,0.340807169675827,0.5,0.16309907511210764,mmlu_offline:llama2-7b:human_aging,test,7.309072559699416
12,0.4166666567325592,0.4166666567325592,0.5,0.08723958333333331,mmlu_offline:llama2-7b:human_sexuality,validation,0.6031477400101721
131,0.47328245639801025,0.47328245639801025,0.5,0.030623807251908386,mmlu_offline:llama2-7b:human_sexuality,test,4.6441461918875575
13,0.23076923191547394,0.23076923191547394,0.5,0.2731370192307692,mmlu_offline:llama2-7b:international_law,validation,0.7610744438134134
121,0.4876033067703247,0.4958677589893341,0.5080645161290323,0.014139979338843001,mmlu_offline:llama2-7b:international_law,test,5.237203537020832
11,0.4545454680919647,0.4545454680919647,0.5,0.04936079545454547,mmlu_offline:llama2-7b:jurisprudence,validation,0.6545752012170851
108,0.37037035822868347,0.37037035822868347,0.5,0.13353587962962965,mmlu_offline:llama2-7b:jurisprudence,test,3.951958294957876
18,0.4444444477558136,0.4444444477558136,0.5,0.05946180555555558,mmlu_offline:llama2-7b:logical_fallacies,validation,0.9684887239709496
163,0.42944785952568054,0.42944785952568054,0.5,0.07445839723926378,mmlu_offline:llama2-7b:logical_fallacies,test,6.118153947871178
11,0.27272728085517883,0.27272728085517883,0.5,0.2311789772727273,mmlu_offline:llama2-7b:machine_learning,validation,0.8300114311277866
112,0.2857142984867096,0.2857142984867096,0.5,0.2181919642857143,mmlu_offline:llama2-7b:machine_learning,test,6.17354184621945
11,0.4545454680919647,0.4545454680919647,0.5,0.04936079545454547,mmlu_offline:llama2-7b:management,validation,0.5676042968407273
103,0.4563106894493103,0.4563106894493103,0.5,0.047595570388349495,mmlu_offline:llama2-7b:management,test,3.0843086568638682
25,0.2800000011920929,0.2800000011920929,0.5,0.22390624999999997,mmlu_offline:llama2-7b:marketing,validation,1.20839823782444
234,0.4572649598121643,0.4572649598121643,0.5,0.04664129273504275,mmlu_offline:llama2-7b:marketing,test,8.29536034213379
11,0.7272727489471436,0.7272727489471436,0.5,0.2233664772727273,mmlu_offline:llama2-7b:medical_genetics,validation,0.6176402098499238
100,0.4699999988079071,0.4699999988079071,0.5,0.03390625000000003,mmlu_offline:llama2-7b:medical_genetics,test,3.2847823877818882
86,0.5465116500854492,0.5465116500854492,0.5,0.042605377906976716,mmlu_offline:llama2-7b:miscellaneous,validation,2.784763331990689
783,0.5874840617179871,0.5874840617179871,0.5,0.08357778575989783,mmlu_offline:llama2-7b:miscellaneous,test,24.362637703772634
38,0.42105263471603394,0.42105263471603394,0.5,0.08285361842105265,mmlu_offline:llama2-7b:moral_disputes,validation,1.7099872431717813
346,0.41040462255477905,0.41040462255477905,0.5,0.09350162572254334,mmlu_offline:llama2-7b:moral_disputes,test,13.155447926837951
100,0.3499999940395355,0.3499999940395355,0.5,0.15390625000000002,mmlu_offline:llama2-7b:moral_scenarios,validation,6.44158266717568
895,0.3173184394836426,0.3173184394836426,0.5,0.18658781424581006,mmlu_offline:llama2-7b:moral_scenarios,test,55.46406413521618
33,0.3636363744735718,0.3636363744735718,0.5,0.14026988636363635,mmlu_offline:llama2-7b:nutrition,validation,1.694913329090923
306,0.38235294818878174,0.38235294818878174,0.5,0.12155330882352944,mmlu_offline:llama2-7b:nutrition,test,13.239188944920897
34,0.29411765933036804,0.29411765933036804,0.5,0.20978860294117646,mmlu_offline:llama2-7b:philosophy,validation,1.3945726919919252
311,0.3247588276863098,0.3247588276863098,0.5,0.17914740755627012,mmlu_offline:llama2-7b:philosophy,test,10.241284055169672
35,0.3142857253551483,0.3142857253551483,0.5,0.18962053571428572,mmlu_offline:llama2-7b:prehistory,validation,1.5477057341486216
324,0.4166666567325592,0.4166666567325592,0.5,0.08723958333333331,mmlu_offline:llama2-7b:prehistory,test,11.554444639012218
31,0.12903225421905518,0.12903225421905518,0.5,0.3748739919354839,mmlu_offline:llama2-7b:professional_accounting,validation,2.3869608878158033
282,0.152482271194458,0.152482271194458,0.5,0.3514239804964539,mmlu_offline:llama2-7b:professional_accounting,test,20.009617500938475
170,0.3117647171020508,0.3117647171020508,0.5,0.19214154411764706,mmlu_offline:llama2-7b:professional_law,validation,27.06186009896919
1534,0.2777053415775299,0.2777053415775299,0.5,0.2262009044980443,mmlu_offline:llama2-7b:professional_law,test,248.17747177975252
31,0.4193548262119293,0.4193548262119293,0.5,0.08455141129032256,mmlu_offline:llama2-7b:professional_medicine,validation,3.9779193438589573
272,0.27941176295280457,0.27941176295280457,0.5,0.22449448529411764,mmlu_offline:llama2-7b:professional_medicine,test,34.907416011206806
69,0.3913043439388275,0.3913043439388275,0.5,0.11260190217391303,mmlu_offline:llama2-7b:professional_psychology,validation,3.4692805847153068
612,0.30882352590560913,0.30882352590560913,0.5,0.19508272058823528,mmlu_offline:llama2-7b:professional_psychology,test,26.39820622932166
12,0.4166666567325592,0.4166666567325592,0.5,0.08723958333333331,mmlu_offline:llama2-7b:public_relations,validation,0.6422706227749586
110,0.3181818127632141,0.3181818127632141,0.5,0.18572443181818182,mmlu_offline:llama2-7b:public_relations,test,4.1370261437259614
27,0.5925925970077515,0.5925925970077515,0.5,0.08868634259259256,mmlu_offline:llama2-7b:security_studies,validation,1.6757992869243026
245,0.5306122303009033,0.5306122303009033,0.5,0.026705994897959218,mmlu_offline:llama2-7b:security_studies,test,12.994361364748329
22,0.3181818127632141,0.3181818127632141,0.5,0.18572443181818182,mmlu_offline:llama2-7b:sociology,validation,0.9122392530553043
201,0.3781094551086426,0.3781094551086426,0.5,0.1257967972636816,mmlu_offline:llama2-7b:sociology,test,6.921403630170971
11,0.6363636255264282,0.6363636255264282,0.5,0.13245738636363635,mmlu_offline:llama2-7b:us_foreign_policy,validation,0.6332040759734809
100,0.5799999833106995,0.5799999833106995,0.5,0.07609374999999996,mmlu_offline:llama2-7b:us_foreign_policy,test,3.5740308091044426
18,0.5,0.5,0.5,0.00390625,mmlu_offline:llama2-7b:virology,validation,1.046076798811555
166,0.3313252925872803,0.3313252925872803,0.5,0.1725809487951807,mmlu_offline:llama2-7b:virology,test,5.698967306874692
19,0.7368420958518982,0.7368420958518982,0.5,0.23293585526315785,mmlu_offline:llama2-7b:world_religions,validation,0.8426241730339825
171,0.5847952961921692,0.5847952961921692,0.5,0.08088907163742687,mmlu_offline:llama2-7b:world_religions,test,5.225038811098784
