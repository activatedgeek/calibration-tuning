N,logits_ece,acc,unc_acc,unc_auroc,unc_ece,dataset,split,ts
11,0.435711451552131,0.27272728085517883,0.7272727489471436,0.08333333333333333,0.17258521101691504,mmlu:abstract_algebra,validation,5.315333025995642
100,0.34320432335138323,0.2199999988079071,0.7799999713897705,0.6858974358974359,0.06394529640674596,mmlu:abstract_algebra,test,6.379454510984942
14,0.13757130077907015,0.7857142686843872,0.2142857164144516,0.6666666666666666,0.6166294600282396,mmlu:anatomy,validation,1.0884719050955027
135,0.23850399101222003,0.43703705072402954,0.5629629492759705,0.36953612845673506,0.2888020705293726,mmlu:anatomy,test,8.773307445924729
16,0.2690412849187851,0.3125,0.6875,0.5,0.12109375,mmlu:astronomy,validation,1.5161820349749178
152,0.24981828916229698,0.3618420958518982,0.6381579041481018,0.5,0.17043585526315785,mmlu:astronomy,test,13.394986503059044
11,0.2799422686750239,0.5454545617103577,0.4545454680919647,0.5,0.16654829545454547,mmlu:business_ethics,validation,1.202289653941989
100,0.23370743960142135,0.4699999988079071,0.5299999713897705,0.5,0.09109374999999997,mmlu:business_ethics,test,8.966576041886583
29,0.31901082602040526,0.48275861144065857,0.517241358757019,0.5285714285714285,0.21134157838492554,mmlu:clinical_knowledge,validation,2.210209285840392
265,0.2069326614433864,0.501886785030365,0.501886785030365,0.5496411483253588,0.23256190385458606,mmlu:clinical_knowledge,test,18.495258323848248
16,0.342888206243515,0.25,0.75,0.8541666666666666,0.1452636681497097,mmlu:college_biology,validation,1.3606179780326784
144,0.1770010516047478,0.4722222089767456,0.5277777910232544,0.5439241486068112,0.1819932605657313,mmlu:college_biology,test,11.429197265068069
8,0.40616875141859055,0.25,0.75,0.3333333333333333,0.19970701634883883,mmlu:college_chemistry,validation,0.8221615990623832
100,0.36922240763902664,0.17000000178813934,0.8299999833106995,0.5871722182849043,0.12867189288139347,mmlu:college_chemistry,test,8.424362772144377
11,0.3835678913376548,0.1818181872367859,0.8181818127632141,0.5,0.14240056818181823,mmlu:college_computer_science,validation,1.4009062629193068
100,0.35432183086872104,0.20000000298023224,0.800000011920929,0.5,0.12421875000000004,mmlu:college_computer_science,test,11.218268766999245
11,0.169007192958485,0.4545454680919647,0.5454545617103577,0.5,0.09907670454545459,mmlu:college_mathematics,validation,1.1252907749731094
100,0.22035382628440858,0.25999999046325684,0.7400000095367432,0.5,0.09546874999999999,mmlu:college_mathematics,test,8.463520703138784
22,0.3394288732246919,0.3181818127632141,0.6363636255264282,0.6190476190476191,0.12056107412685046,mmlu:college_medicine,validation,2.0054146030452102
173,0.33964149042361047,0.34682080149650574,0.6242774724960327,0.6187315634218289,0.10420430188923212,mmlu:college_medicine,test,17.36354275001213
11,0.34916770729151636,0.4545454680919647,0.5454545617103577,0.36666666666666664,0.13991476189006458,mmlu:college_physics,validation,1.0942744431085885
102,0.32260790233518566,0.23529411852359772,0.7647058963775635,0.49118589743589747,0.09857535712859211,mmlu:college_physics,test,7.786034499993548
11,0.43841116536747315,0.3636363744735718,0.4545454680919647,0.46428571428571436,0.15696020559831098,mmlu:computer_security,validation,1.0603218260221183
100,0.26215096294879914,0.49000000953674316,0.44999998807907104,0.5584233693477391,0.09707033693790437,mmlu:computer_security,test,6.858205643948168
26,0.38970727301560915,0.3461538553237915,0.692307710647583,0.7320261437908496,0.14212740843112656,mmlu:conceptual_physics,validation,1.6938206418417394
235,0.3170571749514722,0.4170212745666504,0.5829787254333496,0.5199240280053627,0.1131981372833252,mmlu:conceptual_physics,test,13.171001655049622
12,0.5215101192394892,0.0833333358168602,0.0833333358168602,0.5,0.4557291666666667,mmlu:econometrics,validation,1.2919315621256828
114,0.26633992477467183,0.3245614171028137,0.3245614171028137,0.5,0.21450109649122806,mmlu:econometrics,test,10.314464760944247
16,0.3756761681288481,0.1875,0.8125,0.46153846153846145,0.12280274182558061,mmlu:electrical_engineering,validation,1.2402893409598619
145,0.30078245134189213,0.2896551787853241,0.7103448510169983,0.6219371243643089,0.06201509936102505,mmlu:electrical_engineering,test,10.082645788090304
41,0.2963081983531394,0.17073170840740204,0.8292682766914368,0.7121848739495799,0.11394817509302274,mmlu:elementary_mathematics,validation,3.5833539948798716
378,0.3342761815225006,0.14814814925193787,0.8518518805503845,0.4708019077196096,0.15545429391835736,mmlu:elementary_mathematics,test,30.44095060415566
14,0.40794134778635843,0.2142857164144516,0.7857142686843872,0.5,0.1685267857142857,mmlu:formal_logic,validation,1.4071700309868902
126,0.3623097070625851,0.1587301641702652,0.841269850730896,0.5,0.22408234126984128,mmlu:formal_logic,test,11.39411143399775
10,0.3196156546473503,0.5,0.5,0.44,0.21640625000000002,mmlu:global_facts,validation,0.9277061598841101
100,0.1995510682463646,0.3799999952316284,0.6200000047683716,0.5566638370118846,0.09980468928813932,mmlu:global_facts,test,6.973729062126949
32,0.2674458120018244,0.34375,0.65625,0.41125541125541126,0.16967772319912908,mmlu:high_school_biology,validation,2.7059942630585283
310,0.19146540328379597,0.448387086391449,0.551612913608551,0.5867726871134671,0.2071824665992491,mmlu:high_school_biology,test,25.12381336814724
22,0.29074718735434796,0.3181818127632141,0.7272727489471436,0.6142857142857143,0.1489701731638475,mmlu:high_school_chemistry,validation,1.9680443359538913
203,0.3154352559831929,0.2857142984867096,0.7142857313156128,0.5240190249702735,0.12871380979791644,mmlu:high_school_chemistry,test,16.141530405031517
9,0.3600997924804687,0.3333333432674408,0.6666666865348816,0.5,0.06901041666666663,mmlu:high_school_computer_science,validation,1.328463674057275
100,0.31326128482818605,0.3100000023841858,0.6899999976158142,0.5,0.09234374999999995,mmlu:high_school_computer_science,test,12.270227210130543
18,0.4509513485762808,0.2222222238779068,0.7777777910232544,0.5,0.2621527777777778,mmlu:high_school_european_history,validation,6.78024871298112
165,0.3561936970913049,0.34545454382896423,0.6545454263687134,0.5,0.13892045454545454,mmlu:high_school_european_history,test,60.39078418794088
22,0.11429377712986687,0.6818181872367859,0.3181818127632141,0.34285714285714286,0.4319957332177595,mmlu:high_school_geography,validation,1.5939177740365267
198,0.22402376779402147,0.5101010203361511,0.4898989796638489,0.5991119730529755,0.2516571843262875,mmlu:high_school_geography,test,13.111897313967347
21,0.4873556877885546,0.3333333432674408,0.6666666865348816,0.7142857142857143,0.13690476474307833,mmlu:high_school_government_and_politics,validation,1.7634088690392673
193,0.11746774065679837,0.6994818449020386,0.30051812529563904,0.5597062579821201,0.3424951424870466,mmlu:high_school_government_and_politics,test,14.648488597944379
43,0.28634950310684915,0.44186046719551086,0.5581395626068115,0.5997807017543859,0.10092660992644553,mmlu:high_school_macroeconomics,validation,3.0837381079327315
390,0.24269767533510161,0.4076923131942749,0.5923076868057251,0.5317732581883525,0.029887815163685754,mmlu:high_school_macroeconomics,test,26.138445736840367
29,0.3373511238344785,0.2068965584039688,0.7931034564971924,0.576086956521739,0.13806575742261165,mmlu:high_school_mathematics,validation,2.4535899369511753
270,0.2797751958723422,0.20370370149612427,0.7962962985038757,0.5241014799154333,0.12604168609336566,mmlu:high_school_mathematics,test,21.038346532033756
26,0.3587234341181242,0.38461539149284363,0.42307692766189575,0.5187499999999999,0.10051084023255569,mmlu:high_school_microeconomics,validation,1.9391540070064366
238,0.30095428666647744,0.34873950481414795,0.4075630307197571,0.5446949086669257,0.11894369676333516,mmlu:high_school_microeconomics,test,16.235481487121433
17,0.2866739718353047,0.1764705926179886,0.8235294222831726,0.5476190476190476,0.2532169222831726,mmlu:high_school_physics,validation,1.7058879861142486
151,0.26189394265610655,0.22516556084156036,0.7682119011878967,0.5409753645047763,0.19435535637748164,mmlu:high_school_physics,test,12.877295742044225
60,0.17472776869932813,0.6666666865348816,0.3333333432674408,0.5725,0.3354166646798452,mmlu:high_school_psychology,validation,4.9818462720140815
545,0.13937156069169354,0.5834862589836121,0.4165137708187103,0.5010666888316294,0.24912555786447785,mmlu:high_school_psychology,test,43.90142668108456
23,0.25763335435286816,0.260869562625885,0.260869562625885,0.5,0.2547554347826087,mmlu:high_school_statistics,validation,2.6307107070460916
216,0.3012111377384928,0.24074074625968933,0.24074074625968933,0.5,0.2748842592592593,mmlu:high_school_statistics,test,23.856522988993675
22,0.25062222101471643,0.5,0.5,0.5,0.0390625,mmlu:high_school_us_history,validation,6.389217953197658
204,0.1585016294437296,0.5245097875595093,0.47549018263816833,0.5,0.06357230392156865,mmlu:high_school_us_history,test,57.925422522006556
26,0.32303870068146634,0.42307692766189575,0.5769230723381042,0.5,0.026141826923076872,mmlu:high_school_world_history,validation,5.593135189032182
237,0.13938282662806128,0.5569620132446289,0.4430379867553711,0.5,0.10774327531645572,mmlu:high_school_world_history,test,46.587168575963005
23,0.24969118056089978,0.695652186870575,0.30434781312942505,0.5803571428571428,0.6007133229919103,mmlu:human_aging,validation,1.4699810170568526
223,0.14104987300030322,0.5560538172721863,0.4439461827278137,0.5305474095796675,0.4162871573003418,mmlu:human_aging,test,12.766587839927524
12,0.22208079198996222,0.4166666567325592,0.5833333134651184,0.4285714285714286,0.14843750000000003,mmlu:human_sexuality,validation,0.9695694311521947
131,0.18937305070971716,0.5038167834281921,0.49618321657180786,0.5347319347319348,0.21415792576229298,mmlu:human_sexuality,test,8.527617374202237
13,0.13257774481406578,0.9230769276618958,0.07692307978868484,0.5,0.4543269230769231,mmlu:international_law,validation,1.4103591139428318
121,0.18286301799056942,0.6198347210884094,0.3801652789115906,0.5,0.15108471074380164,mmlu:international_law,test,11.23586077382788
11,0.28368493914604187,0.3636363744735718,0.4545454680919647,0.3928571428571429,0.06001423163847491,mmlu:jurisprudence,validation,0.9728741119615734
108,0.16812362604671055,0.5370370149612427,0.5,0.46172413793103445,0.012188971042633057,mmlu:jurisprudence,test,7.446425358066335
18,0.2873121996720632,0.6111111044883728,0.3888888955116272,0.5909090909090909,0.2450086772441864,mmlu:logical_fallacies,validation,1.518942807102576
163,0.23609467463259318,0.46625766158103943,0.5214723944664001,0.46627344222625533,0.08727951722642396,mmlu:logical_fallacies,test,12.051906595006585
11,0.4027558429674669,0.09090909361839294,0.09090909361839294,0.5,0.47159090909090906,mmlu:machine_learning,validation,1.232596939895302
112,0.38370391460401665,0.1875,0.1875,0.5,0.375,mmlu:machine_learning,test,10.401896168943495
11,0.20211967013098978,0.7272727489471436,0.27272728085517883,0.625,0.6455966071649032,mmlu:management,validation,0.8068578820675611
103,0.1565075264972391,0.6407766938209534,0.35922330617904663,0.4252661752661753,0.47496967813343677,mmlu:management,test,5.573219920042902
25,0.23161819458007815,0.7200000286102295,0.4000000059604645,0.8174603174603174,0.12328127622604368,mmlu:marketing,validation,1.9487354189623147
234,0.06871425494169578,0.7264957427978516,0.3461538553237915,0.5657169117647058,0.19102227789723975,mmlu:marketing,test,15.963908551959321
11,0.2477659528905695,0.7272727489471436,0.27272728085517883,0.3125,0.3217329328710382,mmlu:medical_genetics,validation,0.9405250342097133
100,0.24025268971920016,0.44999998807907104,0.550000011920929,0.584040404040404,0.09195313036441802,mmlu:medical_genetics,test,6.39991357200779
86,0.15050928017427756,0.5813953280448914,0.41860464215278625,0.5438888888888889,0.34511263495267824,mmlu:miscellaneous,validation,4.966622814070433
783,0.1146790961500633,0.6475095748901367,0.351213276386261,0.5723029757310694,0.4015854599375378,mmlu:miscellaneous,test,45.04271607194096
38,0.3289902782753894,0.4736842215061188,0.5263158082962036,0.45555555555555555,0.2301603853702545,mmlu:moral_disputes,validation,3.0037921180482954
346,0.29000350832939154,0.4566473960876465,0.5433526039123535,0.4653077026663076,0.1346753008792855,mmlu:moral_disputes,test,26.21387525717728
100,0.35280446857213976,0.23999999463558197,0.7599999904632568,0.5,0.10765625000000001,mmlu:moral_scenarios,validation,9.245734916068614
895,0.35231019854545587,0.24581006169319153,0.7541899681091309,0.5,0.10184619413407825,mmlu:moral_scenarios,test,80.30638775601983
33,0.305375851464994,0.39393940567970276,0.39393940567970276,0.5,0.14512310606060608,mmlu:nutrition,validation,3.146930731832981
306,0.28601749708839497,0.34967321157455444,0.34967321157455444,0.5,0.18938929738562094,mmlu:nutrition,test,27.52335509005934
34,0.3604088644771015,0.5,0.44117647409439087,0.3062283737024222,0.15659467262380267,mmlu:philosophy,validation,2.3289589548949152
311,0.22598661391298108,0.5691318511962891,0.4244372844696045,0.5388523484273546,0.17295517549637424,mmlu:philosophy,test,19.048950552009046
35,0.33186736617769513,0.37142857909202576,0.6285714507102966,0.5524475524475525,0.054910727909633084,mmlu:prehistory,validation,3.1169157119002193
324,0.23025649224902373,0.4783950746059418,0.5216049551963806,0.5226760832219889,0.16349584600071848,mmlu:prehistory,test,26.850410548970103
31,0.3193904180680552,0.25806450843811035,0.25806450843811035,0.5,0.25365423387096775,mmlu:professional_accounting,validation,3.304924214957282
282,0.2608700836381168,0.304964542388916,0.304964542388916,0.5,0.2067542109929078,mmlu:professional_accounting,test,28.559284305898473
170,0.3461448287262636,0.24705882370471954,0.24705882370471954,0.5,0.2724724264705882,mmlu:professional_law,validation,37.345991770969704
1534,0.3225551533768976,0.2737939953804016,0.2737939953804016,0.5,0.24573724739243807,mmlu:professional_law,test,337.694760507904
31,0.22453524124237798,0.35483869910240173,0.35483869910240173,0.5,0.16859879032258063,mmlu:professional_medicine,validation,4.842090243007988
272,0.19499033338883343,0.3345588147640228,0.3345588147640228,0.5,0.18887867647058826,mmlu:professional_medicine,test,41.980706456815824
69,0.301143209139506,0.3913043439388275,0.6086956262588501,0.5,0.016304347826086918,mmlu:professional_psychology,validation,6.257128627039492
612,0.2958036736140844,0.3741829991340637,0.6258170008659363,0.5,0.0008169934640522847,mmlu:professional_psychology,test,52.576317535946146
12,0.379655584692955,0.5,0.5,0.6111111111111112,0.09147136410077414,mmlu:public_relations,validation,1.0773152280598879
110,0.19797793030738833,0.5454545617103577,0.44545453786849976,0.589,0.1455966098742051,mmlu:public_relations,test,7.786607493180782
27,0.3144044489772232,0.48148149251937866,0.5185185074806213,0.5,0.05179398148148151,mmlu:security_studies,validation,4.564663738012314
245,0.32743740325071374,0.3510203957557678,0.6489796042442322,0.5,0.0786670918367347,mmlu:security_studies,test,39.75603258609772
22,0.14946672997691415,0.7727272510528564,0.22727273404598236,0.3529411764705882,0.43892044641754846,mmlu:sociology,validation,1.7369168309960514
201,0.15348214756197004,0.6815920472145081,0.31840795278549194,0.5348996350364963,0.34332246033113395,mmlu:sociology,test,14.596596793970093
11,0.08332994851199065,0.7272727489471436,0.27272728085517883,0.7291666666666667,0.4183238582177595,mmlu:us_foreign_policy,validation,0.9808558719232678
100,0.13942405581474307,0.699999988079071,0.3400000035762787,0.5292857142857144,0.29757813692092894,mmlu:us_foreign_policy,test,7.002037326106802
18,0.21940550456444424,0.4444444477558136,0.5,0.1875,0.3331163061989678,mmlu:virology,validation,1.5749187089968473
166,0.3263759720756348,0.40963855385780334,0.5903614163398743,0.5826830732292918,0.08318431190697542,mmlu:virology,test,10.588709141826257
19,0.07844283078846179,0.7894737124443054,0.21052631735801697,0.6499999999999999,0.6130756516205638,mmlu:world_religions,validation,1.1666413438506424
171,0.08099033522327041,0.6549707651138306,0.34502923488616943,0.5516797820823244,0.48003470688535455,mmlu:world_religions,test,8.78639402706176
