N,fuzzy_gpt-3.5-turbo-1106_acc,fuzzy_gpt-3.5-turbo-1106_unc_acc,fuzzy_gpt-3.5-turbo-1106_unc_auroc,fuzzy_gpt-3.5-turbo-1106_unc_ece,dataset,split,ts
11,0.1818181872367859,0.8181818127632141,0.4444444444444444,0.09110966053876013,mmlu:abstract_algebra,validation,3.3948322501964867
100,0.2199999988079071,0.7699999809265137,0.6672494172494172,0.1013362741470337,mmlu:abstract_algebra,test,4.838276618858799
14,0.2857142984867096,0.6428571343421936,0.75,0.27194105727331974,mmlu:anatomy,validation,0.8158144648186862
135,0.4148148000240326,0.6518518328666687,0.6670433996383364,0.26923224087114683,mmlu:anatomy,test,5.119104787008837
16,0.4375,0.625,0.746031746031746,0.19212022051215175,mmlu:astronomy,validation,1.0444606081582606
152,0.5065789222717285,0.7302631735801697,0.7942857142857143,0.11961391568183895,mmlu:astronomy,test,7.281813055975363
11,0.6363636255264282,0.8181818127632141,0.8571428571428571,0.14871492169120099,mmlu:business_ethics,validation,0.9482091441750526
100,0.30000001192092896,0.6499999761581421,0.8364285714285714,0.18723285555839544,mmlu:business_ethics,test,5.8488943530246615
29,0.24137930572032928,0.5862069129943848,0.7564935064935066,0.2612150085383448,mmlu:clinical_knowledge,validation,1.4782192190177739
265,0.35849055647850037,0.6075471639633179,0.7570588235294118,0.2266758959248381,mmlu:clinical_knowledge,test,11.260859216097742
16,0.3125,0.8125,0.9090909090909091,0.07128617912530898,mmlu:college_biology,validation,1.0100841750390828
144,0.2986111044883728,0.6736111044883728,0.7159797375086345,0.19256841515501338,mmlu:college_biology,test,7.480488806031644
8,0.125,0.875,0.5714285714285714,0.2419298067688942,mmlu:college_chemistry,validation,0.6000975877977908
100,0.14000000059604645,0.800000011920929,0.7724252491694351,0.08240074396133426,mmlu:college_chemistry,test,6.010829386999831
11,0.0,0.6363636255264282,,0.27557051181793213,mmlu:college_computer_science,validation,1.2465308199170977
100,0.17000000178813934,0.5199999809265137,0.5421686746987953,0.28467070758342744,mmlu:college_computer_science,test,9.398303769063205
11,0.0,1.0,,0.06559181755239313,mmlu:college_mathematics,validation,1.0562948279548436
100,0.1599999964237213,0.8399999737739563,0.6915922619047619,0.10175818502902989,mmlu:college_mathematics,test,6.874378697946668
22,0.3636363744735718,0.7727272510528564,0.7321428571428572,0.15563471750779584,mmlu:college_medicine,validation,1.3517033450771123
173,0.36994218826293945,0.6936416029930115,0.6471473623853211,0.1700953062559139,mmlu:college_medicine,test,12.55578284105286
11,0.27272728085517883,0.8181818127632141,0.8333333333333334,0.24845706874674015,mmlu:college_physics,validation,0.9897887869738042
102,0.14705882966518402,0.8333333134651184,0.8095785440613027,0.05238954752099279,mmlu:college_physics,test,6.333520591957495
11,0.7272727489471436,0.9090909361839294,0.9166666666666667,0.18982048468156296,mmlu:computer_security,validation,0.8420991690363735
100,0.4300000071525574,0.5699999928474426,0.5879232966136271,0.25895959019660947,mmlu:computer_security,test,4.700359944021329
26,0.3076923191547394,0.7307692170143127,0.7430555555555556,0.09872638491483836,mmlu:conceptual_physics,validation,1.2921713469550014
235,0.4127659499645233,0.6170212626457214,0.6214701927386822,0.15012556441286778,mmlu:conceptual_physics,test,9.1626335689798
12,0.0833333358168602,0.25,0.9090909090909091,0.565347303946813,mmlu:econometrics,validation,0.9856032668612897
114,0.14912280440330505,0.34210526943206787,0.6315949060036385,0.42978964719855994,mmlu:econometrics,test,7.918109356891364
16,0.125,0.9375,1.0,0.0926302932202816,mmlu:electrical_engineering,validation,0.9255610380787402
145,0.1862068921327591,0.800000011920929,0.6381042059008161,0.12315305594740239,mmlu:electrical_engineering,test,7.564294841140509
41,0.26829269528388977,0.7560975551605225,0.6924242424242425,0.18457451826188626,mmlu:elementary_mathematics,validation,2.5008552889339626
378,0.32275131344795227,0.6719576716423035,0.5277439805327869,0.23603492659866496,mmlu:elementary_mathematics,test,20.246406056918204
14,0.5,0.6428571343421936,0.8163265306122449,0.2974955184119088,mmlu:formal_logic,validation,1.0979462950490415
126,0.30158731341362,0.6904761791229248,0.7163576555023923,0.10601373513539632,mmlu:formal_logic,test,7.500921434955671
10,0.30000001192092896,0.699999988079071,0.3333333333333333,0.2460221588611603,mmlu:global_facts,validation,0.7078317061532289
100,0.17000000178813934,0.7799999713897705,0.5354358610914246,0.1049980264902115,mmlu:global_facts,test,4.770787273999304
32,0.15625,0.8125,0.43333333333333335,0.15753729082643983,mmlu:high_school_biology,validation,1.7983233761042356
310,0.41290321946144104,0.603225827217102,0.6180245535714285,0.32528103647693507,mmlu:high_school_biology,test,15.786111532943323
22,0.13636364042758942,0.7727272510528564,0.368421052631579,0.1878415562889793,mmlu:high_school_chemistry,validation,1.4269434891175479
203,0.16748768091201782,0.8423645496368408,0.6626348764357815,0.1134011689665282,mmlu:high_school_chemistry,test,11.804214471951127
9,0.2222222238779068,0.7777777910232544,0.7857142857142857,0.24583326445685488,mmlu:high_school_computer_science,validation,1.0289145910646766
100,0.3700000047683716,0.6600000262260437,0.5596310596310596,0.22010200798511503,mmlu:high_school_computer_science,test,8.313997796969488
18,0.7222222089767456,0.6666666865348816,0.6923076923076923,0.24577980571322972,mmlu:high_school_european_history,validation,5.830615710001439
165,0.6727272868156433,0.8303030133247375,0.824074074074074,0.08289532227949663,mmlu:high_school_european_history,test,52.22178455395624
22,0.4545454680919647,0.6818181872367859,0.8041666666666666,0.18936446580019867,mmlu:high_school_geography,validation,1.1666304979007691
198,0.3636363744735718,0.7070707082748413,0.6880511463844797,0.19387886771047957,mmlu:high_school_geography,test,8.15861223009415
21,0.4285714328289032,0.523809552192688,0.6666666666666666,0.42988521144503633,mmlu:high_school_government_and_politics,validation,1.178557313978672
193,0.48704662919044495,0.5492228269577026,0.6736514076939608,0.364364182393168,mmlu:high_school_government_and_politics,test,8.548404308967292
43,0.41860464215278625,0.604651153087616,0.64,0.27355240389358165,mmlu:high_school_macroeconomics,validation,1.9781878378707916
390,0.34358975291252136,0.6358974575996399,0.7150040811567165,0.20123972388414238,mmlu:high_school_macroeconomics,test,16.232701763045043
29,0.03448275849223137,0.9655172228813171,0.8571428571428572,0.012850981334160072,mmlu:high_school_mathematics,validation,2.0771671899128705
270,0.08148147910833359,0.9111111164093018,0.6107954545454546,0.05994565464832159,mmlu:high_school_mathematics,test,16.618900595931336
26,0.3076923191547394,0.5384615659713745,0.5972222222222222,0.3666638090060308,mmlu:high_school_microeconomics,validation,1.3791253499221057
238,0.32773110270500183,0.5378151535987854,0.6620592948717949,0.2674598305666146,mmlu:high_school_microeconomics,test,9.870122425025329
17,0.23529411852359772,0.4117647111415863,0.4326923076923077,0.5552247902926277,mmlu:high_school_physics,validation,1.294147728011012
151,0.17880794405937195,0.503311276435852,0.5813918757467145,0.28751986745177516,mmlu:high_school_physics,test,9.32170241093263
60,0.5,0.6666666865348816,0.7977777777777778,0.21229629516601564,mmlu:high_school_psychology,validation,3.3247310239821672
545,0.5064220428466797,0.5761467814445496,0.672040838316901,0.27369120044445777,mmlu:high_school_psychology,test,28.442314615938812
23,0.1304347813129425,0.739130437374115,0.8333333333333334,0.23000090018562647,mmlu:high_school_statistics,validation,1.891360007924959
216,0.24074074625968933,0.6388888955116272,0.6250586303939962,0.18205157777777425,mmlu:high_school_statistics,test,16.742687206016853
22,0.5909090638160706,0.6818181872367859,0.858974358974359,0.25007427551529626,mmlu:high_school_us_history,validation,5.5402525898534805
204,0.5980392098426819,0.7254902124404907,0.7707916833266694,0.15897116035807368,mmlu:high_school_us_history,test,50.11286466103047
26,0.5384615659713745,0.6153846383094788,0.5238095238095237,0.30983011539165795,mmlu:high_school_world_history,validation,5.012807297986001
237,0.4345991611480713,0.6202531456947327,0.7344950007245328,0.2731944214442611,mmlu:high_school_world_history,test,40.3696850030683
23,0.21739129722118378,0.6521739363670349,0.7277777777777779,0.19718122741450433,mmlu:human_aging,validation,0.9939264699351043
223,0.340807169675827,0.7085201740264893,0.7581453634085213,0.1278119592388649,mmlu:human_aging,test,8.948068068129942
12,0.4166666567325592,0.6666666865348816,0.6285714285714286,0.23972766101360324,mmlu:human_sexuality,validation,0.769475223030895
131,0.47328245639801025,0.6488549709320068,0.7020804114071997,0.19394226338117176,mmlu:human_sexuality,test,5.767065986059606
13,0.23076923191547394,0.3076923191547394,0.7833333333333333,0.5762411722770104,mmlu:international_law,validation,0.9245398589409888
121,0.4876033067703247,0.5289255976676941,0.7643521049753963,0.3684888501797826,mmlu:international_law,test,6.555487437173724
11,0.4545454680919647,0.5454545617103577,0.7666666666666667,0.4036972685293718,mmlu:jurisprudence,validation,0.7389215019065887
108,0.37037035822868347,0.6388888955116272,0.6534926470588235,0.30168225147106026,mmlu:jurisprudence,test,4.760415229946375
18,0.4444444477558136,0.5,0.5625,0.450421906179852,mmlu:logical_fallacies,validation,1.3559221518225968
163,0.42944785952568054,0.5276073813438416,0.729416282642089,0.3070455303221392,mmlu:logical_fallacies,test,10.446925272000954
11,0.27272728085517883,0.7272727489471436,0.7916666666666666,0.371219049800526,mmlu:machine_learning,validation,0.9015207339543849
112,0.2857142984867096,0.5803571343421936,0.70625,0.22126706210630284,mmlu:machine_learning,test,7.211251037195325
11,0.4545454680919647,0.5454545617103577,0.6166666666666667,0.41915823654694995,mmlu:management,validation,0.6223759369459003
103,0.4563106894493103,0.5631067752838135,0.6105623100303951,0.3965888596275478,mmlu:management,test,3.737489121966064
25,0.2800000011920929,0.6800000071525574,0.6904761904761905,0.1301738119125366,mmlu:marketing,validation,1.3856631789822131
234,0.4572649598121643,0.6709401607513428,0.7889837368459783,0.07382513519026272,mmlu:marketing,test,10.259471993893385
11,0.7272727489471436,0.7272727489471436,0.5833333333333334,0.17886399139057507,mmlu:medical_genetics,validation,0.6620003539137542
100,0.4699999988079071,0.6499999761581421,0.6692091529506221,0.14876038551330564,mmlu:medical_genetics,test,3.8567729890346527
86,0.5465116500854492,0.6860465407371521,0.7424986361156575,0.21419040338937628,mmlu:miscellaneous,validation,3.4586777228396386
783,0.5874840617179871,0.725415050983429,0.7860445551218199,0.18302441853673068,mmlu:miscellaneous,test,29.9451775951311
38,0.42105263471603394,0.5,0.6860795454545454,0.30178489810542053,mmlu:moral_disputes,validation,1.9678256718907505
346,0.41040462255477905,0.6387283205986023,0.7298916045291356,0.1635315264925102,mmlu:moral_disputes,test,16.504993855953217
100,0.3499999940395355,0.6499999761581421,0.507912087912088,0.1957857894897461,mmlu:moral_scenarios,validation,7.388591296970844
895,0.3173184394836426,0.6826815605163574,0.5071632742444848,0.16546863770351727,mmlu:moral_scenarios,test,63.82773163798265
33,0.3636363744735718,0.6666666865348816,0.7321428571428571,0.2919556895891825,mmlu:nutrition,validation,1.989100705133751
306,0.38235294818878174,0.6666666865348816,0.7172025505358839,0.16979453201387446,mmlu:nutrition,test,16.045981973176822
34,0.29411765933036804,0.4117647111415863,0.65,0.5046671874382918,mmlu:philosophy,validation,1.6338325010146946
311,0.3247588276863098,0.40192925930023193,0.6676331918906175,0.5002927651742647,mmlu:philosophy,test,12.594663579016924
35,0.3142857253551483,0.6285714507102966,0.6969696969696969,0.2242518390927996,mmlu:prehistory,validation,1.7932045250199735
324,0.4166666567325592,0.6574074029922485,0.6828140309621792,0.1539499244572204,mmlu:prehistory,test,15.117514960933477
31,0.12903225421905518,0.8387096524238586,0.6805555555555556,0.0781002909906449,mmlu:professional_accounting,validation,2.5347170720342547
282,0.152482271194458,0.8191489577293396,0.6462975576530116,0.08166177708206446,mmlu:professional_accounting,test,21.97164388699457
170,0.3117647171020508,0.6176470518112183,0.6142557651991615,0.19734730895827798,mmlu:professional_law,validation,27.12921059317887
1534,0.2777053415775299,0.650586724281311,0.6019876781749461,0.1452434257624824,mmlu:professional_law,test,248.17707849899307
31,0.4193548262119293,0.6129032373428345,0.6410256410256411,0.2845429528144098,mmlu:professional_medicine,validation,4.096286009065807
272,0.27941176295280457,0.6911764740943909,0.6163063909774436,0.18509550239233408,mmlu:professional_medicine,test,35.63812958798371
69,0.3913043439388275,0.37681159377098083,0.6618165784832452,0.45922049836836,mmlu:professional_psychology,validation,3.8588537559844553
612,0.30882352590560913,0.4624182879924774,0.6853352846260647,0.38153483840375163,mmlu:professional_psychology,test,31.957630753982812
12,0.4166666567325592,0.8333333134651184,0.942857142857143,0.09665300945440929,mmlu:public_relations,validation,0.7908839900046587
110,0.3181818127632141,0.699999988079071,0.7146666666666667,0.15794197700240395,mmlu:public_relations,test,5.022633384913206
27,0.5925925970077515,0.6296296119689941,0.7301136363636365,0.2318274113867018,mmlu:security_studies,validation,1.9964261748827994
245,0.5306122303009033,0.6081632375717163,0.6931103678929765,0.24317407024149992,mmlu:security_studies,test,15.230094929924235
22,0.3181818127632141,0.7727272510528564,0.7476190476190476,0.17294325069947677,mmlu:sociology,validation,1.0977679209318012
201,0.3781094551086426,0.6716417670249939,0.729578947368421,0.16567406606911428,mmlu:sociology,test,8.217270012944937
11,0.6363636255264282,0.5454545617103577,0.5892857142857143,0.3119997111233798,mmlu:us_foreign_policy,validation,0.672621845966205
100,0.5799999833106995,0.6499999761581421,0.6892446633825944,0.20147834539413456,mmlu:us_foreign_policy,test,4.214657744858414
18,0.5,0.5,0.7098765432098766,0.4614941477775574,mmlu:virology,validation,1.0409839230123907
166,0.3313252925872803,0.6566265225410461,0.5954135954135954,0.30002630331430097,mmlu:virology,test,6.8213457660749555
19,0.7368420958518982,0.7894737124443054,0.9642857142857143,0.14955832769996238,mmlu:world_religions,validation,0.8655955200083554
171,0.5847952961921692,0.6432748436927795,0.7907042253521126,0.25293827091741283,mmlu:world_religions,test,6.072278757113963
