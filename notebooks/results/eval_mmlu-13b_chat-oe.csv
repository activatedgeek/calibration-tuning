method,category,split,acc,ece
raw_13b_chat,mmlu:high_school_biology,test,0.4322580695152282,0.35447730614293
raw_13b_chat,mmlu:high_school_biology,val,0.46875,0.3857219573110342
raw_13b_chat,mmlu:human_aging,test,0.4170403778553009,0.3083064002841043
raw_13b_chat,mmlu:human_aging,val,0.52173912525177,0.2499259762142015
raw_13b_chat,mmlu:formal_logic,test,0.30158731341362,0.4865076347949013
raw_13b_chat,mmlu:formal_logic,val,0.5714285969734192,0.2338761091232299
raw_13b_chat,mmlu:abstract_algebra,test,0.3499999940395355,0.330926817059517
raw_13b_chat,mmlu:abstract_algebra,val,0.2727272808551788,0.4007779088887301
raw_13b_chat,mmlu:human_sexuality,test,0.404580146074295,0.3449655548306822
raw_13b_chat,mmlu:human_sexuality,val,0.4166666865348816,0.3607435822486878
raw_13b_chat,mmlu:world_religions,test,0.5964912176132202,0.1014781113256488
raw_13b_chat,mmlu:world_religions,val,0.7368420958518982,0.1138536585004706
raw_13b_chat,mmlu:high_school_microeconomics,test,0.4033613801002502,0.3520089360345312
raw_13b_chat,mmlu:high_school_microeconomics,val,0.3076923191547394,0.4551825981873733
raw_13b_chat,mmlu:moral_disputes,test,0.5115606784820557,0.1959647224473126
raw_13b_chat,mmlu:moral_disputes,val,0.6052631735801697,0.1338399021249068
raw_13b_chat,mmlu:prehistory,test,0.4351851940155029,0.3458466375315631
raw_13b_chat,mmlu:prehistory,val,0.4857142865657806,0.3114507590021406
raw_13b_chat,mmlu:high_school_world_history,test,0.5864978432655334,0.1335234737597437
raw_13b_chat,mmlu:high_school_world_history,val,0.6538462042808533,0.0614931560479677
raw_13b_chat,mmlu:high_school_us_history,test,0.6029412150382996,0.104330168927417
raw_13b_chat,mmlu:high_school_us_history,val,0.8181818723678589,0.1286755962805314
raw_13b_chat,mmlu:college_medicine,test,0.4450866878032684,0.3510558839478244
raw_13b_chat,mmlu:college_medicine,val,0.5,0.2675013487989253
raw_13b_chat,mmlu:high_school_physics,test,0.2450331151485443,0.5545657142898105
raw_13b_chat,mmlu:high_school_physics,val,0.0588235296308994,0.7535889008465935
raw_13b_chat,mmlu:college_computer_science,test,0.2199999988079071,0.5714580070972443
raw_13b_chat,mmlu:college_computer_science,val,0.2727272808551788,0.5265097401358864
raw_13b_chat,mmlu:professional_accounting,test,0.2695035338401794,0.499451089412608
raw_13b_chat,mmlu:professional_accounting,val,0.2258064448833465,0.562981657443508
raw_13b_chat,mmlu:econometrics,test,0.3947368562221527,0.3239117536628455
raw_13b_chat,mmlu:econometrics,val,0.4166666865348816,0.3845763156811396
raw_13b_chat,mmlu:anatomy,test,0.5407407283782959,0.3096951241846438
raw_13b_chat,mmlu:anatomy,val,0.2857142984867096,0.6103409826755525
raw_13b_chat,mmlu:college_mathematics,test,0.209999993443489,0.5931916850805283
raw_13b_chat,mmlu:college_mathematics,val,0.0909090936183929,0.7247992862354625
raw_13b_chat,mmlu:management,test,0.5048543810844421,0.2172659843870736
raw_13b_chat,mmlu:management,val,0.6363636255264282,0.2553565773096951
raw_13b_chat,mmlu:moral_scenarios,test,0.4949720501899719,0.1099872756936696
raw_13b_chat,mmlu:moral_scenarios,val,0.5099999904632568,0.0936387163400649
raw_13b_chat,mmlu:electrical_engineering,test,0.2413793057203292,0.5402622888828146
raw_13b_chat,mmlu:electrical_engineering,val,0.25,0.5195934250950813
raw_13b_chat,mmlu:professional_psychology,test,0.4346405267715454,0.3425876424203511
raw_13b_chat,mmlu:professional_psychology,val,0.4347826242446899,0.3318463779877925
raw_13b_chat,mmlu:astronomy,test,0.5328947305679321,0.2444302647521621
raw_13b_chat,mmlu:astronomy,val,0.5625,0.3308205641806125
raw_13b_chat,mmlu:sociology,test,0.5174129009246826,0.2609093998795125
raw_13b_chat,mmlu:sociology,val,0.5454545617103577,0.2666302512992512
raw_13b_chat,mmlu:clinical_knowledge,test,0.3924528360366821,0.4090069332212772
raw_13b_chat,mmlu:clinical_knowledge,val,0.4137930870056152,0.3882596965493827
raw_13b_chat,mmlu:elementary_mathematics,test,0.3650793433189392,0.4143502535643401
raw_13b_chat,mmlu:elementary_mathematics,val,0.2926829159259796,0.4711711871914747
raw_13b_chat,mmlu:professional_medicine,test,0.3382352888584137,0.4496868993429577
raw_13b_chat,mmlu:professional_medicine,val,0.4516128897666931,0.3529528610167965
raw_13b_chat,mmlu:high_school_mathematics,test,0.1666666567325592,0.6000753195197495
raw_13b_chat,mmlu:high_school_mathematics,val,0.1379310339689254,0.6213668934230148
raw_13b_chat,mmlu:high_school_geography,test,0.4848484992980957,0.24697901052658
raw_13b_chat,mmlu:high_school_geography,val,0.5454545617103577,0.4742449630390514
raw_13b_chat,mmlu:international_law,test,0.6694214344024658,0.1345867470276256
raw_13b_chat,mmlu:international_law,val,0.6153846383094788,0.1915418322269733
raw_13b_chat,mmlu:machine_learning,test,0.3035714328289032,0.4392039594905716
raw_13b_chat,mmlu:machine_learning,val,0.5454545617103577,0.189274170181968
raw_13b_chat,mmlu:business_ethics,test,0.5299999713897705,0.1722972363233566
raw_13b_chat,mmlu:business_ethics,val,0.4545454680919647,0.2641854448751969
raw_13b_chat,mmlu:philosophy,test,0.4565916359424591,0.2763063388238765
raw_13b_chat,mmlu:philosophy,val,0.5,0.3199281113989213
raw_13b_chat,mmlu:high_school_statistics,test,0.3194444477558136,0.4405563153602459
raw_13b_chat,mmlu:high_school_statistics,val,0.2173913121223449,0.5485062651012255
raw_13b_chat,mmlu:jurisprudence,test,0.5,0.2173368296137562
raw_13b_chat,mmlu:jurisprudence,val,0.5454545617103577,0.2300670092756098
raw_13b_chat,mmlu:college_physics,test,0.1764705926179886,0.6150431294067233
raw_13b_chat,mmlu:college_physics,val,0.2727272808551788,0.5384305390444669
raw_13b_chat,mmlu:conceptual_physics,test,0.4212765693664551,0.3657875786436365
raw_13b_chat,mmlu:conceptual_physics,val,0.5384615659713745,0.2315496114584115
raw_13b_chat,mmlu:nutrition,test,0.4411764740943908,0.3254331975590949
raw_13b_chat,mmlu:nutrition,val,0.4545454680919647,0.3210692243142561
raw_13b_chat,mmlu:logical_fallacies,test,0.5337423086166382,0.1695079708391903
raw_13b_chat,mmlu:logical_fallacies,val,0.7777777910232544,0.2263114121225145
raw_13b_chat,mmlu:virology,test,0.3554216623306274,0.3978706942983421
raw_13b_chat,mmlu:virology,val,0.4444444477558136,0.3515946269035339
raw_13b_chat,mmlu:college_biology,test,0.4236111044883728,0.3991576532522837
raw_13b_chat,mmlu:college_biology,val,0.25,0.5451840609312057
raw_13b_chat,mmlu:security_studies,test,0.7102040648460388,0.0575267516836828
raw_13b_chat,mmlu:security_studies,val,0.7037037014961243,0.0745638255719785
raw_13b_chat,mmlu:high_school_macroeconomics,test,0.4179487228393554,0.3430588804758512
raw_13b_chat,mmlu:high_school_macroeconomics,val,0.4186046421527862,0.346318336420281
raw_13b_chat,mmlu:public_relations,test,0.4727272689342499,0.2548506221987984
raw_13b_chat,mmlu:public_relations,val,0.75,0.1640523473421732
raw_13b_chat,mmlu:computer_security,test,0.4699999988079071,0.2693430697917938
raw_13b_chat,mmlu:computer_security,val,0.5454545617103577,0.3524454994635148
raw_13b_chat,mmlu:high_school_government_and_politics,test,0.5854921936988831,0.1554236720880696
raw_13b_chat,mmlu:high_school_government_and_politics,val,0.5714285969734192,0.1735743227459135
raw_13b_chat,mmlu:high_school_computer_science,test,0.4399999976158142,0.3580108261108398
raw_13b_chat,mmlu:high_school_computer_science,val,0.2222222238779068,0.5752376251750523
raw_13b_chat,mmlu:high_school_chemistry,test,0.2758620679378509,0.4971346558608445
raw_13b_chat,mmlu:high_school_chemistry,val,0.1818181872367859,0.5621218247847123
raw_13b_chat,mmlu:high_school_psychology,test,0.526605486869812,0.2455981438313055
raw_13b_chat,mmlu:high_school_psychology,val,0.5666667222976685,0.1961768428484599
raw_13b_chat,mmlu:us_foreign_policy,test,0.6899999976158142,0.1280345678329467
raw_13b_chat,mmlu:us_foreign_policy,val,0.8181818723678589,0.201846106485887
raw_13b_chat,mmlu:high_school_european_history,test,0.6484848260879517,0.064426184423042
raw_13b_chat,mmlu:high_school_european_history,val,0.7222222089767456,0.1154279212156931
raw_13b_chat,mmlu:miscellaneous,test,0.625798225402832,0.11876083128297
raw_13b_chat,mmlu:miscellaneous,val,0.5348837375640869,0.1934804209443025
raw_13b_chat,mmlu:college_chemistry,test,0.2299999892711639,0.5545353502035141
raw_13b_chat,mmlu:college_chemistry,val,0.25,0.541083537042141
raw_13b_chat,mmlu:global_facts,test,0.2999999821186065,0.4740463370084763
raw_13b_chat,mmlu:global_facts,val,0.3000000119209289,0.5615139901638031
raw_13b_chat,mmlu:medical_genetics,test,0.4399999976158142,0.3691776466369629
raw_13b_chat,mmlu:medical_genetics,val,0.9090909361839294,0.12538789619099
raw_13b_chat,mmlu:marketing,test,0.5299145579338074,0.1934531567952572
raw_13b_chat,mmlu:marketing,val,0.4399999976158142,0.3112347197532654
raw_13b_chat,mmlu:professional_law,test,0.4432855248451233,0.2973170418782836
raw_13b_chat,mmlu:professional_law,val,0.5058823823928833,0.2836537406725042
ut_13b_chat,mmlu:high_school_biology,test,0.4677419364452362,0.1386300715707963
ut_13b_chat,mmlu:high_school_biology,val,0.5,0.1076144482940435
ut_13b_chat,mmlu:human_aging,test,0.4394619166851043,0.1444633795541497
ut_13b_chat,mmlu:human_aging,val,0.6086956858634949,0.116767766682998
ut_13b_chat,mmlu:formal_logic,test,0.484127014875412,0.0919939473507896
ut_13b_chat,mmlu:formal_logic,val,0.6428571939468384,0.0503834443432944
ut_13b_chat,mmlu:abstract_algebra,test,0.3199999928474426,0.2595570820569992
ut_13b_chat,mmlu:abstract_algebra,val,0.2727272808551788,0.3081142035397616
ut_13b_chat,mmlu:human_sexuality,test,0.5038167834281921,0.0892679036118602
ut_13b_chat,mmlu:human_sexuality,val,0.75,0.3539224316676457
ut_13b_chat,mmlu:world_religions,test,0.567251443862915,0.0346563475870946
ut_13b_chat,mmlu:world_religions,val,0.7894737124443054,0.2410445150576139
ut_13b_chat,mmlu:high_school_microeconomics,test,0.4663865864276886,0.1170777195141095
ut_13b_chat,mmlu:high_school_microeconomics,val,0.3846153914928436,0.214004424902109
ut_13b_chat,mmlu:moral_disputes,test,0.5664739608764648,0.0472234366946137
ut_13b_chat,mmlu:moral_disputes,val,0.6315789222717285,0.1287907597265745
ut_13b_chat,mmlu:prehistory,test,0.4475308656692505,0.1378307090497311
ut_13b_chat,mmlu:prehistory,val,0.5142857432365417,0.1152226141520909
ut_13b_chat,mmlu:high_school_world_history,test,0.6371307969093323,0.0533224761737549
ut_13b_chat,mmlu:high_school_world_history,val,0.6153846383094788,0.0908361031458928
ut_13b_chat,mmlu:high_school_us_history,test,0.6470588445663452,0.0425713041833803
ut_13b_chat,mmlu:high_school_us_history,val,0.6818181872367859,0.1583988097580997
ut_13b_chat,mmlu:college_medicine,test,0.3699421882629394,0.2268783994492768
ut_13b_chat,mmlu:college_medicine,val,0.3181818127632141,0.2756530479951338
ut_13b_chat,mmlu:high_school_physics,test,0.231788083910942,0.3442824715810107
ut_13b_chat,mmlu:high_school_physics,val,0.294117659330368,0.3129062792834113
ut_13b_chat,mmlu:college_computer_science,test,0.3499999940395355,0.2303424823284149
ut_13b_chat,mmlu:college_computer_science,val,0.3636363744735718,0.2266739064996892
ut_13b_chat,mmlu:professional_accounting,test,0.2517730295658111,0.3273847555437832
ut_13b_chat,mmlu:professional_accounting,val,0.3548386991024017,0.2294289623537371
ut_13b_chat,mmlu:econometrics,test,0.359649121761322,0.2146253089110056
ut_13b_chat,mmlu:econometrics,val,0.5,0.1252074340979258
ut_13b_chat,mmlu:anatomy,test,0.4444444477558136,0.1445527204760798
ut_13b_chat,mmlu:anatomy,val,0.2857142984867096,0.2763851072107043
ut_13b_chat,mmlu:college_mathematics,test,0.25,0.3147670847177506
ut_13b_chat,mmlu:college_mathematics,val,0.1818181872367859,0.4076473171060736
ut_13b_chat,mmlu:management,test,0.5145630836486816,0.1015368196570757
ut_13b_chat,mmlu:management,val,0.5454545617103577,0.1328990622000261
ut_13b_chat,mmlu:moral_scenarios,test,0.4301675856113434,0.0998539238668686
ut_13b_chat,mmlu:moral_scenarios,val,0.4699999988079071,0.0573788571357727
ut_13b_chat,mmlu:electrical_engineering,test,0.3793103396892547,0.1922271148911837
ut_13b_chat,mmlu:electrical_engineering,val,0.375,0.1955539807677268
ut_13b_chat,mmlu:professional_psychology,test,0.4493464231491089,0.1473774549618266
ut_13b_chat,mmlu:professional_psychology,val,0.4202898740768432,0.1828280756438987
ut_13b_chat,mmlu:astronomy,test,0.4868420958518982,0.1147367981703657
ut_13b_chat,mmlu:astronomy,val,0.5625,0.0855147019028663
ut_13b_chat,mmlu:sociology,test,0.4676616787910461,0.1559581978997187
ut_13b_chat,mmlu:sociology,val,0.6363636255264282,0.0584339472380551
ut_13b_chat,mmlu:clinical_knowledge,test,0.4377358555793762,0.162038479885965
ut_13b_chat,mmlu:clinical_knowledge,val,0.2413793057203292,0.3591690577309707
ut_13b_chat,mmlu:elementary_mathematics,test,0.3835978806018829,0.1978322078311254
ut_13b_chat,mmlu:elementary_mathematics,val,0.3414633870124817,0.253626484696458
ut_13b_chat,mmlu:professional_medicine,test,0.3602941334247589,0.2276861932786071
ut_13b_chat,mmlu:professional_medicine,val,0.3870967626571655,0.2894942202875691
ut_13b_chat,mmlu:high_school_mathematics,test,0.1296296268701553,0.4479123325259597
ut_13b_chat,mmlu:high_school_mathematics,val,0.0689655169844627,0.5132528584578941
ut_13b_chat,mmlu:high_school_geography,test,0.5050504803657532,0.0800152186191443
ut_13b_chat,mmlu:high_school_geography,val,0.5,0.1057871742682023
ut_13b_chat,mmlu:international_law,test,0.6446280479431152,0.0589850037551123
ut_13b_chat,mmlu:international_law,val,0.6153846383094788,0.1848541635733384
ut_13b_chat,mmlu:machine_learning,test,0.3214285969734192,0.2548126913607121
ut_13b_chat,mmlu:machine_learning,val,0.2727272808551788,0.2947189591147683
ut_13b_chat,mmlu:business_ethics,test,0.4599999785423279,0.1176245880126953
ut_13b_chat,mmlu:business_ethics,val,0.6363636255264282,0.1921462416648864
ut_13b_chat,mmlu:philosophy,test,0.4758842289447784,0.1028810429036425
ut_13b_chat,mmlu:philosophy,val,0.4117647111415863,0.223095318850349
ut_13b_chat,mmlu:high_school_statistics,test,0.3611111044883728,0.223812429441346
ut_13b_chat,mmlu:high_school_statistics,val,0.3478260934352875,0.2644892827324245
ut_13b_chat,mmlu:jurisprudence,test,0.5092592835426331,0.087762893350036
ut_13b_chat,mmlu:jurisprudence,val,0.5454545617103577,0.1805434768850153
ut_13b_chat,mmlu:college_physics,test,0.2058823555707931,0.3630315643899581
ut_13b_chat,mmlu:college_physics,val,0.3636363744735718,0.2100175673311407
ut_13b_chat,mmlu:conceptual_physics,test,0.4893616735935211,0.0826995390526792
ut_13b_chat,mmlu:conceptual_physics,val,0.5384615659713745,0.0469602461044604
ut_13b_chat,mmlu:nutrition,test,0.4803921580314636,0.1467600012916365
ut_13b_chat,mmlu:nutrition,val,0.4545454680919647,0.1660770990631797
ut_13b_chat,mmlu:logical_fallacies,test,0.4907975196838379,0.0905121659940005
ut_13b_chat,mmlu:logical_fallacies,val,0.7222222089767456,0.1629373729228974
ut_13b_chat,mmlu:virology,test,0.4096385240554809,0.1721243061215044
ut_13b_chat,mmlu:virology,val,0.4444444477558136,0.2624100976520114
ut_13b_chat,mmlu:college_biology,test,0.4444444477558136,0.1568476884729332
ut_13b_chat,mmlu:college_biology,val,0.5,0.1175515614449977
ut_13b_chat,mmlu:security_studies,test,0.6816326379776001,0.0297401632581438
ut_13b_chat,mmlu:security_studies,val,0.8148148059844971,0.1258571986798886
ut_13b_chat,mmlu:high_school_macroeconomics,test,0.4666666686534881,0.107124111132744
ut_13b_chat,mmlu:high_school_macroeconomics,val,0.4651162624359131,0.1120348489561746
ut_13b_chat,mmlu:public_relations,test,0.409090906381607,0.1644267331470143
ut_13b_chat,mmlu:public_relations,val,0.4166666865348816,0.2671087781588236
ut_13b_chat,mmlu:computer_security,test,0.4899999797344208,0.0932808512449264
ut_13b_chat,mmlu:computer_security,val,0.4545454680919647,0.1453057852658358
ut_13b_chat,mmlu:high_school_government_and_politics,test,0.5803108811378479,0.0275309669539101
ut_13b_chat,mmlu:high_school_government_and_politics,val,0.380952388048172,0.2207122388340178
ut_13b_chat,mmlu:high_school_computer_science,test,0.449999988079071,0.1703435444831848
ut_13b_chat,mmlu:high_school_computer_science,val,0.5555555820465088,0.2509473628467983
ut_13b_chat,mmlu:high_school_chemistry,test,0.3300492465496063,0.2498381537169658
ut_13b_chat,mmlu:high_school_chemistry,val,0.4545454680919647,0.1482651369138197
ut_13b_chat,mmlu:high_school_psychology,test,0.5192660689353943,0.0682008227077099
ut_13b_chat,mmlu:high_school_psychology,val,0.5166667103767395,0.0711089173952738
ut_13b_chat,mmlu:us_foreign_policy,test,0.6299999952316284,0.0364430665969849
ut_13b_chat,mmlu:us_foreign_policy,val,0.8181818723678589,0.2207476171580227
ut_13b_chat,mmlu:high_school_european_history,test,0.6545454263687134,0.0879136558735009
ut_13b_chat,mmlu:high_school_european_history,val,0.6111111044883728,0.1732632517814636
ut_13b_chat,mmlu:miscellaneous,test,0.6040868163108826,0.0719988810452769
ut_13b_chat,mmlu:miscellaneous,val,0.5581395626068115,0.0141735056111978
ut_13b_chat,mmlu:college_chemistry,test,0.2599999904632568,0.3242996901273727
ut_13b_chat,mmlu:college_chemistry,val,0.375,0.2828442305326462
ut_13b_chat,mmlu:global_facts,test,0.25,0.310468527674675
ut_13b_chat,mmlu:global_facts,val,0.3000000119209289,0.2816921830177307
ut_13b_chat,mmlu:medical_genetics,test,0.449999988079071,0.1377709537744522
ut_13b_chat,mmlu:medical_genetics,val,0.6363636255264282,0.1105134758082303
ut_13b_chat,mmlu:marketing,test,0.5512821078300476,0.0264026548108484
ut_13b_chat,mmlu:marketing,val,0.3599999845027923,0.2017677307128906
ut_13b_chat,mmlu:professional_law,test,0.4452411830425262,0.1589959586692447
ut_13b_chat,mmlu:professional_law,val,0.4764705896377563,0.1200732974445119
