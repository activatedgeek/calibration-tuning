N,unc_acc,unc_auroc,unc_ece,acc,dataset,split,ts
11,0.8181818127632141,0.9,0.15579815886237403,0.09090909361839294,mmlu_offline:llama2-7b_chat:abstract_algebra,validation,4.698572832159698
100,0.7699999809265137,0.7120253164556962,0.20220828652381903,0.20999999344348907,mmlu_offline:llama2-7b_chat:abstract_algebra,test,4.520568092353642
14,0.7142857313156128,0.8484848484848485,0.29300395080021446,0.2142857164144516,mmlu_offline:llama2-7b_chat:anatomy,validation,0.8816021466627717
135,0.6370370388031006,0.7513901760889713,0.34063501269729046,0.385185182094574,mmlu_offline:llama2-7b_chat:anatomy,test,5.027278105728328
16,0.5625,0.6031746031746033,0.4145698547363281,0.5625,mmlu_offline:llama2-7b_chat:astronomy,validation,0.8819885523989797
152,0.6973684430122375,0.7655172413793103,0.26324164043915904,0.42763158679008484,mmlu_offline:llama2-7b_chat:astronomy,test,7.0516794035211205
11,0.4545454680919647,0.6333333333333333,0.5096688162196766,0.4545454680919647,mmlu_offline:llama2-7b_chat:business_ethics,validation,0.8758497461676598
100,0.5099999904632568,0.7282903663500678,0.4317848819494247,0.33000001311302185,mmlu_offline:llama2-7b_chat:business_ethics,test,5.730919527821243
29,0.6896551847457886,0.7791666666666667,0.305515496895231,0.17241379618644714,mmlu_offline:llama2-7b_chat:clinical_knowledge,validation,1.42898965254426
265,0.6377358436584473,0.7266520447148719,0.3120352079283516,0.2792452871799469,mmlu_offline:llama2-7b_chat:clinical_knowledge,test,10.615797868929803
16,0.5625,0.6,0.41003067791461945,0.375,mmlu_offline:llama2-7b_chat:college_biology,validation,0.9612358445301652
144,0.625,0.7039742212674545,0.35250655975606704,0.3402777910232544,mmlu_offline:llama2-7b_chat:college_biology,test,7.199505143798888
8,0.875,,0.11734652519226074,0.0,mmlu_offline:llama2-7b_chat:college_chemistry,validation,0.8663684446364641
100,0.7900000214576721,0.6752491694352158,0.19063320755958557,0.14000000059604645,mmlu_offline:llama2-7b_chat:college_chemistry,test,5.540712419897318
11,0.4545454680919647,0.7083333333333333,0.4864119670607827,0.27272728085517883,mmlu_offline:llama2-7b_chat:college_computer_science,validation,1.1698698429390788
100,0.7400000095367432,0.8098866052445075,0.22294811725616456,0.17000000178813934,mmlu_offline:llama2-7b_chat:college_computer_science,test,8.699739452451468
11,0.8181818127632141,,0.1816333640705456,0.0,mmlu_offline:llama2-7b_chat:college_mathematics,validation,1.0354709029197693
100,0.7799999713897705,0.6350524475524475,0.17876520931720732,0.2199999988079071,mmlu_offline:llama2-7b_chat:college_mathematics,test,6.59471888281405
22,0.5,0.6285714285714286,0.47267473556778644,0.3181818127632141,mmlu_offline:llama2-7b_chat:college_medicine,validation,1.3436470897868276
173,0.7052023410797119,0.7620941819350691,0.25244870075600684,0.2947976887226105,mmlu_offline:llama2-7b_chat:college_medicine,test,12.377057921141386
11,0.9090909361839294,1.0,0.12285648692737929,0.09090909361839294,mmlu_offline:llama2-7b_chat:college_physics,validation,0.8680790662765503
102,0.813725471496582,0.6884707287933094,0.1668558962204877,0.0882352963089943,mmlu_offline:llama2-7b_chat:college_physics,test,5.68276253156364
11,0.6363636255264282,0.5714285714285714,0.36848346211693506,0.6363636255264282,mmlu_offline:llama2-7b_chat:computer_security,validation,0.8371046660467982
100,0.6600000262260437,0.6765,0.31276458203792573,0.5,mmlu_offline:llama2-7b_chat:computer_security,test,4.558826209977269
26,0.6538461446762085,0.75,0.302873235482436,0.1538461595773697,mmlu_offline:llama2-7b_chat:conceptual_physics,validation,1.2472232738509774
235,0.6042553186416626,0.6010658765584116,0.3539074654274798,0.3787234127521515,mmlu_offline:llama2-7b_chat:conceptual_physics,test,8.810201889835298
12,0.5,0.59375,0.45740003387133277,0.3333333432674408,mmlu_offline:llama2-7b_chat:econometrics,validation,1.0734266135841608
114,0.5,0.6356382978723404,0.4408529562908306,0.17543859779834747,mmlu_offline:llama2-7b_chat:econometrics,test,7.494763456285
16,0.8125,0.6153846153846154,0.17713403701782227,0.1875,mmlu_offline:llama2-7b_chat:electrical_engineering,validation,0.9928952576592565
145,0.7379310131072998,0.6716185625353707,0.24193260299748387,0.2137930989265442,mmlu_offline:llama2-7b_chat:electrical_engineering,test,7.254296808503568
41,0.6097561120986938,0.6352564102564102,0.3681437634840244,0.3658536672592163,mmlu_offline:llama2-7b_chat:elementary_mathematics,validation,2.474390134215355
378,0.7407407164573669,0.6794360432852387,0.2181339322259186,0.28042328357696533,mmlu_offline:llama2-7b_chat:elementary_mathematics,test,19.217508800327778
14,0.5,0.39583333333333337,0.4362321283136096,0.4285714328289032,mmlu_offline:llama2-7b_chat:formal_logic,validation,1.0517540592700243
126,0.5555555820465088,0.5749667553191489,0.3636960235853044,0.2539682686328888,mmlu_offline:llama2-7b_chat:formal_logic,test,7.319907210767269
10,0.800000011920929,0.9375,0.12692871093750002,0.20000000298023224,mmlu_offline:llama2-7b_chat:global_facts,validation,0.7942475574091077
100,0.6899999976158142,0.5502717391304348,0.2538148653507233,0.07999999821186066,mmlu_offline:llama2-7b_chat:global_facts,test,4.721951267682016
32,0.53125,0.6556818181818183,0.4140411801636219,0.3125,mmlu_offline:llama2-7b_chat:high_school_biology,validation,1.7531080497428775
310,0.6870967745780945,0.763979937524748,0.27925360510426184,0.3838709592819214,mmlu_offline:llama2-7b_chat:high_school_biology,test,14.897360783070326
22,0.6363636255264282,0.5347222222222223,0.3662483610890129,0.1818181872367859,mmlu_offline:llama2-7b_chat:high_school_chemistry,validation,1.3701258478686213
203,0.738916277885437,0.7231470471660721,0.24112511973075673,0.1428571492433548,mmlu_offline:llama2-7b_chat:high_school_chemistry,test,10.988075646571815
9,0.7777777910232544,0.85,0.2292380200492011,0.4444444477558136,mmlu_offline:llama2-7b_chat:high_school_computer_science,validation,0.9782650116831064
100,0.6600000262260437,0.7023563455973543,0.3088987636566163,0.4099999964237213,mmlu_offline:llama2-7b_chat:high_school_computer_science,test,8.120493571273983
18,0.9444444179534912,0.9777777777777779,0.05938061740663318,0.8333333134651184,mmlu_offline:llama2-7b_chat:high_school_european_history,validation,5.6988105764612556
165,0.7818182110786438,0.795940170940171,0.1884124430743131,0.7090908885002136,mmlu_offline:llama2-7b_chat:high_school_european_history,test,50.35079516470432
22,0.6818181872367859,0.7916666666666666,0.2889789234508167,0.4545454680919647,mmlu_offline:llama2-7b_chat:high_school_geography,validation,1.1124902544543147
198,0.752525269985199,0.7859633827375763,0.23219674163394502,0.3737373650074005,mmlu_offline:llama2-7b_chat:high_school_geography,test,7.590804176405072
21,0.6190476417541504,0.5454545454545454,0.4102628770328703,0.523809552192688,mmlu_offline:llama2-7b_chat:high_school_government_and_politics,validation,1.1026695314794779
193,0.6943005323410034,0.7437849763237194,0.27374359857233077,0.5233160853385925,mmlu_offline:llama2-7b_chat:high_school_government_and_politics,test,7.653603265993297
43,0.6279069781303406,0.6674208144796381,0.35556375287299924,0.39534884691238403,mmlu_offline:llama2-7b_chat:high_school_macroeconomics,validation,1.906819305382669
390,0.6282051205635071,0.7403118111242453,0.3306508551805447,0.2897436022758484,mmlu_offline:llama2-7b_chat:high_school_macroeconomics,test,15.632583008147776
29,0.9655172228813171,,0.031058340237058443,0.0,mmlu_offline:llama2-7b_chat:high_school_mathematics,validation,1.8503938000649214
270,0.9222221970558167,0.6614379084967321,0.061142934913988464,0.0555555559694767,mmlu_offline:llama2-7b_chat:high_school_mathematics,test,16.003063678741455
26,0.6538461446762085,0.8609022556390977,0.3331020176410675,0.26923078298568726,mmlu_offline:llama2-7b_chat:high_school_microeconomics,validation,1.321649193763733
238,0.5714285969734192,0.6662145160022017,0.39235306863023456,0.3403361439704895,mmlu_offline:llama2-7b_chat:high_school_microeconomics,test,9.49420715495944
17,0.47058823704719543,,0.4911608029814327,0.0,mmlu_offline:llama2-7b_chat:high_school_physics,validation,1.25384327955544
151,0.695364236831665,0.7419425087108014,0.2627303446365508,0.18543046712875366,mmlu_offline:llama2-7b_chat:high_school_physics,test,8.587281753309071
60,0.7666666507720947,0.7884615384615384,0.21393343905607853,0.5666666626930237,mmlu_offline:llama2-7b_chat:high_school_psychology,validation,3.266440068371594
545,0.6293578147888184,0.7408795248868778,0.33121102381190026,0.5009174346923828,mmlu_offline:llama2-7b_chat:high_school_psychology,test,27.090954686515033
23,0.695652186870575,0.881578947368421,0.28699567006981896,0.17391304671764374,mmlu_offline:llama2-7b_chat:high_school_statistics,validation,1.924417589791119
216,0.5833333134651184,0.671125151230004,0.36807400705637755,0.19907407462596893,mmlu_offline:llama2-7b_chat:high_school_statistics,test,15.65272918716073
22,0.8636363744735718,0.9129464285714286,0.13619225133549084,0.6363636255264282,mmlu_offline:llama2-7b_chat:high_school_us_history,validation,5.413593988865614
204,0.7156862616539001,0.7661115490737092,0.26186973762278465,0.5784313678741455,mmlu_offline:llama2-7b_chat:high_school_us_history,test,48.63527696207166
26,0.7307692170143127,0.6879699248120301,0.2522268157738906,0.7307692170143127,mmlu_offline:llama2-7b_chat:high_school_world_history,validation,4.751544648781419
237,0.6160337328910828,0.7155588362379376,0.3570762020123156,0.552742600440979,mmlu_offline:llama2-7b_chat:high_school_world_history,test,38.170319699682295
23,0.8260869383811951,0.8660714285714286,0.19507631529932434,0.30434781312942505,mmlu_offline:llama2-7b_chat:human_aging,validation,1.1441852217540145
223,0.6681614518165588,0.7936241610738255,0.3138858646555332,0.33183857798576355,mmlu_offline:llama2-7b_chat:human_aging,test,8.610188909806311
12,0.8333333134651184,0.6296296296296297,0.20463486015796659,0.25,mmlu_offline:llama2-7b_chat:human_sexuality,validation,0.8248955262824893
131,0.6183205842971802,0.6882177033492823,0.34631043308563814,0.4198473393917084,mmlu_offline:llama2-7b_chat:human_sexuality,test,5.576999285258353
13,0.5384615659713745,0.7380952380952381,0.461375525364509,0.4615384638309479,mmlu_offline:llama2-7b_chat:international_law,validation,0.9559203069657087
121,0.6776859760284424,0.5789965497412306,0.3044903938435326,0.6115702390670776,mmlu_offline:llama2-7b_chat:international_law,test,6.315867270343006
11,0.7272727489471436,0.8,0.2078478119590066,0.5454545617103577,mmlu_offline:llama2-7b_chat:jurisprudence,validation,0.846153786405921
108,0.6296296119689941,0.672132554945055,0.3393445969731719,0.5185185074806213,mmlu_offline:llama2-7b_chat:jurisprudence,test,4.807816009037197
18,0.4444444477558136,0.65,0.5319929189152187,0.4444444477558136,mmlu_offline:llama2-7b_chat:logical_fallacies,validation,1.069036583416164
163,0.6871165633201599,0.7994239631336406,0.28183859294177566,0.42944785952568054,mmlu_offline:llama2-7b_chat:logical_fallacies,test,7.491084720008075
11,0.7272727489471436,0.8035714285714286,0.23013737526806918,0.3636363744735718,mmlu_offline:llama2-7b_chat:machine_learning,validation,1.0604322869330645
112,0.6964285969734192,0.705050505050505,0.276612824095147,0.1964285671710968,mmlu_offline:llama2-7b_chat:machine_learning,test,7.060949433594942
11,0.6363636255264282,0.75,0.33725636113773694,0.27272728085517883,mmlu_offline:llama2-7b_chat:management,validation,0.7226266488432884
103,0.737864077091217,0.849089068825911,0.2078687843767185,0.3689320385456085,mmlu_offline:llama2-7b_chat:management,test,3.769471200183034
25,0.4000000059604645,0.6599999999999999,0.5832428956031798,0.20000000298023224,mmlu_offline:llama2-7b_chat:marketing,validation,1.444676587358117
234,0.5726495981216431,0.7061313200640587,0.40610058847655606,0.39743590354919434,mmlu_offline:llama2-7b_chat:marketing,test,9.612642033025622
11,0.8181818127632141,1.0,0.15829335017637772,0.7272727489471436,mmlu_offline:llama2-7b_chat:medical_genetics,validation,0.7498064478859305
100,0.699999988079071,0.7121744522529971,0.2810702633857726,0.4099999964237213,mmlu_offline:llama2-7b_chat:medical_genetics,test,3.9968368243426085
86,0.6860465407371521,0.6994444444444445,0.25191762281018637,0.41860464215278625,mmlu_offline:llama2-7b_chat:miscellaneous,validation,3.5309970946982503
783,0.7049808502197266,0.7447660260629195,0.2580797134840321,0.4533844292163849,mmlu_offline:llama2-7b_chat:miscellaneous,test,29.429434690624475
38,0.7105262875556946,0.7965277777777777,0.25814168076766164,0.4736842215061188,mmlu_offline:llama2-7b_chat:moral_disputes,validation,2.0600757533684373
346,0.5982658863067627,0.6738211129522231,0.35039218278289525,0.41040462255477905,mmlu_offline:llama2-7b_chat:moral_disputes,test,16.08790623024106
100,0.5,0.5867346938775511,0.4799557077884674,0.5099999904632568,mmlu_offline:llama2-7b_chat:moral_scenarios,validation,7.381927138194442
895,0.5452513694763184,0.5171789758170657,0.43531485103362094,0.46145251393318176,mmlu_offline:llama2-7b_chat:moral_scenarios,test,61.249546631239355
33,0.5454545617103577,0.6984126984126984,0.4056784853790746,0.3636363744735718,mmlu_offline:llama2-7b_chat:nutrition,validation,2.043796317651868
306,0.6274510025978088,0.7021215469613259,0.34856410607013827,0.4084967374801636,mmlu_offline:llama2-7b_chat:nutrition,test,15.609787386842072
34,0.5882353186607361,0.7301136363636364,0.3859251053894267,0.3529411852359772,mmlu_offline:llama2-7b_chat:philosophy,validation,1.7569514559581876
311,0.5627009868621826,0.7077901063337957,0.3987373851503206,0.3376205861568451,mmlu_offline:llama2-7b_chat:philosophy,test,12.23480873182416
35,0.7714285850524902,0.8520000000000001,0.23391815083367484,0.2857142984867096,mmlu_offline:llama2-7b_chat:prehistory,validation,1.98289624042809
324,0.6574074029922485,0.7325952713276657,0.30195011842398,0.34259259700775146,mmlu_offline:llama2-7b_chat:prehistory,test,14.91401844471693
31,0.6774193644523621,0.6077586206896551,0.3206172162486661,0.06451612710952759,mmlu_offline:llama2-7b_chat:professional_accounting,validation,2.701694634743035
282,0.6773049831390381,0.6411144003055768,0.27663670046955136,0.1560283750295639,mmlu_offline:llama2-7b_chat:professional_accounting,test,21.56643944978714
170,0.4882352948188782,0.5972946544980444,0.43560697681763594,0.30588236451148987,mmlu_offline:llama2-7b_chat:professional_law,validation,26.26620546914637
1534,0.4843546152114868,0.6016474879567596,0.4600073048235385,0.3200782239437103,mmlu_offline:llama2-7b_chat:professional_law,test,242.46892307233065
31,0.4838709533214569,0.7051630434782609,0.4788694573986915,0.25806450843811035,mmlu_offline:llama2-7b_chat:professional_medicine,validation,4.174749166704714
272,0.6066176295280457,0.6948805588942307,0.3458613036748241,0.23529411852359772,mmlu_offline:llama2-7b_chat:professional_medicine,test,34.056866850703955
69,0.6231883764266968,0.723613595706619,0.3640876697457355,0.37681159377098083,mmlu_offline:llama2-7b_chat:professional_psychology,validation,4.006313498131931
612,0.5915032625198364,0.7180692312396795,0.3724882078716178,0.32189542055130005,mmlu_offline:llama2-7b_chat:professional_psychology,test,30.98074924107641
12,0.5,0.828125,0.4773896088202794,0.3333333432674408,mmlu_offline:llama2-7b_chat:public_relations,validation,0.9154745852574706
110,0.6727272868156433,0.7846283783783784,0.27558243328874765,0.3272727131843567,mmlu_offline:llama2-7b_chat:public_relations,test,5.16850549262017
27,0.8148148059844971,0.8517857142857143,0.15363923929355766,0.7407407164573669,mmlu_offline:llama2-7b_chat:security_studies,validation,1.9540005857124925
245,0.7510204315185547,0.7529172424382006,0.20731663387648916,0.6816326379776001,mmlu_offline:llama2-7b_chat:security_studies,test,14.96068924665451
22,0.7272727489471436,0.7685950413223142,0.256428984078494,0.5,mmlu_offline:llama2-7b_chat:sociology,validation,1.2158002695068717
201,0.6716417670249939,0.7420223374551256,0.3046005528364608,0.45771142840385437,mmlu_offline:llama2-7b_chat:sociology,test,8.644071548245847
11,0.9090909361839294,0.9750000000000001,0.08239444819363684,0.5454545617103577,mmlu_offline:llama2-7b_chat:us_foreign_policy,validation,0.8263565273955464
100,0.6899999976158142,0.7088888888888889,0.29018710494041444,0.550000011920929,mmlu_offline:llama2-7b_chat:us_foreign_policy,test,4.419013223610818
18,0.5,0.41538461538461535,0.44317094484965003,0.2777777910232544,mmlu_offline:llama2-7b_chat:virology,validation,1.1945515349507332
166,0.6325300931930542,0.7053758248833093,0.3249056824000484,0.34337350726127625,mmlu_offline:llama2-7b_chat:virology,test,6.992401317693293
19,0.9473684430122375,0.9642857142857142,0.04332830717689118,0.6315789222717285,mmlu_offline:llama2-7b_chat:world_religions,validation,1.0254750158637762
171,0.7426900863647461,0.8418753439735829,0.2245865746548301,0.5380116701126099,mmlu_offline:llama2-7b_chat:world_religions,test,6.291986185126007
