N,unc_acc,unc_auroc,unc_ece,acc,dataset,split,ts
11,0.8181818127632141,0.30000000000000004,0.16219400817697693,0.09090909361839294,mmlu_offline:llama2-13b_chat:abstract_algebra,validation,5.689896436408162
100,0.6700000166893005,0.5969230769230769,0.31428565621376037,0.3499999940395355,mmlu_offline:llama2-13b_chat:abstract_algebra,test,8.10594660229981
14,0.7142857313156128,0.625,0.3064852527209691,0.2857142984867096,mmlu_offline:llama2-13b_chat:anatomy,validation,0.9626937368884683
135,0.7185184955596924,0.7993139901034638,0.2610232905105308,0.42222222685813904,mmlu_offline:llama2-13b_chat:anatomy,test,8.234496602788568
16,0.8125,0.84375,0.20010364055633545,0.5,mmlu_offline:llama2-13b_chat:astronomy,validation,1.397843549028039
152,0.6710526347160339,0.7438802083333331,0.29513542824669886,0.5263158082962036,mmlu_offline:llama2-13b_chat:astronomy,test,11.990125291980803
11,0.5454545617103577,0.6666666666666667,0.43932838873429736,0.5454545617103577,mmlu_offline:llama2-13b_chat:business_ethics,validation,1.329113306477666
100,0.550000011920929,0.6869255514705882,0.3925744324922562,0.3199999928474426,mmlu_offline:llama2-13b_chat:business_ethics,test,9.416696931235492
29,0.6551724076271057,0.6444805194805194,0.336661501177426,0.24137930572032928,mmlu_offline:llama2-13b_chat:clinical_knowledge,validation,2.5181150306016207
265,0.6264150738716125,0.6674492990336192,0.3392601251602173,0.2981131970882416,mmlu_offline:llama2-13b_chat:clinical_knowledge,test,18.307141229510307
16,0.75,0.7083333333333334,0.23246377706527704,0.25,mmlu_offline:llama2-13b_chat:college_biology,validation,1.5164786847308278
144,0.5833333134651184,0.6412583182093164,0.38325754470295376,0.3958333432674408,mmlu_offline:llama2-13b_chat:college_biology,test,12.14515065215528
8,0.75,,0.21076916903257376,0.0,mmlu_offline:llama2-13b_chat:college_chemistry,validation,1.1670030392706394
100,0.7900000214576721,0.7932317505315378,0.17304759263992314,0.17000000178813934,mmlu_offline:llama2-13b_chat:college_chemistry,test,9.462504326365888
11,0.6363636255264282,0.6145833333333333,0.33824738047339703,0.27272728085517883,mmlu_offline:llama2-13b_chat:college_computer_science,validation,1.8028116822242737
100,0.7300000190734863,0.6796875,0.23489782929420466,0.23999999463558197,mmlu_offline:llama2-13b_chat:college_computer_science,test,15.172459813766181
11,1.0,,0.0345423546704379,0.0,mmlu_offline:llama2-13b_chat:college_mathematics,validation,1.575772250071168
100,0.7699999809265137,0.6132478632478633,0.1585726314783097,0.09000000357627869,mmlu_offline:llama2-13b_chat:college_mathematics,test,11.385490928776562
22,0.6818181872367859,0.7355371900826447,0.31207914244044915,0.5,mmlu_offline:llama2-13b_chat:college_medicine,validation,2.1361769875511527
173,0.6994219422340393,0.7017488851727982,0.27815109801430227,0.3988439440727234,mmlu_offline:llama2-13b_chat:college_medicine,test,20.717062132433057
11,0.7272727489471436,0.875,0.2368909337303855,0.27272728085517883,mmlu_offline:llama2-13b_chat:college_physics,validation,1.3836950417608023
102,0.686274528503418,0.7647165697674418,0.28997917444098226,0.1568627506494522,mmlu_offline:llama2-13b_chat:college_physics,test,10.19533043820411
11,0.6363636255264282,0.9,0.28190896727822046,0.5454545617103577,mmlu_offline:llama2-13b_chat:computer_security,validation,1.2646764442324638
100,0.6200000047683716,0.6454545454545455,0.3455886203050614,0.550000011920929,mmlu_offline:llama2-13b_chat:computer_security,test,7.499350636266172
26,0.5769230723381042,0.6928104575163399,0.39925118363820594,0.3461538553237915,mmlu_offline:llama2-13b_chat:conceptual_physics,validation,1.9859247030690312
235,0.6340425610542297,0.6868246203271028,0.3396803855895996,0.4553191363811493,mmlu_offline:llama2-13b_chat:conceptual_physics,test,15.028292343951762
12,0.5,0.6666666666666667,0.47468534608681995,0.25,mmlu_offline:llama2-13b_chat:econometrics,validation,1.6036424115300179
114,0.5614035129547119,0.6289562289562289,0.38804714878400165,0.1315789520740509,mmlu_offline:llama2-13b_chat:econometrics,test,13.144325695000589
16,0.875,0.96875,0.10480934381484985,0.25,mmlu_offline:llama2-13b_chat:electrical_engineering,validation,1.5828043390065432
145,0.7448275685310364,0.7266182466870541,0.22621517428036392,0.24827586114406586,mmlu_offline:llama2-13b_chat:electrical_engineering,test,12.100718046538532
41,0.39024388790130615,0.5212121212121212,0.5593215459730568,0.26829269528388977,mmlu_offline:llama2-13b_chat:elementary_mathematics,validation,3.825037397444248
378,0.5502645373344421,0.6503552971576227,0.3808458250351053,0.3174603283405304,mmlu_offline:llama2-13b_chat:elementary_mathematics,test,32.85029265284538
14,0.6428571343421936,0.7272727272727273,0.30711688314165386,0.2142857164144516,mmlu_offline:llama2-13b_chat:formal_logic,validation,1.5618040338158607
126,0.5634920597076416,0.5443955685891171,0.3758856327760787,0.261904776096344,mmlu_offline:llama2-13b_chat:formal_logic,test,12.413537826389074
10,0.699999988079071,0.8125,0.2800709486007691,0.20000000298023224,mmlu_offline:llama2-13b_chat:global_facts,validation,1.0210507027804852
100,0.6399999856948853,0.6945181255526083,0.3043765723705292,0.12999999523162842,mmlu_offline:llama2-13b_chat:global_facts,test,7.9405000656843185
32,0.53125,0.6090909090909091,0.45651769638061523,0.3125,mmlu_offline:llama2-13b_chat:high_school_biology,validation,2.694891002960503
310,0.6580645442008972,0.7410124099108705,0.31556677164569985,0.42258065938949585,mmlu_offline:llama2-13b_chat:high_school_biology,test,25.735583376139402
22,0.8636363744735718,0.8472222222222222,0.14569316668943927,0.1818181872367859,mmlu_offline:llama2-13b_chat:high_school_chemistry,validation,2.3243902856484056
203,0.7241379022598267,0.751221100618691,0.2244398394241709,0.1822660118341446,mmlu_offline:llama2-13b_chat:high_school_chemistry,test,18.607339495792985
9,0.6666666865348816,0.8333333333333334,0.32383348544438684,0.6666666865348816,mmlu_offline:llama2-13b_chat:high_school_computer_science,validation,1.5181690398603678
100,0.6499999761581421,0.6695226438188494,0.3266302585601807,0.4300000071525574,mmlu_offline:llama2-13b_chat:high_school_computer_science,test,13.611588036641479
18,0.7222222089767456,0.7678571428571428,0.2829319768481784,0.7777777910232544,mmlu_offline:llama2-13b_chat:high_school_european_history,validation,9.487070041708648
165,0.8121212124824524,0.7657999999999999,0.16557254249399356,0.7575757503509521,mmlu_offline:llama2-13b_chat:high_school_european_history,test,85.30941097810864
22,0.7272727489471436,0.7107438016528926,0.26422231034799054,0.5,mmlu_offline:llama2-13b_chat:high_school_geography,validation,1.7505344906821847
198,0.6767676472663879,0.716159188034188,0.29064731585859044,0.39393940567970276,mmlu_offline:llama2-13b_chat:high_school_geography,test,13.370178636163473
21,0.8571428656578064,0.8545454545454545,0.135494419506618,0.523809552192688,mmlu_offline:llama2-13b_chat:high_school_government_and_politics,validation,1.8257196303457022
193,0.6839378476142883,0.6947875779265759,0.29314068500242085,0.6321243643760681,mmlu_offline:llama2-13b_chat:high_school_government_and_politics,test,14.350409025326371
43,0.7674418687820435,0.8354978354978355,0.23485981309136675,0.4883720874786377,mmlu_offline:llama2-13b_chat:high_school_macroeconomics,validation,3.14115072786808
390,0.6102564334869385,0.7193569331526649,0.3566922400242243,0.3692307770252228,mmlu_offline:llama2-13b_chat:high_school_macroeconomics,test,26.378528063185513
29,0.8965517282485962,0.9259259259259259,0.06946847356598952,0.06896551698446274,mmlu_offline:llama2-13b_chat:high_school_mathematics,validation,3.1897911578416824
270,0.7962962985038757,0.575483920134126,0.14227797278651483,0.10000000149011612,mmlu_offline:llama2-13b_chat:high_school_mathematics,test,27.06379653699696
26,0.7307692170143127,0.8333333333333333,0.2167348105173844,0.3076923191547394,mmlu_offline:llama2-13b_chat:high_school_microeconomics,validation,2.0641247276216745
238,0.6386554837226868,0.7757478876455812,0.3418355669294085,0.36554622650146484,mmlu_offline:llama2-13b_chat:high_school_microeconomics,test,16.262429573573172
17,0.6470588445663452,0.7166666666666667,0.3489632571444792,0.29411765933036804,mmlu_offline:llama2-13b_chat:high_school_physics,validation,1.9221906159073114
151,0.5761589407920837,0.5276153846153846,0.38618744445952363,0.17218543589115143,mmlu_offline:llama2-13b_chat:high_school_physics,test,14.586877486668527
60,0.800000011920929,0.7667824074074074,0.2037813345591227,0.6000000238418579,mmlu_offline:llama2-13b_chat:high_school_psychology,validation,5.323496408760548
545,0.684403657913208,0.7338741177738268,0.29048817212428524,0.5064220428466797,mmlu_offline:llama2-13b_chat:high_school_psychology,test,46.35880881268531
23,0.6086956262588501,0.736842105263158,0.3720692447994067,0.17391304671764374,mmlu_offline:llama2-13b_chat:high_school_statistics,validation,3.0788022726774216
216,0.6064814925193787,0.6672991071428571,0.3507530645087913,0.25925925374031067,mmlu_offline:llama2-13b_chat:high_school_statistics,test,26.153941685333848
22,0.7272727489471436,0.661764705882353,0.26111211559989234,0.7727272510528564,mmlu_offline:llama2-13b_chat:high_school_us_history,validation,9.071019130758941
204,0.7303921580314636,0.7346014492753623,0.2499976613942314,0.6764705777168274,mmlu_offline:llama2-13b_chat:high_school_us_history,test,82.29412508103997
26,0.5769230723381042,0.41013071895424835,0.4224502581816454,0.6538461446762085,mmlu_offline:llama2-13b_chat:high_school_world_history,validation,7.98573861271143
237,0.6793248653411865,0.7246503496503497,0.3080098470555076,0.6033755540847778,mmlu_offline:llama2-13b_chat:high_school_world_history,test,64.87474007997662
23,0.695652186870575,0.8095238095238095,0.29609669550605444,0.3913043439388275,mmlu_offline:llama2-13b_chat:human_aging,validation,1.7161657279357314
223,0.6995515823364258,0.7275819810682893,0.26816882573970224,0.3901345431804657,mmlu_offline:llama2-13b_chat:human_aging,test,14.328169359825552
12,0.5833333134651184,0.40625,0.35143542289733887,0.3333333432674408,mmlu_offline:llama2-13b_chat:human_sexuality,validation,1.2014408754184842
131,0.5877862572669983,0.5962569962686568,0.3749654707107835,0.5114504098892212,mmlu_offline:llama2-13b_chat:human_sexuality,test,9.331770176999271
13,0.5384615659713745,0.45000000000000007,0.39268159866333013,0.6153846383094788,mmlu_offline:llama2-13b_chat:international_law,validation,1.5195648120716214
121,0.6528925895690918,0.6988304093567251,0.30594090834136833,0.6280992031097412,mmlu_offline:llama2-13b_chat:international_law,test,10.315381828695536
11,0.9090909361839294,0.8888888888888888,0.09047651832753958,0.1818181872367859,mmlu_offline:llama2-13b_chat:jurisprudence,validation,1.1119708362966776
108,0.7129629850387573,0.6988108509847639,0.2500977113291069,0.3611111044883728,mmlu_offline:llama2-13b_chat:jurisprudence,test,7.82564793061465
18,0.7777777910232544,0.8368055555555556,0.20900708105829027,0.6666666865348816,mmlu_offline:llama2-13b_chat:logical_fallacies,validation,1.720503456890583
163,0.7300613522529602,0.7900843881856541,0.2528783192663837,0.48466256260871887,mmlu_offline:llama2-13b_chat:logical_fallacies,test,12.807151528075337
11,0.6363636255264282,0.7291666666666667,0.3224296027963812,0.27272728085517883,mmlu_offline:llama2-13b_chat:machine_learning,validation,1.4538288842886686
112,0.5625,0.6336785577291906,0.3968712953584535,0.2946428656578064,mmlu_offline:llama2-13b_chat:machine_learning,test,11.657969767227769
11,0.6363636255264282,0.5714285714285714,0.3933872743086382,0.6363636255264282,mmlu_offline:llama2-13b_chat:management,validation,0.9755036327987909
103,0.6990291476249695,0.712731124807396,0.2862897353264892,0.42718446254730225,mmlu_offline:llama2-13b_chat:management,test,6.225012369453907
25,0.6800000071525574,0.7894736842105263,0.29677186965942387,0.23999999463558197,mmlu_offline:llama2-13b_chat:marketing,validation,2.2896108720451593
234,0.6837607026100159,0.7646548541897379,0.2938384734667264,0.44871795177459717,mmlu_offline:llama2-13b_chat:marketing,test,16.89757927786559
11,0.7272727489471436,0.8,0.23708249222148545,0.9090909361839294,mmlu_offline:llama2-13b_chat:medical_genetics,validation,1.095036176033318
100,0.699999988079071,0.7475757575757576,0.29155468225479125,0.44999998807907104,mmlu_offline:llama2-13b_chat:medical_genetics,test,6.315786845982075
86,0.6395348906517029,0.6724232456140351,0.33418411432310596,0.5581395626068115,mmlu_offline:llama2-13b_chat:miscellaneous,validation,5.698284738697112
783,0.7241379022598267,0.7417268995826308,0.25408701825111457,0.618135392665863,mmlu_offline:llama2-13b_chat:miscellaneous,test,49.17398240696639
38,0.6842105388641357,0.7571022727272727,0.2704423948338157,0.42105263471603394,mmlu_offline:llama2-13b_chat:moral_disputes,validation,3.3356470968574286
346,0.6213873028755188,0.6923738629782306,0.3371650304408432,0.43063583970069885,mmlu_offline:llama2-13b_chat:moral_disputes,test,27.462953900918365
100,0.6000000238418579,0.6317829457364341,0.3141033601760864,0.4300000071525574,mmlu_offline:llama2-13b_chat:moral_scenarios,validation,12.050712365657091
895,0.6558659076690674,0.672950390124607,0.2662292906691909,0.38100558519363403,mmlu_offline:llama2-13b_chat:moral_scenarios,test,104.53835778683424
33,0.5454545617103577,0.6704545454545454,0.44255057428822375,0.3333333432674408,mmlu_offline:llama2-13b_chat:nutrition,validation,3.221360692754388
306,0.656862735748291,0.7394563391203205,0.30471218353003465,0.4542483687400818,mmlu_offline:llama2-13b_chat:nutrition,test,27.19494492933154
34,0.6470588445663452,0.8033596837944664,0.3223492064896752,0.3235294222831726,mmlu_offline:llama2-13b_chat:philosophy,validation,2.695068672299385
311,0.6077170372009277,0.6821757396978636,0.35731237494293905,0.3633440434932709,mmlu_offline:llama2-13b_chat:philosophy,test,21.217176131904125
35,0.4285714328289032,0.5078671328671329,0.5703611016273498,0.37142857909202576,mmlu_offline:llama2-13b_chat:prehistory,validation,3.069488720037043
324,0.6419752836227417,0.6897577560560986,0.30678794983728436,0.4413580298423767,mmlu_offline:llama2-13b_chat:prehistory,test,24.519725273363292
31,0.7419354915618896,0.6615384615384615,0.22962819568572507,0.16129031777381897,mmlu_offline:llama2-13b_chat:professional_accounting,validation,4.313877576030791
282,0.741134762763977,0.69947268682541,0.22745825043806794,0.1879432648420334,mmlu_offline:llama2-13b_chat:professional_accounting,test,36.86860131844878
170,0.6117647290229797,0.5926739926739927,0.3436800062656402,0.38235294818878174,mmlu_offline:llama2-13b_chat:professional_law,validation,44.23631876986474
1534,0.5945241451263428,0.5786704119850188,0.34594838871017114,0.34810951352119446,mmlu_offline:llama2-13b_chat:professional_law,test,406.4622914120555
31,0.5161290168762207,0.6545454545454545,0.4709334815702131,0.35483869910240173,mmlu_offline:llama2-13b_chat:professional_medicine,validation,6.725331904366612
272,0.5882353186607361,0.6404210664392995,0.3702881607939216,0.2904411852359772,mmlu_offline:llama2-13b_chat:professional_medicine,test,58.633875699713826
69,0.5652173757553101,0.673076923076923,0.4099230446677277,0.37681159377098083,mmlu_offline:llama2-13b_chat:professional_psychology,validation,6.371007606387138
612,0.6225489974021912,0.6906102594339623,0.3445193981812671,0.3464052379131317,mmlu_offline:llama2-13b_chat:professional_psychology,test,52.98278487846255
12,0.5833333134651184,0.875,0.4260301192601522,0.1666666716337204,mmlu_offline:llama2-13b_chat:public_relations,validation,1.3711266918107867
110,0.6545454263687134,0.719233746130031,0.32225664258003234,0.30909091234207153,mmlu_offline:llama2-13b_chat:public_relations,test,8.427720089443028
27,0.6666666865348816,0.7222222222222222,0.2898563345273335,0.5555555820465088,mmlu_offline:llama2-13b_chat:security_studies,validation,3.0257359836250544
245,0.7061224579811096,0.7034991819128366,0.24621002309176393,0.6612244844436646,mmlu_offline:llama2-13b_chat:security_studies,test,24.922850728034973
22,0.6363636255264282,0.6837606837606838,0.3460122103040869,0.5909090638160706,mmlu_offline:llama2-13b_chat:sociology,validation,1.79721515532583
201,0.6517412662506104,0.7644394394394394,0.33619205957621484,0.447761207818985,mmlu_offline:llama2-13b_chat:sociology,test,14.320084837265313
11,0.8181818127632141,0.9583333333333333,0.1436291933059692,0.7272727489471436,mmlu_offline:llama2-13b_chat:us_foreign_policy,validation,1.1594140334054828
100,0.6700000166893005,0.6960520876395204,0.30461268007755277,0.5899999737739563,mmlu_offline:llama2-13b_chat:us_foreign_policy,test,7.28918033093214
18,0.6111111044883728,0.6625,0.3565604421827528,0.5555555820465088,mmlu_offline:llama2-13b_chat:virology,validation,1.7811186956241727
166,0.6265060305595398,0.6552123243933589,0.336810227259096,0.34939759969711304,mmlu_offline:llama2-13b_chat:virology,test,12.178631247021258
19,0.8947368264198303,0.9285714285714286,0.15376435455523035,0.6315789222717285,mmlu_offline:llama2-13b_chat:world_religions,validation,1.456168849952519
171,0.719298243522644,0.7964265212399542,0.24940633634377649,0.6081871390342712,mmlu_offline:llama2-13b_chat:world_religions,test,10.184655737131834
