N,logits_ece,acc,unc_acc,unc_auroc,unc_ece,dataset,split,ts
11,0.435711451552131,0.27272728085517883,0.7272727489471436,0.10416666666666666,0.2567471645095132,mmlu:abstract_algebra,validation,5.152378216007492
100,0.34320432335138323,0.2199999988079071,0.7799999713897705,0.6847319347319346,0.12378907263278963,mmlu:abstract_algebra,test,6.604101942997659
14,0.13757130077907015,0.7857142686843872,0.2142857164144516,0.6666666666666666,0.6721540348870414,mmlu:anatomy,validation,1.1799452069972176
135,0.23850399101222003,0.43703705072402954,0.5629629492759705,0.3693131132917039,0.35457177647837884,mmlu:anatomy,test,9.112093848001678
16,0.2690412849187851,0.3125,0.6875,0.5,0.18359375,mmlu:astronomy,validation,1.6191838730010204
152,0.24981828916229698,0.3618420958518982,0.6381579041481018,0.5,0.23293585526315785,mmlu:astronomy,test,14.059173754998483
11,0.2799422686750239,0.5454545617103577,0.4545454680919647,0.5,0.20561079545454547,mmlu:business_ethics,validation,1.2259905709943268
100,0.23370743960142135,0.4699999988079071,0.5299999713897705,0.5,0.13015624999999997,mmlu:business_ethics,test,9.079532855015714
29,0.31901082602040526,0.48275861144065857,0.517241358757019,0.530952380952381,0.26724137519967966,mmlu:clinical_knowledge,validation,2.3116014529950917
265,0.2069326614433864,0.501886785030365,0.501886785030365,0.549669628616997,0.2802329191621744,mmlu:clinical_knowledge,test,18.797000090009533
16,0.342888206243515,0.25,0.75,0.8541666666666666,0.08959960937500003,mmlu:college_biology,validation,1.4018234710092656
144,0.1770010516047478,0.4722222089767456,0.5277777910232544,0.5443111455108359,0.23711480531427598,mmlu:college_biology,test,11.833697994996328
8,0.40616875141859055,0.25,0.75,0.3333333333333333,0.08056641370058061,mmlu:college_chemistry,validation,0.8616984559921548
100,0.36922240763902664,0.17000000178813934,0.8299999833106995,0.5846917080085047,0.07656250178813939,mmlu:college_chemistry,test,8.654545318975579
11,0.3835678913376548,0.1818181872367859,0.8181818127632141,0.5,0.09552556818181823,mmlu:college_computer_science,validation,1.5013491139980033
100,0.35432183086872104,0.20000000298023224,0.800000011920929,0.5,0.07734375000000004,mmlu:college_computer_science,test,11.380903151002713
11,0.169007192958485,0.4545454680919647,0.5454545617103577,0.5,0.14204545454545459,mmlu:college_mathematics,validation,1.2116853879997507
100,0.22035382628440858,0.25999999046325684,0.7400000095367432,0.5,0.05249999999999999,mmlu:college_mathematics,test,8.752866628987249
22,0.3394288732246919,0.3181818127632141,0.6363636255264282,0.6190476190476191,0.11594457517970691,mmlu:college_medicine,validation,2.0955548089987133
173,0.33964149042361047,0.34682080149650574,0.589595377445221,0.6174041297935103,0.06604497039938251,mmlu:college_medicine,test,17.863619859999744
11,0.34916770729151636,0.4545454680919647,0.5454545617103577,0.36666666666666664,0.21946022727272724,mmlu:college_physics,validation,1.081718032975914
102,0.32260790233518566,0.23529411852359772,0.7647058963775635,0.4919871794871795,0.0524663165503857,mmlu:college_physics,test,8.046658912993735
11,0.43841116536747315,0.3636363744735718,0.3636363744735718,0.44642857142857145,0.25426136363636365,mmlu:computer_security,validation,1.1074848890129942
100,0.26215096294879914,0.49000000953674316,0.4399999976158142,0.5588235294117647,0.12316404640674594,mmlu:computer_security,test,7.139958961022785
26,0.38970727301560915,0.3461538553237915,0.692307710647583,0.7320261437908496,0.13716946656887347,mmlu:conceptual_physics,validation,1.7739386879839003
235,0.3170571749514722,0.4170212745666504,0.5829787254333496,0.5201847162222554,0.16476064539970237,mmlu:conceptual_physics,test,13.76793659600662
12,0.5215101192394892,0.0833333358168602,0.0833333358168602,0.5,0.4674479166666667,mmlu:econometrics,validation,1.336060813977383
114,0.26633992477467183,0.3245614171028137,0.3245614171028137,0.5,0.22621984649122806,mmlu:econometrics,test,10.617410359001951
16,0.3756761681288481,0.1875,0.8125,0.46153846153846145,0.0939941331744194,mmlu:electrical_engineering,validation,1.2856449369865004
145,0.30078245134189213,0.2896551787853241,0.7103448510169983,0.6226306056403144,0.11872306486655923,mmlu:electrical_engineering,test,10.418595971015748
41,0.2963081983531394,0.17073170840740204,0.8292682766914368,0.7142857142857143,0.15548782377708245,mmlu:elementary_mathematics,validation,3.7217129379860125
378,0.3342761815225006,0.14814814925193787,0.8518518805503845,0.47044143744454303,0.08783895707635023,mmlu:elementary_mathematics,test,31.290433943999233
14,0.40794134778635843,0.2142857164144516,0.7857142686843872,0.5,0.1333705357142857,mmlu:formal_logic,validation,1.499625478987582
126,0.3623097070625851,0.1587301641702652,0.841269850730896,0.5,0.18892609126984128,mmlu:formal_logic,test,11.688897879008437
10,0.3196156546473503,0.5,0.5,0.42,0.2707031309604645,mmlu:global_facts,validation,0.9678200620110147
100,0.1995510682463646,0.3799999952316284,0.6200000047683716,0.5573005093378608,0.15550779700279238,mmlu:global_facts,test,7.205722275015432
32,0.2674458120018244,0.34375,0.65625,0.40476190476190477,0.24023437313735482,mmlu:high_school_biology,validation,2.801913080998929
310,0.19146540328379597,0.448387086391449,0.551612913608551,0.5867095797046573,0.2666834506296343,mmlu:high_school_biology,test,25.743753553018905
22,0.29074718735434796,0.3181818127632141,0.7272727489471436,0.6142857142857143,0.12624288689006466,mmlu:high_school_chemistry,validation,1.9915828410012182
203,0.3154352559831929,0.2857142984867096,0.7142857313156128,0.5238406658739596,0.11503233404582358,mmlu:high_school_chemistry,test,16.53509707300691
9,0.3600997924804687,0.3333333432674408,0.6666666865348816,0.5,0.03776041666666663,mmlu:high_school_computer_science,validation,1.3870417249854654
100,0.31326128482818605,0.3100000023841858,0.6899999976158142,0.5,0.06109374999999995,mmlu:high_school_computer_science,test,12.620433210016927
18,0.4509513485762808,0.2222222238779068,0.7777777910232544,0.5,0.2582465277777778,mmlu:high_school_european_history,validation,6.90940994900302
165,0.3561936970913049,0.34545454382896423,0.6545454263687134,0.5,0.13501420454545454,mmlu:high_school_european_history,test,61.32714297398343
22,0.11429377712986687,0.6818181872367859,0.3181818127632141,0.34285714285714286,0.49005679650740186,mmlu:high_school_geography,validation,1.640708576014731
198,0.22402376779402147,0.5101010203361511,0.4898989796638489,0.5992650811472899,0.30806109399506537,mmlu:high_school_geography,test,13.755815570999403
21,0.4873556877885546,0.3333333432674408,0.6666666865348816,0.7142857142857143,0.1449032851627895,mmlu:high_school_government_and_politics,validation,1.857361619011499
193,0.11746774065679837,0.6994818449020386,0.30051812529563904,0.5589399744572159,0.3779752093893259,mmlu:high_school_government_and_politics,test,15.197227741009556
43,0.28634950310684915,0.44186046719551086,0.5581395626068115,0.5986842105263158,0.09583939785181093,mmlu:high_school_macroeconomics,validation,3.2175556459988
390,0.24269767533510161,0.4076923131942749,0.5923076868057251,0.5307931062648045,0.0792067492619539,mmlu:high_school_macroeconomics,test,27.185650936997263
29,0.3373511238344785,0.2068965584039688,0.7931034564971924,0.5652173913043478,0.17120153534001314,mmlu:high_school_mathematics,validation,2.560964829986915
270,0.2797751958723422,0.20370370149612427,0.7962962985038757,0.5237209302325582,0.1675057934390174,mmlu:high_school_mathematics,test,21.744709598016925
26,0.3587234341181242,0.38461539149284363,0.42307692766189575,0.528125,0.10787260990876418,mmlu:high_school_microeconomics,validation,2.038423040008638
238,0.30095428666647744,0.34873950481414795,0.39915966987609863,0.5486980178779634,0.14221706109888413,mmlu:high_school_microeconomics,test,16.750782728980994
17,0.2866739718353047,0.1764705926179886,0.8235294222831726,0.5357142857142857,0.23230699230642882,mmlu:high_school_physics,validation,1.7993517199938651
151,0.26189394265610655,0.22516556084156036,0.7682119011878967,0.5418552036199096,0.17182325685261102,mmlu:high_school_physics,test,13.132606404979015
60,0.17472776869932813,0.6666666865348816,0.3333333432674408,0.573125,0.38717448314030967,mmlu:high_school_psychology,validation,5.120171391987242
545,0.13937156069169354,0.5834862589836121,0.4165137708187103,0.5015446208406062,0.29501864844505943,mmlu:high_school_psychology,test,45.19046232898836
23,0.25763335435286816,0.260869562625885,0.260869562625885,0.5,0.2586616847826087,mmlu:high_school_statistics,validation,2.7135768770240247
216,0.3012111377384928,0.24074074625968933,0.24074074625968933,0.5,0.2787905092592593,mmlu:high_school_statistics,test,24.496323428000323
22,0.25062222101471643,0.5,0.5,0.5,0.05078125,mmlu:high_school_us_history,validation,6.5189425040152855
204,0.1585016294437296,0.5245097875595093,0.47549018263816833,0.5,0.07529105392156865,mmlu:high_school_us_history,test,58.92175701900851
26,0.32303870068146634,0.42307692766189575,0.5769230723381042,0.5,0.010516826923076872,mmlu:high_school_world_history,validation,5.7663906830130145
237,0.13938282662806128,0.5569620132446289,0.4430379867553711,0.5,0.12336827531645572,mmlu:high_school_world_history,test,47.60196209300193
23,0.24969118056089978,0.695652186870575,0.30434781312942505,0.5803571428571428,0.6467391304347826,mmlu:human_aging,validation,1.5273930119874422
223,0.14104987300030322,0.5560538172721863,0.4439461827278137,0.530058651026393,0.4686624127118577,mmlu:human_aging,test,13.383796289010206
12,0.22208079198996222,0.4166666567325592,0.5833333134651184,0.4285714285714286,0.19075520833333337,mmlu:human_sexuality,validation,0.9974637760024052
131,0.18937305070971716,0.5038167834281921,0.49618321657180786,0.5341491841491841,0.2660722591494786,mmlu:human_sexuality,test,8.85748078400502
13,0.13257774481406578,0.9230769276618958,0.07692307978868484,0.5,0.4621394230769231,mmlu:international_law,validation,1.4545678669819608
121,0.18286301799056942,0.6198347210884094,0.3801652789115906,0.5,0.15889721074380164,mmlu:international_law,test,11.541755779006053
11,0.28368493914604187,0.3636363744735718,0.4545454680919647,0.375,0.06534091992811725,mmlu:jurisprudence,validation,0.9806028890016023
108,0.16812362604671055,0.5370370149612427,0.5092592835426331,0.46086206896551724,0.006980622256243629,mmlu:jurisprudence,test,7.736966519994894
18,0.2873121996720632,0.6111111044883728,0.3888888955116272,0.5974025974025974,0.24348959657880997,mmlu:logical_fallacies,validation,1.5755294970003888
163,0.23609467463259318,0.46625766158103943,0.5214723944664001,0.46566848154869933,0.11903278666771264,mmlu:logical_fallacies,test,12.32539725600509
11,0.4027558429674669,0.09090909361839294,0.09090909361839294,0.5,0.49112215909090906,mmlu:machine_learning,validation,1.2876439759857021
112,0.38370391460401665,0.1875,0.1875,0.5,0.39453125,mmlu:machine_learning,test,10.619939845986664
11,0.20211967013098978,0.7272727489471436,0.27272728085517883,0.625,0.6441761309450323,mmlu:management,validation,0.8582642819965258
103,0.1565075264972391,0.6407766938209534,0.35922330617904663,0.4248566748566749,0.5230582697877606,mmlu:management,test,5.819405638001626
25,0.23161819458007815,0.7200000286102295,0.4000000059604645,0.8174603174603176,0.16265623092651366,mmlu:marketing,validation,2.032425780023914
234,0.06871425494169578,0.7264957427978516,0.3461538553237915,0.5649816176470589,0.20190639475471953,mmlu:marketing,test,16.479725655983202
11,0.2477659528905695,0.7272727489471436,0.27272728085517883,0.3125,0.34978692098097364,mmlu:medical_genetics,validation,1.0012557749869302
100,0.24025268971920016,0.44999998807907104,0.550000011920929,0.5836363636363636,0.11269532561302188,mmlu:medical_genetics,test,6.560407960001612
86,0.15050928017427756,0.5813953280448914,0.41860464215278625,0.5427777777777778,0.39625727575878766,mmlu:miscellaneous,validation,5.169492168992292
783,0.1146790961500633,0.6475095748901367,0.351213276386261,0.5720099762741904,0.456327818895513,mmlu:miscellaneous,test,46.48714333298267
38,0.3289902782753894,0.4736842215061188,0.5263158082962036,0.4527777777777777,0.20970393638861803,mmlu:moral_disputes,validation,3.133018483989872
346,0.29000350832939154,0.4566473960876465,0.5433526039123535,0.4652908699165096,0.18179866085851812,mmlu:moral_disputes,test,27.130365226999857
100,0.35280446857213976,0.23999999463558197,0.7599999904632568,0.5,0.06078125000000001,mmlu:moral_scenarios,validation,9.458276881021447
895,0.35231019854545587,0.24581006169319153,0.7541899681091309,0.5,0.05497119413407825,mmlu:moral_scenarios,test,82.39322563901078
33,0.305375851464994,0.39393940567970276,0.39393940567970276,0.5,0.16074810606060608,mmlu:nutrition,validation,3.2803752289910335
306,0.28601749708839497,0.34967321157455444,0.34967321157455444,0.5,0.20501429738562094,mmlu:nutrition,test,28.017810833000112
34,0.3604088644771015,0.5,0.44117647409439087,0.31141868512110726,0.2081801365403568,mmlu:philosophy,validation,2.5258446029911283
311,0.22598661391298108,0.5691318511962891,0.4244372844696045,0.539316131208365,0.20088925890600565,mmlu:philosophy,test,19.744469259981997
35,0.33186736617769513,0.37142857909202576,0.6285714507102966,0.5524475524475525,0.10479909181594846,mmlu:prehistory,validation,3.2567114740086254
324,0.23025649224902373,0.4783950746059418,0.5216049551963806,0.5231341859133423,0.20823686910264286,mmlu:prehistory,test,27.515614431991708
31,0.3193904180680552,0.25806450843811035,0.25806450843811035,0.5,0.25756048387096775,mmlu:professional_accounting,validation,3.3334531630098354
282,0.2608700836381168,0.304964542388916,0.304964542388916,0.5,0.2106604609929078,mmlu:professional_accounting,test,29.30283146101283
170,0.3461448287262636,0.24705882370471954,0.24705882370471954,0.5,0.2763786764705882,mmlu:professional_law,validation,38.03947444498772
1534,0.3225551533768976,0.2737939953804016,0.2737939953804016,0.5,0.24964349739243807,mmlu:professional_law,test,343.9277661179949
31,0.22453524124237798,0.35483869910240173,0.35483869910240173,0.5,0.17250504032258063,mmlu:professional_medicine,validation,4.921307724987855
272,0.19499033338883343,0.3345588147640228,0.3345588147640228,0.5,0.19278492647058826,mmlu:professional_medicine,test,42.81935451499885
69,0.301143209139506,0.3913043439388275,0.6086956262588501,0.5,0.05536684782608692,mmlu:professional_psychology,validation,6.4665369060239755
612,0.2958036736140844,0.3741829991340637,0.6258170008659363,0.5,0.038245506535947715,mmlu:professional_psychology,test,54.0769560940098
12,0.379655584692955,0.5,0.5,0.6111111111111112,0.17903647323449454,mmlu:public_relations,validation,1.176618691009935
110,0.19797793030738833,0.5454545617103577,0.44545453786849976,0.5901666666666667,0.17656249078837305,mmlu:public_relations,test,8.14033283101162
27,0.3144044489772232,0.48148149251937866,0.5185185074806213,0.5,0.07523148148148151,mmlu:security_studies,validation,4.696478514990304
245,0.32743740325071374,0.3510203957557678,0.6489796042442322,0.5,0.055229591836734704,mmlu:security_studies,test,40.60252473998116
22,0.14946672997691415,0.7727272510528564,0.22727273404598236,0.3470588235294117,0.48508524352853954,mmlu:sociology,validation,1.7680921659921296
201,0.15348214756197004,0.6815920472145081,0.31840795278549194,0.5339872262773723,0.39130521057850093,mmlu:sociology,test,15.260607714997604
11,0.08332994851199065,0.7272727489471436,0.27272728085517883,0.7500000000000001,0.43004261363636365,mmlu:us_foreign_policy,validation,1.0537050339917187
100,0.13942405581474307,0.699999988079071,0.3400000035762787,0.5292857142857142,0.3348046863079071,mmlu:us_foreign_policy,test,7.402157010015799
18,0.21940550456444424,0.4444444477558136,0.5,0.1875,0.2766926950878567,mmlu:virology,validation,1.673293620988261
166,0.3263759720756348,0.40963855385780334,0.5903614163398743,0.5832082833133254,0.12881213713841266,mmlu:virology,test,11.130571868998231
19,0.07844283078846179,0.7894737124443054,0.21052631735801697,0.6499999999999999,0.6622121710526314,mmlu:world_religions,validation,1.215613514999859
171,0.08099033522327041,0.6549707651138306,0.34502923488616943,0.5518311138014528,0.5373949063451666,mmlu:world_religions,test,9.44027164101135
