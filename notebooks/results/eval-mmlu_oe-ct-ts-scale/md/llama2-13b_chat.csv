N,acc,unc_acc,unc_auroc,unc_ece,dataset,split,ts
11,0.09090909361839294,0.9090909361839294,0.0,0.33601289987564087,mmlu_offline:llama2-13b_chat:abstract_algebra,validation,2.924777691718191
100,0.3499999940395355,0.6399999856948853,0.5978021978021977,0.081602543592453,mmlu_offline:llama2-13b_chat:abstract_algebra,test,6.2878891080617905
14,0.2857142984867096,0.3571428656578064,0.5875,0.35284835100173956,mmlu_offline:llama2-13b_chat:anatomy,validation,0.8613473512232304
135,0.42222222685813904,0.6666666865348816,0.7871120107962213,0.044841483787254076,mmlu_offline:llama2-13b_chat:anatomy,test,6.8717083227820694
16,0.5,0.5625,0.71875,0.15860828012228012,mmlu_offline:llama2-13b_chat:astronomy,validation,1.245227260980755
152,0.5263158082962036,0.6776315569877625,0.7565104166666665,0.06301706441138921,mmlu_offline:llama2-13b_chat:astronomy,test,10.108741105999798
11,0.5454545617103577,0.4545454680919647,0.6333333333333333,0.361523216420954,mmlu_offline:llama2-13b_chat:business_ethics,validation,1.113907303661108
100,0.3199999928474426,0.5600000023841858,0.7040441176470588,0.12349912822246553,mmlu_offline:llama2-13b_chat:business_ethics,test,8.3290861742571
29,0.24137930572032928,0.6551724076271057,0.7467532467532467,0.10673968956388276,mmlu_offline:llama2-13b_chat:clinical_knowledge,validation,1.883109507150948
265,0.2981131970882416,0.6301887035369873,0.6883762079760447,0.0844615124306589,mmlu_offline:llama2-13b_chat:clinical_knowledge,test,15.254705116152763
16,0.25,0.75,0.7708333333333333,0.11459039896726611,mmlu_offline:llama2-13b_chat:college_biology,validation,1.262073383666575
144,0.3958333432674408,0.5972222089767456,0.7126436781609196,0.11618932460745175,mmlu_offline:llama2-13b_chat:college_biology,test,10.3470493350178
8,0.0,0.75,,0.350631020963192,mmlu_offline:llama2-13b_chat:college_chemistry,validation,0.878048341255635
100,0.17000000178813934,0.8199999928474426,0.8812898653437278,0.16314433455467228,mmlu_offline:llama2-13b_chat:college_chemistry,test,8.189607089851052
11,0.27272728085517883,0.8181818127632141,0.7916666666666667,0.12230764735828747,mmlu_offline:llama2-13b_chat:college_computer_science,validation,1.660993305966258
100,0.23999999463558197,0.7400000095367432,0.6998355263157894,0.05780493378639223,mmlu_offline:llama2-13b_chat:college_computer_science,test,13.91498898807913
11,0.0,1.0,,0.26564196023074066,mmlu_offline:llama2-13b_chat:college_mathematics,validation,1.3562532369978726
100,0.09000000357627869,0.8500000238418579,0.6288156288156288,0.1304007703065872,mmlu_offline:llama2-13b_chat:college_mathematics,test,10.082178022712469
22,0.5,0.6363636255264282,0.6818181818181818,0.2126084024255926,mmlu_offline:llama2-13b_chat:college_medicine,validation,1.8275535241700709
173,0.3988439440727234,0.6878612637519836,0.7888795986622074,0.061947052533915976,mmlu_offline:llama2-13b_chat:college_medicine,test,18.47675384907052
11,0.27272728085517883,0.8181818127632141,0.9166666666666666,0.21136786179109057,mmlu_offline:llama2-13b_chat:college_physics,validation,1.1537023829296231
102,0.1568627506494522,0.6960784196853638,0.721656976744186,0.11211740853739717,mmlu_offline:llama2-13b_chat:college_physics,test,8.6219073520042
11,0.5454545617103577,0.7272727489471436,0.7333333333333333,0.19962917132811114,mmlu_offline:llama2-13b_chat:computer_security,validation,1.0174574116244912
100,0.550000011920929,0.6100000143051147,0.6745454545454546,0.09004062592983245,mmlu_offline:llama2-13b_chat:computer_security,test,6.244002409745008
26,0.3461538553237915,0.5,0.5816993464052287,0.1775500040787917,mmlu_offline:llama2-13b_chat:conceptual_physics,validation,1.5721378889866173
235,0.4553191363811493,0.651063859462738,0.6959331191588785,0.06778235790577344,mmlu_offline:llama2-13b_chat:conceptual_physics,test,11.92942306958139
12,0.25,0.5833333134651184,0.9074074074074074,0.22669031719366708,mmlu_offline:llama2-13b_chat:econometrics,validation,1.4627914619632065
114,0.1315789520740509,0.6491228342056274,0.5612794612794613,0.11264459187524357,mmlu_offline:llama2-13b_chat:econometrics,test,11.717222892213613
16,0.25,0.8125,0.75,0.12600571662187574,mmlu_offline:llama2-13b_chat:electrical_engineering,validation,1.274960109964013
145,0.24827586114406586,0.7586206793785095,0.8132008154943934,0.09141404135473845,mmlu_offline:llama2-13b_chat:electrical_engineering,test,10.281438989099115
41,0.26829269528388977,0.4878048896789551,0.5196969696969697,0.20163682321222817,mmlu_offline:llama2-13b_chat:elementary_mathematics,validation,3.3874745881184936
378,0.3174603283405304,0.6190476417541504,0.6955426356589147,0.07130554208049067,mmlu_offline:llama2-13b_chat:elementary_mathematics,test,28.5348957660608
14,0.2142857164144516,0.7142857313156128,0.7878787878787878,0.12919931752341135,mmlu_offline:llama2-13b_chat:formal_logic,validation,1.345156011171639
126,0.261904776096344,0.579365074634552,0.5581622678396871,0.17906681034300065,mmlu_offline:llama2-13b_chat:formal_logic,test,10.771725826896727
10,0.20000000298023224,0.800000011920929,0.4375,0.1482771217823029,mmlu_offline:llama2-13b_chat:global_facts,validation,0.8414925960823894
100,0.12999999523162842,0.8399999737739563,0.6445623342175066,0.13172846436500546,mmlu_offline:llama2-13b_chat:global_facts,test,6.688161844853312
32,0.3125,0.6875,0.740909090909091,0.07761140540242195,mmlu_offline:llama2-13b_chat:high_school_biology,validation,2.357073259074241
310,0.42258065938949585,0.7161290049552917,0.7689027250629024,0.0372930605565348,mmlu_offline:llama2-13b_chat:high_school_biology,test,22.01662171119824
22,0.1818181872367859,0.8636363744735718,0.7916666666666666,0.18698374249718402,mmlu_offline:llama2-13b_chat:high_school_chemistry,validation,1.956367057748139
203,0.1822660118341446,0.8275862336158752,0.7643275805926408,0.14248108951916247,mmlu_offline:llama2-13b_chat:high_school_chemistry,test,16.22834947006777
9,0.6666666865348816,0.5555555820465088,0.6666666666666667,0.26442405912611217,mmlu_offline:llama2-13b_chat:high_school_computer_science,validation,1.3161510317586362
100,0.4300000071525574,0.6399999856948853,0.689922480620155,0.09135037422180173,mmlu_offline:llama2-13b_chat:high_school_computer_science,test,12.456033022142947
18,0.7777777910232544,0.7222222089767456,0.8214285714285714,0.16332480973667568,mmlu_offline:llama2-13b_chat:high_school_european_history,validation,9.321132594253868
165,0.7575757503509521,0.8242424130439758,0.8372,0.08176180449399079,mmlu_offline:llama2-13b_chat:high_school_european_history,test,84.87500277766958
22,0.5,0.7272727489471436,0.8264462809917356,0.0554869472980499,mmlu_offline:llama2-13b_chat:high_school_geography,validation,1.4374044258147478
198,0.39393940567970276,0.691919207572937,0.7792735042735043,0.09031445209426112,mmlu_offline:llama2-13b_chat:high_school_geography,test,11.158760006073862
21,0.523809552192688,0.8095238208770752,0.8909090909090909,0.0433771127746219,mmlu_offline:llama2-13b_chat:high_school_government_and_politics,validation,1.4958859151229262
193,0.6321243643760681,0.7046632170677185,0.7555414453936734,0.07019513814560494,mmlu_offline:llama2-13b_chat:high_school_government_and_politics,test,11.765992033760995
43,0.4883720874786377,0.7209302186965942,0.7651515151515151,0.11416227595750675,mmlu_offline:llama2-13b_chat:high_school_macroeconomics,validation,2.5723755257204175
390,0.3692307770252228,0.6717948913574219,0.7291525519421861,0.03302708619680162,mmlu_offline:llama2-13b_chat:high_school_macroeconomics,test,21.878624804783612
29,0.06896551698446274,0.8965517282485962,0.7314814814814815,0.20216579683895766,mmlu_offline:llama2-13b_chat:high_school_mathematics,validation,2.7532328846864402
270,0.10000000149011612,0.8370370268821716,0.6650663008687699,0.12019343243704907,mmlu_offline:llama2-13b_chat:high_school_mathematics,test,23.839787147939205
26,0.3076923191547394,0.5384615659713745,0.78125,0.19947971518223104,mmlu_offline:llama2-13b_chat:high_school_microeconomics,validation,1.6217360366135836
238,0.36554622650146484,0.5882353186607361,0.7922280581563523,0.15187684553010125,mmlu_offline:llama2-13b_chat:high_school_microeconomics,test,13.410531810019165
17,0.29411765933036804,0.5882353186607361,0.6666666666666666,0.1713838542208952,mmlu_offline:llama2-13b_chat:high_school_physics,validation,1.6585093140602112
151,0.17218543589115143,0.6092715263366699,0.5412307692307694,0.1007270457728809,mmlu_offline:llama2-13b_chat:high_school_physics,test,12.72924009617418
60,0.6000000238418579,0.7833333611488342,0.8194444444444444,0.09473523696263632,mmlu_offline:llama2-13b_chat:high_school_psychology,validation,4.520486470777541
545,0.5064220428466797,0.6752293705940247,0.7646947901513927,0.06445981242241117,mmlu_offline:llama2-13b_chat:high_school_psychology,test,39.56922867381945
23,0.17391304671764374,0.6521739363670349,0.6052631578947368,0.36288234721059387,mmlu_offline:llama2-13b_chat:high_school_statistics,validation,2.6571049788035452
216,0.25925925374031067,0.625,0.7146763392857143,0.13984982007079655,mmlu_offline:llama2-13b_chat:high_school_statistics,test,23.86818619677797
22,0.7727272510528564,0.7727272510528564,0.6176470588235294,0.06731172854250127,mmlu_offline:llama2-13b_chat:high_school_us_history,validation,8.862955653108656
204,0.6764705777168274,0.7450980544090271,0.7933684672815108,0.03676267143558054,mmlu_offline:llama2-13b_chat:high_school_us_history,test,81.08585158083588
26,0.6538461446762085,0.6153846383094788,0.5392156862745099,0.2288581362137428,mmlu_offline:llama2-13b_chat:high_school_world_history,validation,7.630858364049345
237,0.6033755540847778,0.7130801677703857,0.7517482517482519,0.08527295649806156,mmlu_offline:llama2-13b_chat:high_school_world_history,test,62.96390427323058
23,0.3913043439388275,0.782608687877655,0.8333333333333333,0.161975331928419,mmlu_offline:llama2-13b_chat:human_aging,validation,1.3611174221150577
223,0.3901345431804657,0.681614339351654,0.7375760649087221,0.044734981562524645,mmlu_offline:llama2-13b_chat:human_aging,test,11.781232669949532
12,0.3333333432674408,0.5833333134651184,0.5625,0.09952240188916522,mmlu_offline:llama2-13b_chat:human_sexuality,validation,0.8880283948965371
131,0.5114504098892212,0.6106870174407959,0.6438899253731345,0.13940515363489397,mmlu_offline:llama2-13b_chat:human_sexuality,test,7.751082205213606
13,0.6153846383094788,0.5384615659713745,0.5125,0.17404610835588896,mmlu_offline:llama2-13b_chat:international_law,validation,1.1034411741420627
121,0.6280992031097412,0.6859503984451294,0.6839181286549708,0.06419961314556027,mmlu_offline:llama2-13b_chat:international_law,test,8.799628449138254
11,0.1818181872367859,0.9090909361839294,0.9444444444444445,0.31420067765495996,mmlu_offline:llama2-13b_chat:jurisprudence,validation,0.8761010458692908
108,0.3611111044883728,0.6203703880310059,0.6614641397250093,0.07540421187877654,mmlu_offline:llama2-13b_chat:jurisprudence,test,6.6056220340542495
18,0.6666666865348816,0.8333333134651184,0.8333333333333334,0.11379363470607332,mmlu_offline:llama2-13b_chat:logical_fallacies,validation,1.3243404678069055
163,0.48466256260871887,0.7116564512252808,0.8613622664255576,0.08366519468693642,mmlu_offline:llama2-13b_chat:logical_fallacies,test,10.785804345738143
11,0.27272728085517883,0.7272727489471436,0.7291666666666666,0.21499926393682306,mmlu_offline:llama2-13b_chat:machine_learning,validation,1.1902174358256161
112,0.2946428656578064,0.6160714030265808,0.734752589182969,0.08961201512387822,mmlu_offline:llama2-13b_chat:machine_learning,test,10.43851729016751
11,0.6363636255264282,0.6363636255264282,0.5535714285714286,0.30977547710592096,mmlu_offline:llama2-13b_chat:management,validation,0.7441480248235166
103,0.42718446254730225,0.7669903039932251,0.8332049306625577,0.0789201276973613,mmlu_offline:llama2-13b_chat:management,test,4.996054999995977
25,0.23999999463558197,0.6399999856948853,0.7149122807017544,0.12968820810317996,mmlu_offline:llama2-13b_chat:marketing,validation,1.7933186823502183
234,0.44871795177459717,0.7179487347602844,0.8007751937984496,0.033229934354113706,mmlu_offline:llama2-13b_chat:marketing,test,14.152156833093613
11,0.9090909361839294,0.9090909361839294,1.0,0.16291973807594992,mmlu_offline:llama2-13b_chat:medical_genetics,validation,0.8004465959966183
100,0.44999998807907104,0.6600000262260437,0.7327272727272728,0.08382945835590364,mmlu_offline:llama2-13b_chat:medical_genetics,test,5.068212896119803
86,0.5581395626068115,0.7093023061752319,0.7694627192982456,0.057461888291115,mmlu_offline:llama2-13b_chat:miscellaneous,validation,4.424466106109321
783,0.618135392665863,0.7369093298912048,0.8027205008430305,0.05561344932626795,mmlu_offline:llama2-13b_chat:miscellaneous,test,40.35312076797709
38,0.42105263471603394,0.6842105388641357,0.8196022727272727,0.10374814585635536,mmlu_offline:llama2-13b_chat:moral_disputes,validation,2.6842470671981573
346,0.43063583970069885,0.6329479813575745,0.7099615030831601,0.08296599846354796,mmlu_offline:llama2-13b_chat:moral_disputes,test,22.846491407137364
100,0.4300000071525574,0.6000000238418579,0.6758465932272543,0.11351144790649415,mmlu_offline:llama2-13b_chat:moral_scenarios,validation,10.80326489219442
895,0.38100558519363403,0.618994414806366,0.6210683168002371,0.029420048231519107,mmlu_offline:llama2-13b_chat:moral_scenarios,test,94.99698648694903
33,0.3333333432674408,0.6363636255264282,0.7665289256198347,0.10597557551933058,mmlu_offline:llama2-13b_chat:nutrition,validation,2.6331121930852532
306,0.4542483687400818,0.6405228972434998,0.7702149657519493,0.0782773810274461,mmlu_offline:llama2-13b_chat:nutrition,test,23.52064684405923
34,0.3235294222831726,0.529411792755127,0.7470355731225297,0.20546569719034083,mmlu_offline:llama2-13b_chat:philosophy,validation,2.1382813481613994
311,0.3633440434932709,0.5787781476974487,0.6863993921516045,0.11866384667982242,mmlu_offline:llama2-13b_chat:philosophy,test,17.466245230287313
35,0.37142857909202576,0.4285714328289032,0.5314685314685315,0.28107923269271856,mmlu_offline:llama2-13b_chat:prehistory,validation,2.488820652011782
324,0.4413580298423767,0.6697530746459961,0.7334350732140789,0.05043925786459889,mmlu_offline:llama2-13b_chat:prehistory,test,20.566111042629927
31,0.16129031777381897,0.6451612710952759,0.6769230769230768,0.08894348529077345,mmlu_offline:llama2-13b_chat:professional_accounting,validation,3.856330662034452
282,0.1879432648420334,0.695035457611084,0.6985251709648184,0.045056599674495434,mmlu_offline:llama2-13b_chat:professional_accounting,test,33.67787153320387
170,0.38235294818878174,0.5882353186607361,0.5471794871794872,0.0912639800240012,mmlu_offline:llama2-13b_chat:professional_law,validation,42.567330585327
1534,0.34810951352119446,0.6310299634933472,0.6063501872659176,0.05906260619729253,mmlu_offline:llama2-13b_chat:professional_law,test,392.87491110991687
31,0.35483869910240173,0.5806451439857483,0.6909090909090909,0.16019342599376557,mmlu_offline:llama2-13b_chat:professional_medicine,validation,6.374134418088943
272,0.2904411852359772,0.6397058963775635,0.6741654095887716,0.0478191480917089,mmlu_offline:llama2-13b_chat:professional_medicine,test,55.71374244103208
69,0.37681159377098083,0.5507246255874634,0.6887298747763864,0.18736530300499732,mmlu_offline:llama2-13b_chat:professional_psychology,validation,5.544975752942264
612,0.3464052379131317,0.5571895241737366,0.6990271226415095,0.152090146261103,mmlu_offline:llama2-13b_chat:professional_psychology,test,45.40142521401867
12,0.1666666716337204,0.8333333134651184,0.9500000000000001,0.1326931267976761,mmlu_offline:llama2-13b_chat:public_relations,validation,0.9831221769563854
110,0.30909091234207153,0.7545454502105713,0.8312693498452012,0.08020559278401461,mmlu_offline:llama2-13b_chat:public_relations,test,6.938382555730641
27,0.5555555820465088,0.7777777910232544,0.8472222222222222,0.09031715437217992,mmlu_offline:llama2-13b_chat:security_studies,validation,2.5506128198467195
245,0.6612244844436646,0.7102040648460388,0.7605979473449352,0.07196980374200006,mmlu_offline:llama2-13b_chat:security_studies,test,21.89506671531126
22,0.5909090638160706,0.7727272510528564,0.7863247863247863,0.0789100473577326,mmlu_offline:llama2-13b_chat:sociology,validation,1.4547611558809876
201,0.447761207818985,0.6666666865348816,0.8115115115115116,0.06811825612291177,mmlu_offline:llama2-13b_chat:sociology,test,11.88922535115853
11,0.7272727489471436,0.9090909361839294,0.8541666666666667,0.14446155591444537,mmlu_offline:llama2-13b_chat:us_foreign_policy,validation,0.8861624430865049
100,0.5899999737739563,0.699999988079071,0.7974369574204216,0.06677664637565615,mmlu_offline:llama2-13b_chat:us_foreign_policy,test,5.939933323301375
18,0.5555555820465088,0.6111111044883728,0.7375,0.13175536857710946,mmlu_offline:llama2-13b_chat:virology,validation,1.4722597319632769
166,0.34939759969711304,0.6385542154312134,0.6625957854406129,0.12299259994403426,mmlu_offline:llama2-13b_chat:virology,test,9.967426088638604
19,0.6315789222717285,0.7894737124443054,0.8928571428571428,0.03846948711495651,mmlu_offline:llama2-13b_chat:world_religions,validation,1.0942466016858816
171,0.6081871390342712,0.707602322101593,0.8267795637198622,0.07213508246237771,mmlu_offline:llama2-13b_chat:world_religions,test,8.126432375982404
