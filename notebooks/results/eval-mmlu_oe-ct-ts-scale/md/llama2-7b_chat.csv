N,acc,unc_acc,unc_auroc,unc_ece,dataset,split,ts
11,0.09090909361839294,0.8181818127632141,0.8,0.1576220068064603,mmlu_offline:llama2-7b_chat:abstract_algebra,validation,3.791440586093813
100,0.20999999344348907,0.7799999713897705,0.7022302591922844,0.09968906819820403,mmlu_offline:llama2-7b_chat:abstract_algebra,test,4.442963818553835
14,0.2142857164144516,0.8571428656578064,0.8787878787878787,0.19058001041412354,mmlu_offline:llama2-7b_chat:anatomy,validation,0.7231114660389721
135,0.385185182094574,0.6888889074325562,0.7395736793327156,0.05146617933555887,mmlu_offline:llama2-7b_chat:anatomy,test,4.925713951699436
16,0.5625,0.5625,0.7777777777777777,0.16492115706205368,mmlu_offline:llama2-7b_chat:astronomy,validation,0.9067415199242532
152,0.42763158679008484,0.7105262875556946,0.7272325375773652,0.05289463698863985,mmlu_offline:llama2-7b_chat:astronomy,test,6.999512709211558
11,0.4545454680919647,0.5454545617103577,0.5,0.24153513258153744,mmlu_offline:llama2-7b_chat:business_ethics,validation,0.7894681240431964
100,0.33000001311302185,0.699999988079071,0.7105382180009046,0.05616568446159362,mmlu_offline:llama2-7b_chat:business_ethics,test,5.597239582799375
29,0.17241379618644714,0.8620689511299133,0.925,0.12701351272648775,mmlu_offline:llama2-7b_chat:clinical_knowledge,validation,1.2864786451682448
265,0.2792452871799469,0.7509434223175049,0.7307910004245082,0.04171527984007352,mmlu_offline:llama2-7b_chat:clinical_knowledge,test,10.51694648899138
16,0.375,0.6875,0.5666666666666667,0.09401553496718407,mmlu_offline:llama2-7b_chat:college_biology,validation,0.8576742568984628
144,0.3402777910232544,0.6736111044883728,0.6989258861439314,0.07274345598287049,mmlu_offline:llama2-7b_chat:college_biology,test,6.9878062517382205
8,0.0,0.875,,0.2131730169057846,mmlu_offline:llama2-7b_chat:college_chemistry,validation,0.7318213991820812
100,0.14000000059604645,0.8199999928474426,0.6615448504983389,0.06063468217849729,mmlu_offline:llama2-7b_chat:college_chemistry,test,5.512566352263093
11,0.27272728085517883,0.5454545617103577,0.5,0.20931277491829614,mmlu_offline:llama2-7b_chat:college_computer_science,validation,1.1190809532999992
100,0.17000000178813934,0.75,0.6580439404677534,0.07251463890075682,mmlu_offline:llama2-7b_chat:college_computer_science,test,8.911467256955802
11,0.0,0.9090909361839294,,0.1739081523635171,mmlu_offline:llama2-7b_chat:college_mathematics,validation,0.9163192086853087
100,0.2199999988079071,0.7300000190734863,0.6331585081585082,0.038953360319137584,mmlu_offline:llama2-7b_chat:college_mathematics,test,6.540102638769895
22,0.3181818127632141,0.7272727489471436,0.8095238095238095,0.15091721307147632,mmlu_offline:llama2-7b_chat:college_medicine,validation,1.2693311269395053
173,0.2947976887226105,0.7572254538536072,0.7524911603985858,0.04344967748388388,mmlu_offline:llama2-7b_chat:college_medicine,test,11.900484014768153
11,0.09090909361839294,0.9090909361839294,0.6,0.23787383599714798,mmlu_offline:llama2-7b_chat:college_physics,validation,0.8204453499056399
102,0.0882352963089943,0.8921568393707275,0.553763440860215,0.11723447897854969,mmlu_offline:llama2-7b_chat:college_physics,test,5.716156576760113
11,0.6363636255264282,0.6363636255264282,0.7857142857142857,0.15809860554608432,mmlu_offline:llama2-7b_chat:computer_security,validation,0.7619189056567848
100,0.5,0.6399999856948853,0.6658000000000001,0.06758894801139834,mmlu_offline:llama2-7b_chat:computer_security,test,4.406544419936836
26,0.1538461595773697,0.807692289352417,0.6931818181818181,0.0825771574790661,mmlu_offline:llama2-7b_chat:conceptual_physics,validation,1.1819981327280402
235,0.3787234127521515,0.6340425610542297,0.6184008003694011,0.09278666389749406,mmlu_offline:llama2-7b_chat:conceptual_physics,test,8.690956375095993
12,0.3333333432674408,0.5,0.65625,0.24616898596286776,mmlu_offline:llama2-7b_chat:econometrics,validation,0.9797621518373489
114,0.17543859779834747,0.7105262875556946,0.6117021276595743,0.05041883441439844,mmlu_offline:llama2-7b_chat:econometrics,test,7.553647073917091
16,0.1875,0.8125,0.5384615384615384,0.04140108823776242,mmlu_offline:llama2-7b_chat:electrical_engineering,validation,0.8770134611986578
145,0.2137930989265442,0.7931034564971924,0.6005942275042444,0.04088526265374542,mmlu_offline:llama2-7b_chat:electrical_engineering,test,6.948791827075183
41,0.3658536672592163,0.6341463327407837,0.6935897435897436,0.12094042650083217,mmlu_offline:llama2-7b_chat:elementary_mathematics,validation,2.352341136429459
378,0.28042328357696533,0.7407407164573669,0.6396018312985572,0.02754050351324536,mmlu_offline:llama2-7b_chat:elementary_mathematics,test,19.04858530405909
14,0.4285714328289032,0.5714285969734192,0.5729166666666666,0.19451246942792622,mmlu_offline:llama2-7b_chat:formal_logic,validation,0.9382872558198869
126,0.2539682686328888,0.7698412537574768,0.6120345744680852,0.09629893019085842,mmlu_offline:llama2-7b_chat:formal_logic,test,7.082069265190512
10,0.20000000298023224,0.800000011920929,0.75,0.11278347969055176,mmlu_offline:llama2-7b_chat:global_facts,validation,0.6639753179624677
100,0.07999999821186066,0.9300000071525574,0.546875,0.21217728257179258,mmlu_offline:llama2-7b_chat:global_facts,test,4.68964713299647
32,0.3125,0.6875,0.5954545454545455,0.08850042521953583,mmlu_offline:llama2-7b_chat:high_school_biology,validation,1.6225997498258948
310,0.3838709592819214,0.7032257914543152,0.7287386158651942,0.05190417016706158,mmlu_offline:llama2-7b_chat:high_school_biology,test,14.939310448244214
22,0.1818181872367859,0.7727272510528564,0.4027777777777778,0.0647717226635326,mmlu_offline:llama2-7b_chat:high_school_chemistry,validation,1.3598091606982052
203,0.1428571492433548,0.8423645496368408,0.7309750297265161,0.09497049494917167,mmlu_offline:llama2-7b_chat:high_school_chemistry,test,10.769725634716451
9,0.4444444477558136,0.5555555820465088,0.7000000000000001,0.24984444512261283,mmlu_offline:llama2-7b_chat:high_school_computer_science,validation,0.8935039937496185
100,0.4099999964237213,0.6299999952316284,0.6603968582058701,0.08053189039230349,mmlu_offline:llama2-7b_chat:high_school_computer_science,test,8.012611641082913
18,0.8333333134651184,0.7222222089767456,0.8222222222222222,0.21836487783326042,mmlu_offline:llama2-7b_chat:high_school_european_history,validation,5.688819132279605
165,0.7090908885002136,0.739393949508667,0.7810719373219374,0.06479751521890817,mmlu_offline:llama2-7b_chat:high_school_european_history,test,51.20666631311178
22,0.4545454680919647,0.7272727489471436,0.7416666666666667,0.17528115348382425,mmlu_offline:llama2-7b_chat:high_school_geography,validation,1.0347284227609634
198,0.3737373650074005,0.7222222089767456,0.7681451612903225,0.010641293694274594,mmlu_offline:llama2-7b_chat:high_school_geography,test,7.686441999860108
21,0.523809552192688,0.4761904776096344,0.6045454545454545,0.29703344333739506,mmlu_offline:llama2-7b_chat:high_school_government_and_politics,validation,1.056403855793178
193,0.5233160853385925,0.6943005323410034,0.784061558329746,0.062284569048510915,mmlu_offline:llama2-7b_chat:high_school_government_and_politics,test,8.145340892020613
43,0.39534884691238403,0.6279069781303406,0.5633484162895928,0.13745913394661835,mmlu_offline:llama2-7b_chat:high_school_macroeconomics,validation,1.8100224216468632
390,0.2897436022758484,0.7487179636955261,0.7509504488674483,0.04247351884841917,mmlu_offline:llama2-7b_chat:high_school_macroeconomics,test,15.212715715169907
29,0.0,1.0,,0.22552990091258085,mmlu_offline:llama2-7b_chat:high_school_mathematics,validation,1.880108899436891
270,0.0555555559694767,0.9407407641410828,0.6249673202614379,0.17667469580968226,mmlu_offline:llama2-7b_chat:high_school_mathematics,test,15.551011817995459
26,0.26923078298568726,0.807692289352417,0.8045112781954887,0.12282712642963116,mmlu_offline:llama2-7b_chat:high_school_microeconomics,validation,1.2126422780565917
238,0.3403361439704895,0.7100840210914612,0.6906503106078478,0.04703081280243495,mmlu_offline:llama2-7b_chat:high_school_microeconomics,test,9.252376422751695
17,0.0,0.529411792755127,,0.268827431342181,mmlu_offline:llama2-7b_chat:high_school_physics,validation,1.1942399530671537
151,0.18543046712875366,0.7880794405937195,0.7867305458768873,0.11327935765121154,mmlu_offline:llama2-7b_chat:high_school_physics,test,8.356903906911612
60,0.5666666626930237,0.6833333373069763,0.745475113122172,0.08173153201738992,mmlu_offline:llama2-7b_chat:high_school_psychology,validation,3.0801259032450616
545,0.5009174346923828,0.6642201542854309,0.7316917151475975,0.031485742822699614,mmlu_offline:llama2-7b_chat:high_school_psychology,test,26.526447812095284
23,0.17391304671764374,0.782608687877655,0.7631578947368421,0.12077145731967427,mmlu_offline:llama2-7b_chat:high_school_statistics,validation,1.7699251188896596
216,0.19907407462596893,0.7037037014961243,0.6596316709235113,0.08682027679902535,mmlu_offline:llama2-7b_chat:high_school_statistics,test,15.590168570168316
22,0.6363636255264282,0.8636363744735718,0.9196428571428572,0.1210628937591206,mmlu_offline:llama2-7b_chat:high_school_us_history,validation,5.407047918997705
204,0.5784313678741455,0.6960784196853638,0.7679345683878597,0.04656008733253852,mmlu_offline:llama2-7b_chat:high_school_us_history,test,49.18510963022709
26,0.7307692170143127,0.7307692170143127,0.7293233082706767,0.09745582479697006,mmlu_offline:llama2-7b_chat:high_school_world_history,validation,4.83043104223907
237,0.552742600440979,0.6371307969093323,0.7046305631571367,0.10349706054236817,mmlu_offline:llama2-7b_chat:high_school_world_history,test,38.66990203363821
23,0.30434781312942505,0.8260869383811951,0.7410714285714285,0.1988376145777495,mmlu_offline:llama2-7b_chat:human_aging,validation,0.9876020909287035
223,0.33183857798576355,0.7623318433761597,0.793578813713042,0.043014820648415775,mmlu_offline:llama2-7b_chat:human_aging,test,8.330117051955312
12,0.25,0.8333333134651184,0.7407407407407407,0.05437123278776805,mmlu_offline:llama2-7b_chat:human_sexuality,validation,0.651293474715203
131,0.4198473393917084,0.6488549709320068,0.6738038277511962,0.07607435907116372,mmlu_offline:llama2-7b_chat:human_sexuality,test,5.38763377815485
13,0.4615384638309479,0.692307710647583,0.6428571428571429,0.1040737904035128,mmlu_offline:llama2-7b_chat:international_law,validation,0.7885929467156529
121,0.6115702390670776,0.64462810754776,0.6937895342150661,0.09306848640284265,mmlu_offline:llama2-7b_chat:international_law,test,6.029397259000689
11,0.5454545617103577,0.5454545617103577,0.7666666666666666,0.17637657035480847,mmlu_offline:llama2-7b_chat:jurisprudence,validation,0.6505830720998347
108,0.5185185074806213,0.6203703880310059,0.6852678571428572,0.10795497839097626,mmlu_offline:llama2-7b_chat:jurisprudence,test,4.620923787355423
18,0.4444444477558136,0.5,0.53125,0.25023409393098617,mmlu_offline:llama2-7b_chat:logical_fallacies,validation,0.9828185755759478
163,0.42944785952568054,0.6625766754150391,0.7491551459293395,0.0440067440454214,mmlu_offline:llama2-7b_chat:logical_fallacies,test,7.832735363859683
11,0.3636363744735718,0.8181818127632141,0.7142857142857143,0.26383926651694556,mmlu_offline:llama2-7b_chat:machine_learning,validation,0.8647264428436756
112,0.1964285671710968,0.8214285969734192,0.765909090909091,0.1198279815060752,mmlu_offline:llama2-7b_chat:machine_learning,test,6.951629329007119
11,0.27272728085517883,0.8181818127632141,0.75,0.10141913457350302,mmlu_offline:llama2-7b_chat:management,validation,0.5804856321774423
103,0.3689320385456085,0.7669903039932251,0.8419028340080972,0.0767040351062145,mmlu_offline:llama2-7b_chat:management,test,3.677429936360568
25,0.20000000298023224,0.5199999809265137,0.72,0.17393449306488037,mmlu_offline:llama2-7b_chat:marketing,validation,1.3224964761175215
234,0.39743590354919434,0.6837607026100159,0.7333180812933731,0.07110864713660672,mmlu_offline:llama2-7b_chat:marketing,test,9.615090074017644
11,0.7272727489471436,0.7272727489471436,0.875,0.1949360424822027,mmlu_offline:llama2-7b_chat:medical_genetics,validation,0.5934750949963927
100,0.4099999964237213,0.6399999856948853,0.7035965274906986,0.05834604620933533,mmlu_offline:llama2-7b_chat:medical_genetics,test,3.724574724212289
86,0.41860464215278625,0.7093023061752319,0.7111111111111111,0.060342528099237486,mmlu_offline:llama2-7b_chat:miscellaneous,validation,3.203953603282571
783,0.4533844292163849,0.733077883720398,0.7638969329998684,0.0313227542058718,mmlu_offline:llama2-7b_chat:miscellaneous,test,28.71114578191191
38,0.4736842215061188,0.4736842215061188,0.7513888888888889,0.2583320893739399,mmlu_offline:llama2-7b_chat:moral_disputes,validation,1.8879776070825756
346,0.41040462255477905,0.589595377445221,0.6112089201877935,0.10424880106325098,mmlu_offline:llama2-7b_chat:moral_disputes,test,15.814881678670645
100,0.5099999904632568,0.5,0.5948379351740696,0.22662804901599887,mmlu_offline:llama2-7b_chat:moral_scenarios,validation,7.099277010653168
895,0.46145251393318176,0.5430167317390442,0.6000447087900495,0.16986500590873163,mmlu_offline:llama2-7b_chat:moral_scenarios,test,60.999921234790236
33,0.3636363744735718,0.6666666865348816,0.7142857142857143,0.11334189862915964,mmlu_offline:llama2-7b_chat:nutrition,validation,1.8132535759359598
306,0.4084967374801636,0.6797385811805725,0.7256353591160221,0.04069770648588542,mmlu_offline:llama2-7b_chat:nutrition,test,15.552542989142239
34,0.3529411852359772,0.6764705777168274,0.6609848484848485,0.13268813666175394,mmlu_offline:llama2-7b_chat:philosophy,validation,1.5456037293188274
311,0.3376205861568451,0.672025740146637,0.6871937124364308,0.028525340403774514,mmlu_offline:llama2-7b_chat:philosophy,test,11.85444152681157
35,0.2857142984867096,0.8857142925262451,0.916,0.20174196277345927,mmlu_offline:llama2-7b_chat:prehistory,validation,1.7837755740620196
324,0.34259259700775146,0.7067901492118835,0.7133612485725161,0.03273145909662601,mmlu_offline:llama2-7b_chat:prehistory,test,14.380406538024545
31,0.06451612710952759,0.7419354915618896,0.5689655172413792,0.06821765630475937,mmlu_offline:llama2-7b_chat:professional_accounting,validation,2.499610607046634
282,0.1560283750295639,0.7907801270484924,0.6397058823529412,0.09066674218955613,mmlu_offline:llama2-7b_chat:professional_accounting,test,21.43892477406189
170,0.30588236451148987,0.6411764621734619,0.5501140808344198,0.13987407438895283,mmlu_offline:llama2-7b_chat:professional_law,validation,26.501766202040017
1534,0.3200782239437103,0.5945241451263428,0.5717790800077327,0.051028636474808306,mmlu_offline:llama2-7b_chat:professional_law,test,242.68448194023222
31,0.25806450843811035,0.6774193644523621,0.6576086956521738,0.068623504331035,mmlu_offline:llama2-7b_chat:professional_medicine,validation,4.0325632761232555
272,0.23529411852359772,0.6948529481887817,0.6212064302884615,0.0228538855033762,mmlu_offline:llama2-7b_chat:professional_medicine,test,34.455242770724
69,0.37681159377098083,0.6666666865348816,0.7021466905187835,0.07210363512453824,mmlu_offline:llama2-7b_chat:professional_psychology,validation,3.736570240929723
612,0.32189542055130005,0.6944444179534912,0.6968258822090393,0.0345647825914271,mmlu_offline:llama2-7b_chat:professional_psychology,test,30.491121559869498
12,0.3333333432674408,0.75,0.75,0.12797982494036358,mmlu_offline:llama2-7b_chat:public_relations,validation,0.7297084899619222
110,0.3272727131843567,0.7272727489471436,0.7481231231231232,0.057708011973987915,mmlu_offline:llama2-7b_chat:public_relations,test,4.799122349359095
27,0.7407407164573669,0.7037037014961243,0.8535714285714285,0.13132722951747758,mmlu_offline:llama2-7b_chat:security_studies,validation,1.8038622858002782
245,0.6816326379776001,0.6285714507102966,0.7896514662981728,0.05125561368708707,mmlu_offline:llama2-7b_chat:security_studies,test,14.93233442492783
22,0.5,0.6363636255264282,0.7148760330578512,0.06043695319782604,mmlu_offline:llama2-7b_chat:sociology,validation,1.036605720873922
201,0.45771142840385437,0.7263681888580322,0.7822098125249302,0.06142986621429669,mmlu_offline:llama2-7b_chat:sociology,test,8.188164099119604
11,0.5454545617103577,0.8181818127632141,1.0,0.16542078148234976,mmlu_offline:llama2-7b_chat:us_foreign_policy,validation,0.638014008756727
100,0.550000011920929,0.6299999952316284,0.7080808080808081,0.0784741908311844,mmlu_offline:llama2-7b_chat:us_foreign_policy,test,4.43045608419925
18,0.2777777910232544,0.6111111044883728,0.4153846153846154,0.17111075255605912,mmlu_offline:llama2-7b_chat:virology,validation,1.0439345883205533
166,0.34337350726127625,0.6927710771560669,0.6979719942056977,0.053338875612580626,mmlu_offline:llama2-7b_chat:virology,test,6.785518561955541
19,0.6315789222717285,0.6842105388641357,0.8809523809523809,0.24852113661013153,mmlu_offline:llama2-7b_chat:world_religions,validation,0.8328022807836533
171,0.5380116701126099,0.6666666865348816,0.7854980737479361,0.08007089889537525,mmlu_offline:llama2-7b_chat:world_religions,test,6.057675004005432
