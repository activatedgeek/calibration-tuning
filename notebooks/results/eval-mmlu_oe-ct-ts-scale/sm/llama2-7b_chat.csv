N,acc,unc_acc,unc_auroc,unc_ece,dataset,split,ts
11,0.09090909361839294,0.9090909361839294,0.4,0.23683922941034488,mmlu_offline:llama2-7b_chat:abstract_algebra,validation,4.444118919083849
100,0.20999999344348907,0.7900000214576721,0.5729355033152501,0.13233481287956236,mmlu_offline:llama2-7b_chat:abstract_algebra,test,5.757599750999361
14,0.2142857164144516,0.8571428656578064,0.9393939393939393,0.24074877159936087,mmlu_offline:llama2-7b_chat:anatomy,validation,1.004183033015579
135,0.385185182094574,0.6962962746620178,0.7433966635773865,0.09917184114456175,mmlu_offline:llama2-7b_chat:anatomy,test,6.41260128095746
16,0.5625,0.6875,0.753968253968254,0.0996631383895874,mmlu_offline:llama2-7b_chat:astronomy,validation,1.1938501389231533
152,0.42763158679008484,0.6381579041481018,0.708576480990274,0.041164208399622024,mmlu_offline:llama2-7b_chat:astronomy,test,8.791878658812493
11,0.4545454680919647,0.4545454680919647,0.39999999999999997,0.1591026674617421,mmlu_offline:llama2-7b_chat:business_ethics,validation,0.9876986849121749
100,0.33000001311302185,0.46000000834465027,0.6872455902306649,0.1188978934288025,mmlu_offline:llama2-7b_chat:business_ethics,test,6.975862906081602
29,0.17241379618644714,0.8275862336158752,0.875,0.2402820155538362,mmlu_offline:llama2-7b_chat:clinical_knowledge,validation,1.6543639060109854
265,0.2792452871799469,0.6339622735977173,0.6914178576482242,0.05137105190529012,mmlu_offline:llama2-7b_chat:clinical_knowledge,test,13.849854282103479
16,0.375,0.625,0.55,0.06296909600496292,mmlu_offline:llama2-7b_chat:college_biology,validation,1.1309507589321584
144,0.3402777910232544,0.625,0.7309344790547798,0.023397790888945266,mmlu_offline:llama2-7b_chat:college_biology,test,8.73939863895066
8,0.0,0.875,,0.2498406618833542,mmlu_offline:llama2-7b_chat:college_chemistry,validation,0.8391963350586593
100,0.14000000059604645,0.7699999809265137,0.635797342192691,0.14280756771564487,mmlu_offline:llama2-7b_chat:college_chemistry,test,6.959929706063122
11,0.27272728085517883,0.6363636255264282,0.625,0.1250158927657387,mmlu_offline:llama2-7b_chat:college_computer_science,validation,1.3716602670028806
100,0.17000000178813934,0.8399999737739563,0.7374202693125442,0.23758249282836913,mmlu_offline:llama2-7b_chat:college_computer_science,test,10.373183623887599
11,0.0,1.0,,0.35202566602013324,mmlu_offline:llama2-7b_chat:college_mathematics,validation,1.2626997779589146
100,0.2199999988079071,0.8100000023841858,0.5769230769230769,0.18376005768775935,mmlu_offline:llama2-7b_chat:college_mathematics,test,8.049854208016768
22,0.3181818127632141,0.8181818127632141,0.8428571428571429,0.23778600042516534,mmlu_offline:llama2-7b_chat:college_medicine,validation,1.65097562992014
173,0.2947976887226105,0.7052023410797119,0.7468659594985536,0.11859579720249068,mmlu_offline:llama2-7b_chat:college_medicine,test,14.523506853030995
11,0.09090909361839294,0.9090909361839294,0.9,0.2858344153924422,mmlu_offline:llama2-7b_chat:college_physics,validation,1.0190043330658227
102,0.0882352963089943,0.8529411554336548,0.6129032258064516,0.21210990524759482,mmlu_offline:llama2-7b_chat:college_physics,test,7.044051422039047
11,0.6363636255264282,0.6363636255264282,0.7857142857142857,0.19246386939829044,mmlu_offline:llama2-7b_chat:computer_security,validation,0.9756978200748563
100,0.5,0.5799999833106995,0.6662,0.011381430029869102,mmlu_offline:llama2-7b_chat:computer_security,test,5.485380243044347
26,0.1538461595773697,0.5769230723381042,0.6079545454545454,0.01321773345653829,mmlu_offline:llama2-7b_chat:conceptual_physics,validation,1.5553943978156894
235,0.3787234127521515,0.6297872066497803,0.613783284592889,0.0536430346204879,mmlu_offline:llama2-7b_chat:conceptual_physics,test,11.486964140087366
12,0.3333333432674408,0.5833333134651184,0.703125,0.12403211494286853,mmlu_offline:llama2-7b_chat:econometrics,validation,1.2476363158784807
114,0.17543859779834747,0.6842105388641357,0.6321808510638298,0.10903718178732356,mmlu_offline:llama2-7b_chat:econometrics,test,9.010593954008073
16,0.1875,0.8125,0.3846153846153846,0.20690295845270157,mmlu_offline:llama2-7b_chat:electrical_engineering,validation,1.0653810049407184
145,0.2137930989265442,0.7310344576835632,0.6088002263723825,0.12471778187258492,mmlu_offline:llama2-7b_chat:electrical_engineering,test,8.683956024004146
41,0.3658536672592163,0.6829268336296082,0.7448717948717949,0.0581968993675418,mmlu_offline:llama2-7b_chat:elementary_mathematics,validation,3.113936730893329
378,0.28042328357696533,0.7354497313499451,0.5923973362930078,0.11765176057815549,mmlu_offline:llama2-7b_chat:elementary_mathematics,test,24.01015259581618
14,0.4285714328289032,0.5,0.6979166666666666,0.15219499383653912,mmlu_offline:llama2-7b_chat:formal_logic,validation,1.2132910271175206
126,0.2539682686328888,0.7698412537574768,0.7571476063829788,0.21543654988682462,mmlu_offline:llama2-7b_chat:formal_logic,test,8.778545469976962
10,0.20000000298023224,0.6000000238418579,0.6875,0.09282232522964477,mmlu_offline:llama2-7b_chat:global_facts,validation,1.003152335062623
100,0.07999999821186066,0.47999998927116394,0.5115489130434783,0.0819213342666626,mmlu_offline:llama2-7b_chat:global_facts,test,5.967384496005252
32,0.3125,0.5625,0.6068181818181818,0.06209932267665866,mmlu_offline:llama2-7b_chat:high_school_biology,validation,2.117236761841923
310,0.3838709592819214,0.6225806474685669,0.7040784900347573,0.03796773956667992,mmlu_offline:llama2-7b_chat:high_school_biology,test,18.555467505939305
22,0.1818181872367859,0.8181818127632141,0.5277777777777779,0.20603677359494296,mmlu_offline:llama2-7b_chat:high_school_chemistry,validation,1.7369820110034198
203,0.1428571492433548,0.8177340030670166,0.7489100277447484,0.20707193002324975,mmlu_offline:llama2-7b_chat:high_school_chemistry,test,13.351005747914314
9,0.4444444477558136,0.4444444477558136,0.55,0.1383783221244812,mmlu_offline:llama2-7b_chat:high_school_computer_science,validation,1.1388517159502953
100,0.4099999964237213,0.6000000238418579,0.6279454319966928,0.0242465084791183,mmlu_offline:llama2-7b_chat:high_school_computer_science,test,9.449823987903073
18,0.8333333134651184,0.8888888955116272,0.8,0.3211499684386784,mmlu_offline:llama2-7b_chat:high_school_european_history,validation,6.1235432049725205
165,0.7090908885002136,0.7090908885002136,0.7396723646723647,0.06543348052284932,mmlu_offline:llama2-7b_chat:high_school_european_history,test,55.230313459876925
22,0.4545454680919647,0.7272727489471436,0.7041666666666667,0.13286100734363904,mmlu_offline:llama2-7b_chat:high_school_geography,validation,1.378935992019251
198,0.3737373650074005,0.7222222089767456,0.7575196163905842,0.1402844822768009,mmlu_offline:llama2-7b_chat:high_school_geography,test,9.88359959400259
21,0.523809552192688,0.6190476417541504,0.6318181818181818,0.04889315082913356,mmlu_offline:llama2-7b_chat:high_school_government_and_politics,validation,1.373638967052102
193,0.5233160853385925,0.6994818449020386,0.7548966853207061,0.1135982441778628,mmlu_offline:llama2-7b_chat:high_school_government_and_politics,test,10.568723671138287
43,0.39534884691238403,0.6279069781303406,0.5542986425339367,0.0839599689771963,mmlu_offline:llama2-7b_chat:high_school_macroeconomics,validation,2.3966087931767106
390,0.2897436022758484,0.7333333492279053,0.7359189802242739,0.13478497786399646,mmlu_offline:llama2-7b_chat:high_school_macroeconomics,test,19.945837809005752
29,0.0,1.0,,0.3256783896479113,mmlu_offline:llama2-7b_chat:high_school_mathematics,validation,2.233258781954646
270,0.0555555559694767,0.9333333373069763,0.6056209150326797,0.2862970155698282,mmlu_offline:llama2-7b_chat:high_school_mathematics,test,18.92597323190421
26,0.26923078298568726,0.7307692170143127,0.8007518796992481,0.1459624950702374,mmlu_offline:llama2-7b_chat:high_school_microeconomics,validation,1.609970964025706
238,0.3403361439704895,0.6218487620353699,0.6433514193599119,0.044890804451052896,mmlu_offline:llama2-7b_chat:high_school_microeconomics,test,12.04847164498642
17,0.0,0.5882353186607361,,0.08242340999491077,mmlu_offline:llama2-7b_chat:high_school_physics,validation,1.5463627770077437
151,0.18543046712875366,0.7218543291091919,0.766260162601626,0.15428735207248206,mmlu_offline:llama2-7b_chat:high_school_physics,test,10.532851460855454
60,0.5666666626930237,0.6666666865348816,0.7584841628959276,0.0760094861189524,mmlu_offline:llama2-7b_chat:high_school_psychology,validation,4.0154171460308135
545,0.5009174346923828,0.594495415687561,0.7039969834087482,0.04246727884362597,mmlu_offline:llama2-7b_chat:high_school_psychology,test,33.661626391811296
23,0.17391304671764374,0.695652186870575,0.730263157894737,0.11540678013925963,mmlu_offline:llama2-7b_chat:high_school_statistics,validation,2.1888035798911005
216,0.19907407462596893,0.6342592835426331,0.6563382175023525,0.07137850561627637,mmlu_offline:llama2-7b_chat:high_school_statistics,test,18.28157533914782
22,0.6363636255264282,0.6818181872367859,0.8392857142857143,0.16443667357618158,mmlu_offline:llama2-7b_chat:high_school_us_history,validation,5.82743576192297
204,0.5784313678741455,0.6421568393707275,0.7778379976350019,0.06757886298731264,mmlu_offline:llama2-7b_chat:high_school_us_history,test,53.164046393940225
26,0.7307692170143127,0.7307692170143127,0.6954887218045114,0.05660922710712139,mmlu_offline:llama2-7b_chat:high_school_world_history,validation,5.495435492135584
237,0.552742600440979,0.5822784900665283,0.639889096932162,0.10075091209089707,mmlu_offline:llama2-7b_chat:high_school_world_history,test,43.75451955711469
23,0.30434781312942505,0.5652173757553101,0.5401785714285714,0.031185766924982497,mmlu_offline:llama2-7b_chat:human_aging,validation,1.3254436729475856
223,0.33183857798576355,0.6681614518165588,0.7594322510429893,0.08867517955634624,mmlu_offline:llama2-7b_chat:human_aging,test,10.848246907815337
12,0.25,0.8333333134651184,0.8148148148148148,0.2393260399500529,mmlu_offline:llama2-7b_chat:human_sexuality,validation,0.9725114898756146
131,0.4198473393917084,0.6641221642494202,0.6916267942583731,0.07639361246851568,mmlu_offline:llama2-7b_chat:human_sexuality,test,6.840118450112641
13,0.4615384638309479,0.5384615659713745,0.6904761904761905,0.11453146659410915,mmlu_offline:llama2-7b_chat:international_law,validation,1.0617619389668107
121,0.6115702390670776,0.64462810754776,0.6663312248418631,0.013354279285620082,mmlu_offline:llama2-7b_chat:international_law,test,7.5953115408774465
11,0.5454545617103577,0.7272727489471436,0.6666666666666666,0.16272264719009394,mmlu_offline:llama2-7b_chat:jurisprudence,validation,0.921179847093299
108,0.5185185074806213,0.6111111044883728,0.6710164835164836,0.044218221748316765,mmlu_offline:llama2-7b_chat:jurisprudence,test,5.810853865928948
18,0.4444444477558136,0.5,0.6062500000000001,0.06134512689378526,mmlu_offline:llama2-7b_chat:logical_fallacies,validation,1.3690814790315926
163,0.42944785952568054,0.6687116622924805,0.7519201228878649,0.10103396440576191,mmlu_offline:llama2-7b_chat:logical_fallacies,test,9.553213212871924
11,0.3636363744735718,0.6363636255264282,0.6428571428571429,0.16551517898386175,mmlu_offline:llama2-7b_chat:machine_learning,validation,1.1473830889444798
112,0.1964285671710968,0.8125,0.7626262626262627,0.2196922201131071,mmlu_offline:llama2-7b_chat:machine_learning,test,8.541084400145337
11,0.27272728085517883,0.7272727489471436,0.7708333333333333,0.1642359332604842,mmlu_offline:llama2-7b_chat:management,validation,0.9485913319513202
103,0.3689320385456085,0.6893203854560852,0.7791497975708501,0.10949346336346229,mmlu_offline:llama2-7b_chat:management,test,4.911366991931573
25,0.20000000298023224,0.4000000059604645,0.7050000000000001,0.22253434419631957,mmlu_offline:llama2-7b_chat:marketing,validation,1.8295740620233119
234,0.39743590354919434,0.5341880321502686,0.7136810798444292,0.06713361031988749,mmlu_offline:llama2-7b_chat:marketing,test,12.505548033863306
11,0.7272727489471436,0.7272727489471436,0.75,0.12224318222566084,mmlu_offline:llama2-7b_chat:medical_genetics,validation,0.9296638430096209
100,0.4099999964237213,0.699999988079071,0.7215791649441919,0.11977868318557736,mmlu_offline:llama2-7b_chat:medical_genetics,test,4.884326314087957
86,0.41860464215278625,0.6511628031730652,0.6411111111111111,0.07660681286523506,mmlu_offline:llama2-7b_chat:miscellaneous,validation,4.405845843954012
783,0.4533844292163849,0.7164750695228577,0.7290641042516782,0.12572299147626603,mmlu_offline:llama2-7b_chat:miscellaneous,test,37.8878392141778
38,0.4736842215061188,0.7105262875556946,0.6430555555555556,0.14895813402376673,mmlu_offline:llama2-7b_chat:moral_disputes,validation,2.482133904006332
346,0.41040462255477905,0.5520231127738953,0.5757905274785972,0.012355523819179159,mmlu_offline:llama2-7b_chat:moral_disputes,test,19.89375289203599
100,0.5099999904632568,0.49000000953674316,0.5914365746298519,0.07566665530204775,mmlu_offline:llama2-7b_chat:moral_scenarios,validation,8.488772810902447
895,0.46145251393318176,0.54078209400177,0.5326776044126069,0.02190411430497408,mmlu_offline:llama2-7b_chat:moral_scenarios,test,74.10841755499132
33,0.3636363744735718,0.6666666865348816,0.7400793650793651,0.14192649270548965,mmlu_offline:llama2-7b_chat:nutrition,validation,2.3596313649322838
306,0.4084967374801636,0.6601307392120361,0.7022762430939226,0.0675213223968456,mmlu_offline:llama2-7b_chat:nutrition,test,19.4092718930915
34,0.3529411852359772,0.529411792755127,0.615530303030303,0.22537903224720676,mmlu_offline:llama2-7b_chat:philosophy,validation,2.0010282939765602
311,0.3376205861568451,0.4437299072742462,0.6300739713361072,0.16304359558694229,mmlu_offline:llama2-7b_chat:philosophy,test,15.61618370981887
35,0.2857142984867096,0.6285714507102966,0.8580000000000001,0.05279460123607095,mmlu_offline:llama2-7b_chat:prehistory,validation,2.4059376060031354
324,0.34259259700775146,0.6172839403152466,0.7240832381677452,0.04185880113531046,mmlu_offline:llama2-7b_chat:prehistory,test,18.745230132015422
31,0.06451612710952759,0.6774193644523621,0.4827586206896552,0.0909325268960768,mmlu_offline:llama2-7b_chat:professional_accounting,validation,3.11293356702663
282,0.1560283750295639,0.7482269406318665,0.6423796791443851,0.1682072142337231,mmlu_offline:llama2-7b_chat:professional_accounting,test,25.574304691981524
170,0.30588236451148987,0.4000000059604645,0.49502933507170793,0.16831627768628737,mmlu_offline:llama2-7b_chat:professional_law,validation,29.576316796941683
1534,0.3200782239437103,0.43741852045059204,0.5392511027839558,0.13341252993418312,mmlu_offline:llama2-7b_chat:professional_law,test,270.9226191760972
31,0.25806450843811035,0.6129032373428345,0.6059782608695652,0.055608860908016054,mmlu_offline:llama2-7b_chat:professional_medicine,validation,4.53114513377659
272,0.23529411852359772,0.6470588445663452,0.6053185096153846,0.07603474224314967,mmlu_offline:llama2-7b_chat:professional_medicine,test,40.371082935947925
69,0.37681159377098083,0.6231883764266968,0.6144901610017889,0.04948498546213345,mmlu_offline:llama2-7b_chat:professional_psychology,validation,4.790821539005265
612,0.32189542055130005,0.6094771027565002,0.6236254663323344,0.03826316226931181,mmlu_offline:llama2-7b_chat:professional_psychology,test,38.08500906196423
12,0.3333333432674408,0.5,0.5,0.07179909944534302,mmlu_offline:llama2-7b_chat:public_relations,validation,0.9892050111666322
110,0.3272727131843567,0.5545454621315002,0.6786786786786786,0.03677329475229437,mmlu_offline:llama2-7b_chat:public_relations,test,6.130395257147029
27,0.7407407164573669,0.7777777910232544,0.8464285714285714,0.16374773449367946,mmlu_offline:llama2-7b_chat:security_studies,validation,2.214759430848062
245,0.6816326379776001,0.7346938848495483,0.7688085367726086,0.13524568543142204,mmlu_offline:llama2-7b_chat:security_studies,test,18.315496182069182
22,0.5,0.6363636255264282,0.6446280991735538,0.10788240757855502,mmlu_offline:llama2-7b_chat:sociology,validation,1.3932001600041986
201,0.45771142840385437,0.711442768573761,0.7687475069804546,0.12097956825844686,mmlu_offline:llama2-7b_chat:sociology,test,10.690203591948375
11,0.5454545617103577,1.0,1.0,0.3794247724793174,mmlu_offline:llama2-7b_chat:us_foreign_policy,validation,1.0344448778778315
100,0.550000011920929,0.7200000286102295,0.7309090909090908,0.11824456810951232,mmlu_offline:llama2-7b_chat:us_foreign_policy,test,5.355404522037134
18,0.2777777910232544,0.3333333432674408,0.423076923076923,0.23806201418240863,mmlu_offline:llama2-7b_chat:virology,validation,1.4865252419840544
166,0.34337350726127625,0.5240963697433472,0.6461451794624175,0.06012686045773057,mmlu_offline:llama2-7b_chat:virology,test,8.606496328953654
19,0.6315789222717285,0.6315789222717285,0.7261904761904762,0.05692229145451595,mmlu_offline:llama2-7b_chat:world_religions,validation,1.2786431680433452
171,0.5380116701126099,0.7426900863647461,0.7716703357182169,0.15505949725881654,mmlu_offline:llama2-7b_chat:world_religions,test,8.163606806891039
