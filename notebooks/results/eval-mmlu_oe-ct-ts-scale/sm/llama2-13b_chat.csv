N,acc,unc_acc,unc_auroc,unc_ece,dataset,split,ts
11,0.09090909361839294,0.9090909361839294,0.0,0.15500686385414816,mmlu_offline:llama2-13b_chat:abstract_algebra,validation,4.830499371979386
100,0.3499999940395355,0.6499999761581421,0.6463736263736264,0.17360236942768095,mmlu_offline:llama2-13b_chat:abstract_algebra,test,6.567483402322978
14,0.2857142984867096,0.7857142686843872,0.65,0.09423141820090154,mmlu_offline:llama2-13b_chat:anatomy,validation,0.9292809651233256
135,0.42222222685813904,0.6740740537643433,0.7623706702654071,0.09229193307735303,mmlu_offline:llama2-13b_chat:anatomy,test,7.201424413826317
16,0.5,0.625,0.703125,0.12468768283724788,mmlu_offline:llama2-13b_chat:astronomy,validation,1.2941798390820622
152,0.5263158082962036,0.6907894611358643,0.7679687500000001,0.02726905753738002,mmlu_offline:llama2-13b_chat:astronomy,test,10.535175336990505
11,0.5454545617103577,0.6363636255264282,0.6499999999999999,0.13544260371815076,mmlu_offline:llama2-13b_chat:business_ethics,validation,1.2003828422166407
100,0.3199999928474426,0.7599999904632568,0.6767003676470589,0.11661527276039121,mmlu_offline:llama2-13b_chat:business_ethics,test,8.707229661289603
29,0.24137930572032928,0.7586206793785095,0.7305194805194806,0.09837460723416558,mmlu_offline:llama2-13b_chat:clinical_knowledge,validation,1.9660896179266274
265,0.2981131970882416,0.7358490824699402,0.694569211923234,0.015956001236753635,mmlu_offline:llama2-13b_chat:clinical_knowledge,test,16.037139230873436
16,0.25,0.8125,0.5625,0.22603594884276387,mmlu_offline:llama2-13b_chat:college_biology,validation,1.318312597926706
144,0.3958333432674408,0.7152777910232544,0.7319015930631175,0.035901990201738156,mmlu_offline:llama2-13b_chat:college_biology,test,10.67983777122572
8,0.0,1.0,,0.24403411149978638,mmlu_offline:llama2-13b_chat:college_chemistry,validation,0.9768373211845756
100,0.17000000178813934,0.8399999737739563,0.8532955350815025,0.1006294536590576,mmlu_offline:llama2-13b_chat:college_chemistry,test,8.394679611548781
11,0.27272728085517883,0.9090909361839294,0.9166666666666666,0.1868425932797519,mmlu_offline:llama2-13b_chat:college_computer_science,validation,1.7374499388970435
100,0.23999999463558197,0.8100000023841858,0.7025767543859649,0.07067563951015472,mmlu_offline:llama2-13b_chat:college_computer_science,test,14.616515459027141
11,0.0,1.0,,0.19035522504286334,mmlu_offline:llama2-13b_chat:college_mathematics,validation,1.4172151070088148
100,0.09000000357627869,0.9100000262260437,0.7503052503052503,0.11070992708206173,mmlu_offline:llama2-13b_chat:college_mathematics,test,10.316115169785917
22,0.5,0.4545454680919647,0.5909090909090909,0.2631122930483385,mmlu_offline:llama2-13b_chat:college_medicine,validation,1.9171392638236284
173,0.3988439440727234,0.7109826803207397,0.796892419175028,0.04278607588971972,mmlu_offline:llama2-13b_chat:college_medicine,test,19.112809914164245
11,0.27272728085517883,0.8181818127632141,0.9583333333333334,0.2199835181236267,mmlu_offline:llama2-13b_chat:college_physics,validation,1.2171784210950136
102,0.1568627506494522,0.8235294222831726,0.6101017441860466,0.06643265836379104,mmlu_offline:llama2-13b_chat:college_physics,test,8.95744105335325
11,0.5454545617103577,0.5454545617103577,0.5,0.18593431060964413,mmlu_offline:llama2-13b_chat:computer_security,validation,1.0945094698108733
100,0.550000011920929,0.5400000214576721,0.6696969696969697,0.14977230608463285,mmlu_offline:llama2-13b_chat:computer_security,test,6.527274155989289
26,0.3461538553237915,0.6153846383094788,0.5588235294117647,0.1434935606442965,mmlu_offline:llama2-13b_chat:conceptual_physics,validation,1.6621076660230756
235,0.4553191363811493,0.6170212626457214,0.6794319509345794,0.15927920113218597,mmlu_offline:llama2-13b_chat:conceptual_physics,test,12.514663574751467
12,0.25,0.6666666865348816,0.8148148148148148,0.07056890428066254,mmlu_offline:llama2-13b_chat:econometrics,validation,1.5008227908983827
114,0.1315789520740509,0.780701756477356,0.6710437710437711,0.125405140090407,mmlu_offline:llama2-13b_chat:econometrics,test,11.983557909727097
16,0.25,0.8125,0.6458333333333334,0.07918662577867505,mmlu_offline:llama2-13b_chat:electrical_engineering,validation,1.3312724181450903
145,0.24827586114406586,0.7862069010734558,0.7580275229357798,0.09576711408023178,mmlu_offline:llama2-13b_chat:electrical_engineering,test,10.711691142059863
41,0.26829269528388977,0.7560975551605225,0.40909090909090906,0.08244041698734939,mmlu_offline:llama2-13b_chat:elementary_mathematics,validation,3.548915992025286
378,0.3174603283405304,0.7089946866035461,0.6137758397932817,0.030870477989237116,mmlu_offline:llama2-13b_chat:elementary_mathematics,test,29.591222572140396
14,0.2142857164144516,0.7857142686843872,0.7272727272727273,0.05815187096595767,mmlu_offline:llama2-13b_chat:formal_logic,validation,1.4150148523040116
126,0.261904776096344,0.7539682388305664,0.5454545454545454,0.0397472736381349,mmlu_offline:llama2-13b_chat:formal_logic,test,11.128140602260828
10,0.20000000298023224,0.800000011920929,0.3125,0.1535033822059631,mmlu_offline:llama2-13b_chat:global_facts,validation,0.9539672737009823
100,0.12999999523162842,0.8500000238418579,0.5809018567639258,0.08782466471195222,mmlu_offline:llama2-13b_chat:global_facts,test,7.021690906025469
32,0.3125,0.75,0.7431818181818182,0.08878559060394764,mmlu_offline:llama2-13b_chat:high_school_biology,validation,2.5034297821111977
310,0.42258065938949585,0.699999988079071,0.7394131945925201,0.03236565013085642,mmlu_offline:llama2-13b_chat:high_school_biology,test,23.058592602144927
22,0.1818181872367859,0.8181818127632141,0.826388888888889,0.17778732559897686,mmlu_offline:llama2-13b_chat:high_school_chemistry,validation,2.073788265697658
203,0.1822660118341446,0.8177340030670166,0.7335558450016282,0.11233234963393562,mmlu_offline:llama2-13b_chat:high_school_chemistry,test,16.797579657752067
9,0.6666666865348816,0.6666666865348816,0.6666666666666666,0.2700065506829157,mmlu_offline:llama2-13b_chat:high_school_computer_science,validation,1.3724649888463318
100,0.4300000071525574,0.6499999761581421,0.6966544267645859,0.10284228563308714,mmlu_offline:llama2-13b_chat:high_school_computer_science,test,12.860906694084406
18,0.7777777910232544,0.7777777910232544,0.8571428571428571,0.13532736897468567,mmlu_offline:llama2-13b_chat:high_school_european_history,validation,9.511289591901004
165,0.7575757503509521,0.7939394116401672,0.7916,0.0724323251030662,mmlu_offline:llama2-13b_chat:high_school_european_history,test,86.47748094331473
22,0.5,0.6818181872367859,0.8016528925619835,0.0862494246526198,mmlu_offline:llama2-13b_chat:high_school_geography,validation,1.4823045772500336
198,0.39393940567970276,0.7020202279090881,0.7589743589743589,0.027186538534935058,mmlu_offline:llama2-13b_chat:high_school_geography,test,11.588667945005
21,0.523809552192688,0.761904776096344,0.8954545454545455,0.1396880206607637,mmlu_offline:llama2-13b_chat:high_school_government_and_politics,validation,1.581783875823021
193,0.6321243643760681,0.6580311059951782,0.7604479335026554,0.07008564626614663,mmlu_offline:llama2-13b_chat:high_school_government_and_politics,test,12.311458576004952
43,0.4883720874786377,0.604651153087616,0.7922077922077922,0.14665689717891606,mmlu_offline:llama2-13b_chat:high_school_macroeconomics,validation,2.7523071272298694
390,0.3692307770252228,0.6974359154701233,0.6931741192411922,0.06342306947096799,mmlu_offline:llama2-13b_chat:high_school_macroeconomics,test,22.85925190616399
29,0.06896551698446274,0.931034505367279,0.6111111111111112,0.14238039583995427,mmlu_offline:llama2-13b_chat:high_school_mathematics,validation,2.847252229694277
270,0.10000000149011612,0.8999999761581421,0.5743789056546258,0.11854314208030703,mmlu_offline:llama2-13b_chat:high_school_mathematics,test,24.701866421848536
26,0.3076923191547394,0.7692307829856873,0.7881944444444444,0.1512415867585402,mmlu_offline:llama2-13b_chat:high_school_microeconomics,validation,1.660475555807352
238,0.36554622650146484,0.7352941036224365,0.7893735251579509,0.05192435689333105,mmlu_offline:llama2-13b_chat:high_school_microeconomics,test,14.027082916814834
17,0.29411765933036804,0.7058823704719543,0.6583333333333333,0.058003313401166164,mmlu_offline:llama2-13b_chat:high_school_physics,validation,1.7582571282982826
151,0.17218543589115143,0.8211920261383057,0.6230769230769231,0.06736985579231716,mmlu_offline:llama2-13b_chat:high_school_physics,test,13.186295374762267
60,0.6000000238418579,0.7333333492279053,0.8026620370370371,0.08926260769367216,mmlu_offline:llama2-13b_chat:high_school_psychology,validation,4.66437624162063
545,0.5064220428466797,0.6862385272979736,0.743521361995582,0.018726904457862233,mmlu_offline:llama2-13b_chat:high_school_psychology,test,41.286079288925976
23,0.17391304671764374,0.695652186870575,0.48026315789473684,0.08969961560290794,mmlu_offline:llama2-13b_chat:high_school_statistics,validation,2.7421551099978387
216,0.25925925374031067,0.7407407164573669,0.7111607142857143,0.039370229674710176,mmlu_offline:llama2-13b_chat:high_school_statistics,test,24.70579960383475
22,0.7727272510528564,0.7272727489471436,0.5294117647058824,0.013840117237784657,mmlu_offline:llama2-13b_chat:high_school_us_history,validation,9.060271035414189
204,0.6764705777168274,0.7401960492134094,0.7947408871321915,0.04781404049957501,mmlu_offline:llama2-13b_chat:high_school_us_history,test,83.04590014088899
26,0.6538461446762085,0.5769230723381042,0.5196078431372548,0.17503532538047206,mmlu_offline:llama2-13b_chat:high_school_world_history,validation,7.827343515120447
237,0.6033755540847778,0.700421929359436,0.7336333878887069,0.08423555224253655,mmlu_offline:llama2-13b_chat:high_school_world_history,test,64.75645357882604
23,0.3913043439388275,0.6521739363670349,0.8134920634920635,0.21228365794472076,mmlu_offline:llama2-13b_chat:human_aging,validation,1.4575546560809016
223,0.3901345431804657,0.6367713212966919,0.7109110885733604,0.09515861438528839,mmlu_offline:llama2-13b_chat:human_aging,test,12.391005374956876
12,0.3333333432674408,0.6666666865348816,0.59375,0.08680459856987001,mmlu_offline:llama2-13b_chat:human_sexuality,validation,0.9291805420070887
131,0.5114504098892212,0.5267175436019897,0.6082089552238806,0.20550947016432086,mmlu_offline:llama2-13b_chat:human_sexuality,test,8.062908601015806
13,0.6153846383094788,0.38461539149284363,0.525,0.25920069217681885,mmlu_offline:llama2-13b_chat:international_law,validation,1.1915967860259116
121,0.6280992031097412,0.6198347210884094,0.639766081871345,0.06439252431727639,mmlu_offline:llama2-13b_chat:international_law,test,9.156553364824504
11,0.1818181872367859,0.9090909361839294,0.9444444444444444,0.1745366345752369,mmlu_offline:llama2-13b_chat:jurisprudence,validation,0.9425618769600987
108,0.3611111044883728,0.6666666865348816,0.68561872909699,0.06800992069420987,mmlu_offline:llama2-13b_chat:jurisprudence,test,7.045488346833736
18,0.6666666865348816,0.7222222089767456,0.7638888888888888,0.15034406714969212,mmlu_offline:llama2-13b_chat:logical_fallacies,validation,1.4057703209109604
163,0.48466256260871887,0.7668711543083191,0.8457655213984329,0.06471911712658182,mmlu_offline:llama2-13b_chat:logical_fallacies,test,11.4982999288477
11,0.27272728085517883,0.6363636255264282,0.7916666666666666,0.18410816517743195,mmlu_offline:llama2-13b_chat:machine_learning,validation,1.2855106028728187
112,0.2946428656578064,0.7589285969734192,0.7253548139624089,0.06282051120485577,mmlu_offline:llama2-13b_chat:machine_learning,test,10.823405113071203
11,0.6363636255264282,0.5454545617103577,0.5357142857142857,0.27235896478999744,mmlu_offline:llama2-13b_chat:management,validation,0.7872059429064393
103,0.42718446254730225,0.6796116232872009,0.8147149460708784,0.11577695955350559,mmlu_offline:llama2-13b_chat:management,test,5.262951107695699
25,0.23999999463558197,0.6399999856948853,0.6228070175438597,0.06047012329101563,mmlu_offline:llama2-13b_chat:marketing,validation,1.8882110300473869
234,0.44871795177459717,0.6623931527137756,0.7638242894056848,0.08201179876286759,mmlu_offline:llama2-13b_chat:marketing,test,14.654416639823467
11,0.9090909361839294,0.7272727489471436,1.0,0.2596355351534757,mmlu_offline:llama2-13b_chat:medical_genetics,validation,0.8529087789356709
100,0.44999998807907104,0.6800000071525574,0.7222222222222223,0.0631060940027237,mmlu_offline:llama2-13b_chat:medical_genetics,test,5.40954059176147
86,0.5581395626068115,0.6511628031730652,0.7645285087719299,0.07439809691074284,mmlu_offline:llama2-13b_chat:miscellaneous,validation,4.6636116290465
783,0.618135392665863,0.7075351476669312,0.7819695127007379,0.014133223248015282,mmlu_offline:llama2-13b_chat:miscellaneous,test,42.36428389605135
38,0.42105263471603394,0.5526315569877625,0.6789772727272727,0.17256335992562144,mmlu_offline:llama2-13b_chat:moral_disputes,validation,2.7980644232593477
346,0.43063583970069885,0.6184971332550049,0.6608183149933565,0.0799741429745117,mmlu_offline:llama2-13b_chat:moral_disputes,test,23.86748193530366
100,0.4300000071525574,0.5799999833106995,0.6552427580579356,0.16458038270473477,mmlu_offline:llama2-13b_chat:moral_scenarios,validation,11.175100249703974
895,0.38100558519363403,0.6201117038726807,0.6055744942142985,0.12209455926991039,mmlu_offline:llama2-13b_chat:moral_scenarios,test,97.34817187394947
33,0.3333333432674408,0.7272727489471436,0.706611570247934,0.06777719295386114,mmlu_offline:llama2-13b_chat:nutrition,validation,2.772688503842801
306,0.4542483687400818,0.7058823704719543,0.7473829319777711,0.03470388795036116,mmlu_offline:llama2-13b_chat:nutrition,test,24.467591034714133
34,0.3235294222831726,0.7352941036224365,0.7134387351778657,0.09507949562633743,mmlu_offline:llama2-13b_chat:philosophy,validation,2.2786037460900843
311,0.3633440434932709,0.7041800618171692,0.6578841512469831,0.04589517783505358,mmlu_offline:llama2-13b_chat:philosophy,test,18.307048903778195
35,0.37142857909202576,0.6857143044471741,0.5699300699300699,0.0529435702732631,mmlu_offline:llama2-13b_chat:prehistory,validation,2.6129786199890077
324,0.4413580298423767,0.6512345671653748,0.7094424912104471,0.05950967636373307,mmlu_offline:llama2-13b_chat:prehistory,test,21.314179928041995
31,0.16129031777381897,0.8387096524238586,0.6461538461538462,0.1175964686178392,mmlu_offline:llama2-13b_chat:professional_accounting,validation,3.977146611083299
282,0.1879432648420334,0.8120567202568054,0.6703468731976602,0.08632003922834462,mmlu_offline:llama2-13b_chat:professional_accounting,test,34.40919439308345
170,0.38235294818878174,0.6117647290229797,0.5666666666666667,0.10608683614169848,mmlu_offline:llama2-13b_chat:professional_law,validation,44.21911425469443
1534,0.34810951352119446,0.6408083438873291,0.5969382022471909,0.05352343006892495,mmlu_offline:llama2-13b_chat:professional_law,test,405.0259372550063
31,0.35483869910240173,0.6451612710952759,0.7181818181818181,0.02091793860158614,mmlu_offline:llama2-13b_chat:professional_medicine,validation,6.514341983012855
272,0.2904411852359772,0.658088207244873,0.658326228110448,0.01583354350398571,mmlu_offline:llama2-13b_chat:professional_medicine,test,57.5278051760979
69,0.37681159377098083,0.695652186870575,0.7415026833631484,0.05763988304829252,mmlu_offline:llama2-13b_chat:professional_psychology,validation,5.802007421851158
612,0.3464052379131317,0.6715686321258545,0.6862971698113208,0.016998073341799723,mmlu_offline:llama2-13b_chat:professional_psychology,test,47.65983122866601
12,0.1666666716337204,0.75,0.85,0.27602291107177734,mmlu_offline:llama2-13b_chat:public_relations,validation,1.0515835429541767
110,0.30909091234207153,0.7454545497894287,0.801470588235294,0.08080522526394238,mmlu_offline:llama2-13b_chat:public_relations,test,7.220362071879208
27,0.5555555820465088,0.7777777910232544,0.9,0.15849496258629694,mmlu_offline:llama2-13b_chat:security_studies,validation,2.6978833889588714
245,0.6612244844436646,0.6040816307067871,0.749553770638108,0.03895136434204726,mmlu_offline:llama2-13b_chat:security_studies,test,22.82013824302703
22,0.5909090638160706,0.5454545617103577,0.7777777777777778,0.16706437956203116,mmlu_offline:llama2-13b_chat:sociology,validation,1.4588479083031416
201,0.447761207818985,0.7263681888580322,0.7742242242242241,0.04184150399260256,mmlu_offline:llama2-13b_chat:sociology,test,12.345456862356514
11,0.7272727489471436,0.6363636255264282,0.7916666666666667,0.1333703778006814,mmlu_offline:llama2-13b_chat:us_foreign_policy,validation,0.9776887279003859
100,0.5899999737739563,0.7200000286102295,0.7961967755270772,0.03445318460464479,mmlu_offline:llama2-13b_chat:us_foreign_policy,test,6.217926238197833
18,0.5555555820465088,0.5555555820465088,0.6875,0.3026904894245996,mmlu_offline:llama2-13b_chat:virology,validation,1.5483048860915005
166,0.34939759969711304,0.6626505851745605,0.6414431673052362,0.06577377506049281,mmlu_offline:llama2-13b_chat:virology,test,10.557348309084773
19,0.6315789222717285,0.7368420958518982,0.7976190476190476,0.1595822760933324,mmlu_offline:llama2-13b_chat:world_religions,validation,1.1906008571386337
171,0.6081871390342712,0.707602322101593,0.7825774971297359,0.050678504489318675,mmlu_offline:llama2-13b_chat:world_religions,test,8.542903103400022
