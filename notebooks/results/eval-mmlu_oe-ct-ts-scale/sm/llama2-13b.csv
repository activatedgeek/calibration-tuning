N,acc,unc_acc,unc_auroc,unc_ece,dataset,split,ts
11,0.09090909361839294,0.9090909361839294,0.9,0.4169794429432262,mmlu_offline:llama2-13b:abstract_algebra,validation,4.992085644975305
100,0.23000000417232513,0.5899999737739563,0.5866741953698474,0.08089471399784089,mmlu_offline:llama2-13b:abstract_algebra,test,6.248980741016567
14,0.2142857164144516,0.7857142686843872,0.9393939393939393,0.12796864765030996,mmlu_offline:llama2-13b:anatomy,validation,0.8579273629002273
135,0.45185184478759766,0.6666666865348816,0.7461231723526806,0.03966865230489663,mmlu_offline:llama2-13b:anatomy,test,6.8992889346554875
16,0.25,0.6875,0.8333333333333334,0.08095385879278186,mmlu_offline:llama2-13b:astronomy,validation,1.2915944051928818
152,0.5131579041481018,0.7171052694320679,0.7931392931392931,0.053716735228111885,mmlu_offline:llama2-13b:astronomy,test,10.354650848079473
11,0.4545454680919647,0.4545454680919647,0.8666666666666667,0.3198344978419217,mmlu_offline:llama2-13b:business_ethics,validation,1.1071710400283337
100,0.3199999928474426,0.4099999964237213,0.6985294117647058,0.3567323917150498,mmlu_offline:llama2-13b:business_ethics,test,8.372117874212563
29,0.24137930572032928,0.6896551847457886,0.672077922077922,0.11826360225677487,mmlu_offline:llama2-13b:clinical_knowledge,validation,1.891061867121607
265,0.3207547068595886,0.7433962225914001,0.7946405228758169,0.0715719722351938,mmlu_offline:llama2-13b:clinical_knowledge,test,15.607282110955566
16,0.25,0.4375,0.5625,0.19197289645671844,mmlu_offline:llama2-13b:college_biology,validation,1.237710818182677
144,0.3958333432674408,0.625,0.7259528130671506,0.07263568250669372,mmlu_offline:llama2-13b:college_biology,test,10.203506249003112
8,0.125,0.875,1.0,0.2521277070045471,mmlu_offline:llama2-13b:college_chemistry,validation,0.7403089180588722
100,0.15000000596046448,0.800000011920929,0.8898039215686274,0.1727573508024216,mmlu_offline:llama2-13b:college_chemistry,test,8.42137860879302
11,0.1818181872367859,0.7272727489471436,0.75,0.2543914697387002,mmlu_offline:llama2-13b:college_computer_science,validation,1.6467666886746883
100,0.2199999988079071,0.7200000286102295,0.7456293706293706,0.12755754828453067,mmlu_offline:llama2-13b:college_computer_science,test,14.146423636935651
11,0.1818181872367859,0.8181818127632141,0.8888888888888888,0.3399355086413297,mmlu_offline:llama2-13b:college_mathematics,validation,1.6255743941292167
100,0.10000000149011612,0.8799999952316284,0.5355555555555556,0.17753516256809232,mmlu_offline:llama2-13b:college_mathematics,test,10.015855254605412
22,0.5,0.6363636255264282,0.6900826446280992,0.08560671860521489,mmlu_offline:llama2-13b:college_medicine,validation,1.8166445209644735
173,0.3583815097808838,0.6820809245109558,0.8036181342632955,0.03855630565929967,mmlu_offline:llama2-13b:college_medicine,test,18.560353328939527
11,0.3636363744735718,0.8181818127632141,0.8928571428571428,0.14747929031198675,mmlu_offline:llama2-13b:college_physics,validation,1.2602391350083053
102,0.21568627655506134,0.5980392098426819,0.6946022727272728,0.13349456120939818,mmlu_offline:llama2-13b:college_physics,test,8.902683394961059
11,0.3636363744735718,0.8181818127632141,0.7499999999999999,0.33210680159655487,mmlu_offline:llama2-13b:computer_security,validation,1.1459543062373996
100,0.46000000834465027,0.6700000166893005,0.7159822866344605,0.08584845364093782,mmlu_offline:llama2-13b:computer_security,test,6.439154084771872
26,0.3461538553237915,0.692307710647583,0.741830065359477,0.04135581621756919,mmlu_offline:llama2-13b:conceptual_physics,validation,1.4901066208258271
235,0.44255319237709045,0.5617021322250366,0.6024295361127423,0.15793506262150214,mmlu_offline:llama2-13b:conceptual_physics,test,11.982995911035687
12,0.0833333358168602,0.5833333134651184,0.9090909090909091,0.06468043724695838,mmlu_offline:llama2-13b:econometrics,validation,1.421019446104765
114,0.19298245012760162,0.7017543911933899,0.6766304347826086,0.13219545284907022,mmlu_offline:llama2-13b:econometrics,test,11.597975235898048
16,0.375,0.625,0.5583333333333333,0.25682469457387924,mmlu_offline:llama2-13b:electrical_engineering,validation,1.2742935786955059
145,0.2344827651977539,0.7103448510169983,0.755431902490726,0.09181920495526545,mmlu_offline:llama2-13b:electrical_engineering,test,10.670513595920056
41,0.2926829159259796,0.707317054271698,0.6048850574712643,0.07475160680166101,mmlu_offline:llama2-13b:elementary_mathematics,validation,3.484950692858547
378,0.4021163880825043,0.6084656119346619,0.6186976013041454,0.043256131114152364,mmlu_offline:llama2-13b:elementary_mathematics,test,28.66467908071354
14,0.3571428656578064,0.6428571343421936,0.6555555555555557,0.06164073944091796,mmlu_offline:llama2-13b:formal_logic,validation,1.3910576929338276
126,0.190476194024086,0.60317462682724,0.639501633986928,0.06727763395460823,mmlu_offline:llama2-13b:formal_logic,test,10.800248627085239
10,0.30000001192092896,0.5,0.4285714285714286,0.17151504158973693,mmlu_offline:llama2-13b:global_facts,validation,0.8480837559327483
100,0.23999999463558197,0.6299999952316284,0.5372807017543859,0.07159052848815922,mmlu_offline:llama2-13b:global_facts,test,6.640159219969064
32,0.34375,0.71875,0.6103896103896104,0.04941849038004875,mmlu_offline:llama2-13b:high_school_biology,validation,2.4085997277870774
310,0.47096773982048035,0.6290322542190552,0.7226862679585699,0.06177393351831744,mmlu_offline:llama2-13b:high_school_biology,test,22.004356087185442
22,0.04545454680919647,1.0,0.9999999999999999,0.3146385171196678,mmlu_offline:llama2-13b:high_school_chemistry,validation,1.964870308060199
203,0.19704432785511017,0.8325123190879822,0.7230828220858896,0.1689766645431519,mmlu_offline:llama2-13b:high_school_chemistry,test,16.625988205894828
9,0.2222222238779068,0.5555555820465088,0.9285714285714286,0.32670380009545225,mmlu_offline:llama2-13b:high_school_computer_science,validation,1.3203726089559495
100,0.41999998688697815,0.6700000166893005,0.7208538587848932,0.04618272006511689,mmlu_offline:llama2-13b:high_school_computer_science,test,12.546778814867139
18,0.7222222089767456,0.7777777910232544,0.8153846153846154,0.1809856858518388,mmlu_offline:llama2-13b:high_school_european_history,validation,9.330122197046876
165,0.6000000238418579,0.6242424249649048,0.7554331190694827,0.10932243809555516,mmlu_offline:llama2-13b:high_school_european_history,test,84.88391278171912
22,0.40909090638160706,0.6818181872367859,0.8376068376068376,0.16661083698272708,mmlu_offline:llama2-13b:high_school_geography,validation,1.3910246919840574
198,0.3787878751754761,0.6818181872367859,0.7787533875338754,0.10588075446360039,mmlu_offline:llama2-13b:high_school_geography,test,11.132056853733957
21,0.4761904776096344,0.4761904776096344,0.6727272727272727,0.29075356324513757,mmlu_offline:llama2-13b:high_school_government_and_politics,validation,1.4975544810295105
193,0.5544041395187378,0.6010362505912781,0.7325581395348837,0.15320265879902814,mmlu_offline:llama2-13b:high_school_government_and_politics,test,11.701223225332797
43,0.41860464215278625,0.5813953280448914,0.6000000000000001,0.08437491294949555,mmlu_offline:llama2-13b:high_school_macroeconomics,validation,2.6166924592107534
390,0.34358975291252136,0.6948717832565308,0.7144502098880597,0.08352951331016344,mmlu_offline:llama2-13b:high_school_macroeconomics,test,22.2130391090177
29,0.13793103396892548,0.8620689511299133,0.49,0.1663224286046521,mmlu_offline:llama2-13b:high_school_mathematics,validation,2.8700655228458345
270,0.10000000149011612,0.885185182094574,0.5860387136107301,0.1556004870820929,mmlu_offline:llama2-13b:high_school_mathematics,test,24.091800362803042
26,0.38461539149284363,0.5769230723381042,0.584375,0.13826237045801604,mmlu_offline:llama2-13b:high_school_microeconomics,validation,1.6424853396601975
238,0.3781512677669525,0.6176470518112183,0.7339714714714715,0.052759279723928756,mmlu_offline:llama2-13b:high_school_microeconomics,test,13.410324973985553
17,0.11764705926179886,0.5882353186607361,0.8,0.11701272866305182,mmlu_offline:llama2-13b:high_school_physics,validation,1.6373049532994628
151,0.20529800653457642,0.5894039869308472,0.606720430107527,0.05569563993555033,mmlu_offline:llama2-13b:high_school_physics,test,12.784360154066235
60,0.6000000238418579,0.6166666746139526,0.7175925925925926,0.14477621316909786,mmlu_offline:llama2-13b:high_school_psychology,validation,4.525596321094781
545,0.5357798337936401,0.6000000238418579,0.7102712653635822,0.14810950187368133,mmlu_offline:llama2-13b:high_school_psychology,test,39.77870375197381
23,0.043478261679410934,0.739130437374115,1.0,0.11681623562522557,mmlu_offline:llama2-13b:high_school_statistics,validation,2.7036360441707075
216,0.25925925374031067,0.6481481194496155,0.6172433035714285,0.04546652761874376,mmlu_offline:llama2-13b:high_school_statistics,test,24.26661741035059
22,0.5909090638160706,0.5909090638160706,0.7521367521367521,0.11890015547925775,mmlu_offline:llama2-13b:high_school_us_history,validation,8.857496860902756
204,0.6225489974021912,0.6813725233078003,0.7346354432968606,0.058818236285564936,mmlu_offline:llama2-13b:high_school_us_history,test,81.84324421314523
26,0.6153846383094788,0.6153846383094788,0.659375,0.24531601025507999,mmlu_offline:llama2-13b:high_school_world_history,validation,7.626528967171907
237,0.5274261832237244,0.5443037748336792,0.7218571428571428,0.29299857973549437,mmlu_offline:llama2-13b:high_school_world_history,test,63.16166483191773
23,0.3478260934352875,0.6086956262588501,0.6125,0.08941515632297684,mmlu_offline:llama2-13b:human_aging,validation,1.2870952957309783
223,0.3497757911682129,0.7219731211662292,0.7757736516357208,0.0899428778699695,mmlu_offline:llama2-13b:human_aging,test,12.036637181881815
12,0.25,0.5833333134651184,0.7407407407407407,0.2258895983298619,mmlu_offline:llama2-13b:human_sexuality,validation,0.9468705290928483
131,0.442748099565506,0.7480915784835815,0.8032593292394898,0.04626829114579064,mmlu_offline:llama2-13b:human_sexuality,test,8.225463420618325
13,0.3076923191547394,0.38461539149284363,0.5277777777777778,0.24034155790622422,mmlu_offline:llama2-13b:international_law,validation,1.0908339438028634
121,0.5206611752510071,0.5537189841270447,0.7115489874110563,0.12852707086515822,mmlu_offline:llama2-13b:international_law,test,8.766585732344538
11,0.3636363744735718,0.4545454680919647,0.4642857142857143,0.22858596389943903,mmlu_offline:llama2-13b:jurisprudence,validation,0.844814722891897
108,0.39814814925193787,0.6111111044883728,0.689087656529517,0.11096779119085386,mmlu_offline:llama2-13b:jurisprudence,test,6.662127588875592
18,0.5555555820465088,0.6666666865348816,1.0,0.27356943819257945,mmlu_offline:llama2-13b:logical_fallacies,validation,1.3321003247983754
163,0.4601227045059204,0.558282196521759,0.8164393939393939,0.17289700537371488,mmlu_offline:llama2-13b:logical_fallacies,test,10.761470428202301
11,0.4545454680919647,0.8181818127632141,0.8666666666666667,0.1689944484017112,mmlu_offline:llama2-13b:machine_learning,validation,1.237230550032109
112,0.2232142835855484,0.6071428656578064,0.6696551724137931,0.09712846098201618,mmlu_offline:llama2-13b:machine_learning,test,10.594959700014442
11,0.8181818127632141,0.4545454680919647,0.4444444444444444,0.24420903487638992,mmlu_offline:llama2-13b:management,validation,0.7298668189905584
103,0.43689319491386414,0.6213592290878296,0.6775862068965517,0.04219304936603437,mmlu_offline:llama2-13b:management,test,4.926721956580877
25,0.36000001430511475,0.4000000059604645,0.579861111111111,0.3605099844932556,mmlu_offline:llama2-13b:marketing,validation,1.7788173859007657
234,0.5170940160751343,0.6068376302719116,0.7265779273019821,0.1328879854617975,mmlu_offline:llama2-13b:marketing,test,14.036990110296756
11,0.7272727489471436,0.9090909361839294,1.0,0.22304687174883758,mmlu_offline:llama2-13b:medical_genetics,validation,0.753616102039814
100,0.4699999988079071,0.6499999761581421,0.7179847450822963,0.09471239268779755,mmlu_offline:llama2-13b:medical_genetics,test,5.284643779974431
86,0.604651153087616,0.7093023061752319,0.7446266968325792,0.128639162972916,mmlu_offline:llama2-13b:miscellaneous,validation,4.462237141095102
783,0.6168582439422607,0.7867177724838257,0.8004520358868185,0.06026359033767292,mmlu_offline:llama2-13b:miscellaneous,test,41.09523080289364
38,0.3947368562221527,0.6052631735801697,0.6855072463768116,0.12466839740150852,mmlu_offline:llama2-13b:moral_disputes,validation,2.723439041990787
346,0.424855500459671,0.615606963634491,0.6882712884148633,0.06868645531593717,mmlu_offline:llama2-13b:moral_disputes,test,22.833843195810914
100,0.47999998927116394,0.5199999809265137,0.5695112179487178,0.22842917919158934,mmlu_offline:llama2-13b:moral_scenarios,validation,10.84507331205532
895,0.44134077429771423,0.5586591958999634,0.6035822784810125,0.18773052878885965,mmlu_offline:llama2-13b:moral_scenarios,test,95.04036563495174
33,0.3030303120613098,0.4848484992980957,0.7434782608695653,0.2238345832535715,mmlu_offline:llama2-13b:nutrition,validation,2.666304396931082
306,0.36274510622024536,0.5686274766921997,0.7181566181566181,0.17654798389260284,mmlu_offline:llama2-13b:nutrition,test,22.9337076456286
34,0.3235294222831726,0.5,0.7509881422924901,0.17880916946074543,mmlu_offline:llama2-13b:philosophy,validation,2.18385514523834
311,0.34405145049095154,0.5369774699211121,0.690489279824079,0.12999541372348258,mmlu_offline:llama2-13b:philosophy,test,17.522999173030257
35,0.3142857253551483,0.7428571581840515,0.8579545454545455,0.17497444323131012,mmlu_offline:llama2-13b:prehistory,validation,2.4840661827474833
324,0.48148149251937866,0.6481481194496155,0.7095161782661783,0.018471339050634406,mmlu_offline:llama2-13b:prehistory,test,20.58988634077832
31,0.09677419066429138,0.4193548262119293,0.6190476190476191,0.26293371185179687,mmlu_offline:llama2-13b:professional_accounting,validation,3.870827734004706
282,0.14539006352424622,0.5106382966041565,0.6343487501265055,0.13169429509352287,mmlu_offline:llama2-13b:professional_accounting,test,33.52406713925302
170,0.364705890417099,0.48235294222831726,0.5508512544802866,0.14073877264471618,mmlu_offline:llama2-13b:professional_law,validation,42.94926384836435
1534,0.29400262236595154,0.4960886538028717,0.6107654478710488,0.13075947310809516,mmlu_offline:llama2-13b:professional_law,test,394.4154233946465
31,0.35483869910240173,0.5161290168762207,0.65,0.1254642874963822,mmlu_offline:llama2-13b:professional_medicine,validation,6.371038246899843
272,0.3345588147640228,0.6213235259056091,0.6907595167263674,0.02448771631016454,mmlu_offline:llama2-13b:professional_medicine,test,55.91155874310061
69,0.3913043439388275,0.4202898442745209,0.5970017636684303,0.2898608316545901,mmlu_offline:llama2-13b:professional_psychology,validation,5.675389701966196
612,0.3006536066532135,0.3888888955116272,0.6743511275904105,0.318077900733044,mmlu_offline:llama2-13b:professional_psychology,test,45.652470566798
12,0.25,0.75,0.7777777777777778,0.2107134560743968,mmlu_offline:llama2-13b:public_relations,validation,0.978002664167434
110,0.30909091234207153,0.6727272868156433,0.8144349845201239,0.0951860406182029,mmlu_offline:llama2-13b:public_relations,test,7.092878620605916
27,0.48148149251937866,0.6666666865348816,0.7142857142857143,0.06338534311011984,mmlu_offline:llama2-13b:security_studies,validation,2.5722767440602183
245,0.5265306234359741,0.6448979377746582,0.6792969794172681,0.016281350534789435,mmlu_offline:llama2-13b:security_studies,test,21.7703941562213
22,0.5909090638160706,0.5909090638160706,0.8076923076923077,0.2743915861303156,mmlu_offline:llama2-13b:sociology,validation,1.4018354429863393
201,0.4129353165626526,0.6815920472145081,0.7623545027567898,0.03561910378992261,mmlu_offline:llama2-13b:sociology,test,11.814754997380078
11,0.6363636255264282,0.7272727489471436,0.9107142857142857,0.11915924332358616,mmlu_offline:llama2-13b:us_foreign_policy,validation,0.8598434003069997
100,0.5699999928474426,0.6800000071525574,0.7625458996328028,0.0963327121734619,mmlu_offline:llama2-13b:us_foreign_policy,test,5.939061914104968
18,0.3333333432674408,0.7222222089767456,0.7222222222222223,0.05504900879330106,mmlu_offline:llama2-13b:virology,validation,1.4509547161869705
166,0.33734938502311707,0.6927710771560669,0.6908279220779221,0.054572747414370605,mmlu_offline:llama2-13b:virology,test,9.782661926932633
19,0.6315789222717285,0.7368420958518982,0.8214285714285714,0.07030458826767773,mmlu_offline:llama2-13b:world_religions,validation,1.0898000509478152
171,0.6432748436927795,0.6842105388641357,0.7413561847988078,0.048049892946990615,mmlu_offline:llama2-13b:world_religions,test,8.037828424945474
