N,acc,unc_acc,unc_auroc,unc_ece,dataset,split,ts
11,0.09090909361839294,0.9090909361839294,1.0,0.3135024580088529,mmlu_offline:llama2-13b_chat:abstract_algebra,validation,6.092895298730582
100,0.3499999940395355,0.6499999761581421,0.5863736263736263,0.04885244071483612,mmlu_offline:llama2-13b_chat:abstract_algebra,test,8.504605495370924
14,0.2857142984867096,0.7142857313156128,0.7375,0.13716487373624528,mmlu_offline:llama2-13b_chat:anatomy,validation,1.457008820027113
135,0.42222222685813904,0.5777778029441833,0.5689383715699505,0.23989807323173243,mmlu_offline:llama2-13b_chat:anatomy,test,9.63693961314857
16,0.5,0.5,0.4765625,0.2293817289173603,mmlu_offline:llama2-13b_chat:astronomy,validation,1.7492827717214823
152,0.5263158082962036,0.45394736528396606,0.6288194444444444,0.2412123880104015,mmlu_offline:llama2-13b_chat:astronomy,test,13.368100812658668
11,0.5454545617103577,0.5454545617103577,0.6333333333333333,0.07006483728235419,mmlu_offline:llama2-13b_chat:business_ethics,validation,1.7275988748297095
100,0.3199999928474426,0.6000000238418579,0.5441176470588235,0.05459985613822937,mmlu_offline:llama2-13b_chat:business_ethics,test,10.764087439049035
29,0.24137930572032928,0.7586206793785095,0.737012987012987,0.08425731083442423,mmlu_offline:llama2-13b_chat:clinical_knowledge,validation,2.720080146100372
265,0.2981131970882416,0.701886773109436,0.6046685722063426,0.06689100400456846,mmlu_offline:llama2-13b_chat:clinical_knowledge,test,20.778163843322545
16,0.25,0.75,0.3645833333333333,0.1266484148800373,mmlu_offline:llama2-13b_chat:college_biology,validation,1.8042616783641279
144,0.3958333432674408,0.6041666865348816,0.5977011494252874,0.08586261669794716,mmlu_offline:llama2-13b_chat:college_biology,test,13.42278285510838
8,0.0,0.625,,0.21263879537582397,mmlu_offline:llama2-13b_chat:college_chemistry,validation,1.4289838098920882
100,0.17000000178813934,0.5400000214576721,0.7274982282069454,0.12688382089138028,mmlu_offline:llama2-13b_chat:college_chemistry,test,10.438058806117624
11,0.27272728085517883,0.27272728085517883,0.9583333333333333,0.461508723822507,mmlu_offline:llama2-13b_chat:college_computer_science,validation,2.2554741627536714
100,0.23999999463558197,0.3199999928474426,0.5718201754385964,0.40693994760513297,mmlu_offline:llama2-13b_chat:college_computer_science,test,16.9540108689107
11,0.0,0.1818181872367859,,0.42717473615299567,mmlu_offline:llama2-13b_chat:college_mathematics,validation,1.9583898726850748
100,0.09000000357627869,0.10999999940395355,0.6611721611721612,0.5293692088127135,mmlu_offline:llama2-13b_chat:college_mathematics,test,12.413702394347638
22,0.5,0.3636363744735718,0.3347107438016529,0.32377393950115546,mmlu_offline:llama2-13b_chat:college_medicine,validation,2.5626815827563405
173,0.3988439440727234,0.6242774724960327,0.6192865105908585,0.05937549661349698,mmlu_offline:llama2-13b_chat:college_medicine,test,23.06238781521097
11,0.27272728085517883,0.7272727489471436,0.6666666666666666,0.1533585624261336,mmlu_offline:llama2-13b_chat:college_physics,validation,1.7012592549435794
102,0.1568627506494522,0.6764705777168274,0.4811046511627907,0.09769900055492627,mmlu_offline:llama2-13b_chat:college_physics,test,10.817913647275418
11,0.5454545617103577,0.4545454680919647,0.6000000000000001,0.21150860461321747,mmlu_offline:llama2-13b_chat:computer_security,validation,1.5625651529990137
100,0.550000011920929,0.5099999904632568,0.558989898989899,0.12178383529186246,mmlu_offline:llama2-13b_chat:computer_security,test,8.428909617941827
26,0.3461538553237915,0.6538461446762085,0.41503267973856206,0.17535776587632987,mmlu_offline:llama2-13b_chat:conceptual_physics,validation,2.3525444851256907
235,0.4553191363811493,0.5489361882209778,0.5415084696261683,0.24387714406277267,mmlu_offline:llama2-13b_chat:conceptual_physics,test,16.388035914860666
12,0.25,0.25,0.6666666666666667,0.5242814719676971,mmlu_offline:llama2-13b_chat:econometrics,validation,1.9703407613560557
114,0.1315789520740509,0.1666666716337204,0.6417508417508417,0.5448640553574813,mmlu_offline:llama2-13b_chat:econometrics,test,14.409539696760476
16,0.25,0.6875,0.7395833333333334,0.17271393537521362,mmlu_offline:llama2-13b_chat:electrical_engineering,validation,1.763615540228784
145,0.24827586114406586,0.682758629322052,0.605249745158002,0.07234196169623014,mmlu_offline:llama2-13b_chat:electrical_engineering,test,13.289629277773201
41,0.26829269528388977,0.5609756112098694,0.4545454545454546,0.04743828424593299,mmlu_offline:llama2-13b_chat:elementary_mathematics,validation,4.598673515953124
378,0.3174603283405304,0.6058201193809509,0.6172803617571059,0.040519611860709216,mmlu_offline:llama2-13b_chat:elementary_mathematics,test,36.08765034331009
14,0.2142857164144516,0.5,0.4696969696969696,0.13091427513531276,mmlu_offline:llama2-13b_chat:formal_logic,validation,1.9885842008516192
126,0.261904776096344,0.4047619104385376,0.42163571195829264,0.19047557739984422,mmlu_offline:llama2-13b_chat:formal_logic,test,13.593379731755704
10,0.20000000298023224,0.800000011920929,0.1875,0.2644765675067902,mmlu_offline:llama2-13b_chat:global_facts,validation,1.359396114014089
100,0.12999999523162842,0.8700000047683716,0.4093722369584438,0.20627350449562068,mmlu_offline:llama2-13b_chat:global_facts,test,9.08062860602513
32,0.3125,0.71875,0.6363636363636364,0.07956219278275965,mmlu_offline:llama2-13b_chat:high_school_biology,validation,3.337329830043018
310,0.42258065938949585,0.5677419304847717,0.5194251354002303,0.09580597492956348,mmlu_offline:llama2-13b_chat:high_school_biology,test,28.851001317147166
22,0.1818181872367859,0.7727272510528564,0.5069444444444444,0.23279838128523392,mmlu_offline:llama2-13b_chat:high_school_chemistry,validation,2.6827629576437175
203,0.1822660118341446,0.6847290396690369,0.659801367632693,0.12271686639691812,mmlu_offline:llama2-13b_chat:high_school_chemistry,test,21.08966591907665
9,0.6666666865348816,0.5555555820465088,0.25,0.2956043349372017,mmlu_offline:llama2-13b_chat:high_school_computer_science,validation,1.9114730269648135
100,0.4300000071525574,0.4699999988079071,0.43492452060383513,0.15767434298992156,mmlu_offline:llama2-13b_chat:high_school_computer_science,test,15.303434223402292
18,0.7777777910232544,0.7777777910232544,0.7232142857142857,0.16807029313511315,mmlu_offline:llama2-13b_chat:high_school_european_history,validation,10.488621573895216
165,0.7575757503509521,0.7636363506317139,0.6866000000000001,0.13882055932825257,mmlu_offline:llama2-13b_chat:high_school_european_history,test,92.27500254055485
22,0.5,0.5454545617103577,0.3347107438016529,0.2773561694405296,mmlu_offline:llama2-13b_chat:high_school_geography,validation,2.0620602359995246
198,0.39393940567970276,0.6515151262283325,0.6250534188034188,0.04027833511130977,mmlu_offline:llama2-13b_chat:high_school_geography,test,15.468528843019158
21,0.523809552192688,0.6190476417541504,0.8590909090909091,0.10485361587433588,mmlu_offline:llama2-13b_chat:high_school_government_and_politics,validation,2.2739145029336214
193,0.6321243643760681,0.5854922533035278,0.7016855229739091,0.03191699049015734,mmlu_offline:llama2-13b_chat:high_school_government_and_politics,test,16.299166364595294
43,0.4883720874786377,0.5116279125213623,0.3939393939393939,0.17762016002521958,mmlu_offline:llama2-13b_chat:high_school_macroeconomics,validation,3.7502675340510905
390,0.3692307770252228,0.6333333253860474,0.5534948057813912,0.055903827991241106,mmlu_offline:llama2-13b_chat:high_school_macroeconomics,test,30.6799554717727
29,0.06896551698446274,0.48275861144065857,0.8333333333333334,0.10719008897912914,mmlu_offline:llama2-13b_chat:high_school_mathematics,validation,3.6776945488527417
270,0.10000000149011612,0.4888888895511627,0.5247675659198292,0.07838314992410166,mmlu_offline:llama2-13b_chat:high_school_mathematics,test,30.36841768398881
26,0.3076923191547394,0.692307710647583,0.5,0.1469123111321376,mmlu_offline:llama2-13b_chat:high_school_microeconomics,validation,2.5105342441238463
238,0.36554622650146484,0.6386554837226868,0.5389738905381747,0.046966290023146555,mmlu_offline:llama2-13b_chat:high_school_microeconomics,test,18.36419760994613
17,0.29411765933036804,0.4117647111415863,0.325,0.1451701136196361,mmlu_offline:llama2-13b_chat:high_school_physics,validation,2.4031323925592005
151,0.17218543589115143,0.556291401386261,0.5289230769230769,0.03333784728650223,mmlu_offline:llama2-13b_chat:high_school_physics,test,16.4435578789562
60,0.6000000238418579,0.6499999761581421,0.7280092592592592,0.10028457442919413,mmlu_offline:llama2-13b_chat:high_school_psychology,validation,6.21264219796285
545,0.5064220428466797,0.6348623633384705,0.6689968212919563,0.04903046712962861,mmlu_offline:llama2-13b_chat:high_school_psychology,test,52.34254237776622
23,0.17391304671764374,0.17391304671764374,0.5855263157894738,0.6035383965658105,mmlu_offline:llama2-13b_chat:high_school_statistics,validation,3.5236259661614895
216,0.25925925374031067,0.25462964177131653,0.6146763392857142,0.5061800733760551,mmlu_offline:llama2-13b_chat:high_school_statistics,test,29.736419412773103
22,0.7727272510528564,0.7727272510528564,0.3647058823529412,0.14048103310845117,mmlu_offline:llama2-13b_chat:high_school_us_history,validation,10.515725600067526
204,0.6764705777168274,0.6813725233078003,0.6850021958717611,0.1776197424121932,mmlu_offline:llama2-13b_chat:high_school_us_history,test,89.52420770330355
26,0.6538461446762085,0.6538461446762085,0.40849673202614384,0.27726786640974194,mmlu_offline:llama2-13b_chat:high_school_world_history,validation,8.913544000592083
237,0.6033755540847778,0.6118143200874329,0.6205177800922482,0.23914790480448728,mmlu_offline:llama2-13b_chat:high_school_world_history,test,71.73159368569031
23,0.3913043439388275,0.6521739363670349,0.39285714285714285,0.21198952198028567,mmlu_offline:llama2-13b_chat:human_aging,validation,2.203203432261944
223,0.3901345431804657,0.573991060256958,0.5232420554428668,0.09087752387128072,mmlu_offline:llama2-13b_chat:human_aging,test,16.826623728964478
12,0.3333333432674408,0.6666666865348816,0.71875,0.12622105081876117,mmlu_offline:llama2-13b_chat:human_sexuality,validation,1.4702367009595037
131,0.5114504098892212,0.4885496199131012,0.47609608208955223,0.30933262737652734,mmlu_offline:llama2-13b_chat:human_sexuality,test,10.973024572245777
13,0.6153846383094788,0.5384615659713745,0.525,0.16499205277516293,mmlu_offline:llama2-13b_chat:international_law,validation,1.6984645421616733
121,0.6280992031097412,0.4876033067703247,0.5922514619883041,0.13995879740754438,mmlu_offline:llama2-13b_chat:international_law,test,12.029819461982697
11,0.1818181872367859,0.8181818127632141,0.9444444444444444,0.20291367985985498,mmlu_offline:llama2-13b_chat:jurisprudence,validation,1.42330921581015
108,0.3611111044883728,0.6481481194496155,0.49386845039018956,0.10450907051563262,mmlu_offline:llama2-13b_chat:jurisprudence,test,9.280594292096794
18,0.6666666865348816,0.3888888955116272,0.6319444444444444,0.17678797245025632,mmlu_offline:llama2-13b_chat:logical_fallacies,validation,2.072927303146571
163,0.48466256260871887,0.546012282371521,0.6055605786618444,0.028675356159912287,mmlu_offline:llama2-13b_chat:logical_fallacies,test,14.954039226751775
11,0.27272728085517883,0.5454545617103577,0.8333333333333334,0.09042093428698451,mmlu_offline:llama2-13b_chat:machine_learning,validation,1.7726069050841033
112,0.2946428656578064,0.3482142984867096,0.5690448791714614,0.28904575535229277,mmlu_offline:llama2-13b_chat:machine_learning,test,13.421338161919266
11,0.6363636255264282,0.3636363744735718,0.4642857142857143,0.33115819909355854,mmlu_offline:llama2-13b_chat:management,validation,1.2721672803163528
103,0.42718446254730225,0.582524299621582,0.7290061633281972,0.11664977640781589,mmlu_offline:llama2-13b_chat:management,test,7.3457970935851336
25,0.23999999463558197,0.7599999904632568,0.5131578947368421,0.17895383119583133,mmlu_offline:llama2-13b_chat:marketing,validation,2.793148970231414
234,0.44871795177459717,0.5940170884132385,0.6535252860834256,0.034339462844734506,mmlu_offline:llama2-13b_chat:marketing,test,19.840589591301978
11,0.9090909361839294,0.27272728085517883,0.95,0.2982056357643821,mmlu_offline:llama2-13b_chat:medical_genetics,validation,1.3323628040961921
100,0.44999998807907104,0.6200000047683716,0.6305050505050506,0.05670016169548038,mmlu_offline:llama2-13b_chat:medical_genetics,test,7.592951889149845
86,0.5581395626068115,0.569767415523529,0.546875,0.04803475252417632,mmlu_offline:llama2-13b_chat:miscellaneous,validation,6.637100180611014
783,0.618135392665863,0.6155810952186584,0.5937629564111778,0.04143571480632892,mmlu_offline:llama2-13b_chat:miscellaneous,test,56.97743914928287
38,0.42105263471603394,0.5,0.3380681818181818,0.19321134372761375,mmlu_offline:llama2-13b_chat:moral_disputes,validation,3.8617513310164213
346,0.43063583970069885,0.5693641901016235,0.5695499608217219,0.09121850374117066,mmlu_offline:llama2-13b_chat:moral_disputes,test,30.940168742090464
100,0.4300000071525574,0.5699999928474426,0.6101591187270502,0.07880223035812381,mmlu_offline:llama2-13b_chat:moral_scenarios,validation,13.48694784194231
895,0.38100558519363403,0.6178770661354065,0.48481848883619,0.041851235434995714,mmlu_offline:llama2-13b_chat:moral_scenarios,test,117.1311968350783
33,0.3333333432674408,0.5757575631141663,0.5950413223140496,0.1518612467881405,mmlu_offline:llama2-13b_chat:nutrition,validation,3.7326081567443907
306,0.4542483687400818,0.5947712659835815,0.6332443027613837,0.05883356636645747,mmlu_offline:llama2-13b_chat:nutrition,test,30.377789168152958
34,0.3235294222831726,0.6470588445663452,0.5810276679841897,0.06532101245487437,mmlu_offline:llama2-13b_chat:philosophy,validation,3.2353911329992115
311,0.3633440434932709,0.5980707406997681,0.5373201036917851,0.05944668326730508,mmlu_offline:llama2-13b_chat:philosophy,test,24.49376967502758
35,0.37142857909202576,0.48571428656578064,0.465034965034965,0.15878762006759645,mmlu_offline:llama2-13b_chat:prehistory,validation,3.507382484152913
324,0.4413580298423767,0.5895061492919922,0.6115790287061004,0.04010576782403168,mmlu_offline:llama2-13b_chat:prehistory,test,27.759120014030486
31,0.16129031777381897,0.6129032373428345,0.7307692307692307,0.15226870198403633,mmlu_offline:llama2-13b_chat:professional_accounting,validation,4.993377726059407
282,0.1879432648420334,0.5567376017570496,0.6383373156463705,0.03522385773083846,mmlu_offline:llama2-13b_chat:professional_accounting,test,41.6137392828241
170,0.38235294818878174,0.43529412150382996,0.4936996336996337,0.1998041331768036,mmlu_offline:llama2-13b_chat:professional_law,validation,48.88461893238127
1534,0.34810951352119446,0.4276401698589325,0.4856741573033708,0.20511093599699934,mmlu_offline:llama2-13b_chat:professional_law,test,445.0248050717637
31,0.35483869910240173,0.3870967626571655,0.5431818181818182,0.29678453168561386,mmlu_offline:llama2-13b_chat:professional_medicine,validation,8.22736499691382
272,0.2904411852359772,0.3235294222831726,0.5335803764675019,0.33729718800853276,mmlu_offline:llama2-13b_chat:professional_medicine,test,65.01122479187325
69,0.37681159377098083,0.6376811861991882,0.6019677996422181,0.08373027780781624,mmlu_offline:llama2-13b_chat:professional_psychology,validation,7.542389519978315
612,0.3464052379131317,0.5833333134651184,0.6012323113207547,0.031442677565649446,mmlu_offline:llama2-13b_chat:professional_psychology,test,61.04602757515386
12,0.1666666716337204,0.75,0.45,0.2509614030520122,mmlu_offline:llama2-13b_chat:public_relations,validation,1.6989710349589586
110,0.30909091234207153,0.7090908885002136,0.7496130030959752,0.09348988262089818,mmlu_offline:llama2-13b_chat:public_relations,test,9.833641842007637
27,0.5555555820465088,0.5925925970077515,0.7277777777777777,0.10604044243141458,mmlu_offline:llama2-13b_chat:security_studies,validation,3.6186925056390464
245,0.6612244844436646,0.44081631302833557,0.6214859437751005,0.1798704675265721,mmlu_offline:llama2-13b_chat:security_studies,test,28.680457770824432
22,0.5909090638160706,0.40909090638160706,0.641025641025641,0.34188786961815576,mmlu_offline:llama2-13b_chat:sociology,validation,2.1802047677338123
201,0.447761207818985,0.5970149040222168,0.65985985985986,0.1376205718339379,mmlu_offline:llama2-13b_chat:sociology,test,16.718793543986976
11,0.7272727489471436,0.3636363744735718,0.5833333333333334,0.3547039953145114,mmlu_offline:llama2-13b_chat:us_foreign_policy,validation,1.4343096297234297
100,0.5899999737739563,0.4699999988079071,0.6076891277387351,0.19827551484107972,mmlu_offline:llama2-13b_chat:us_foreign_policy,test,8.442473297938704
18,0.5555555820465088,0.4444444477558136,0.45625,0.38465572396914166,mmlu_offline:llama2-13b_chat:virology,validation,2.1699032858014107
166,0.34939759969711304,0.650602400302887,0.5494891443167306,0.1185039540371263,mmlu_offline:llama2-13b_chat:virology,test,13.904319427907467
19,0.6315789222717285,0.42105263471603394,0.47023809523809523,0.15333702689722967,mmlu_offline:llama2-13b_chat:world_religions,validation,1.863375535234809
171,0.6081871390342712,0.5204678177833557,0.554247990815155,0.05825954187683197,mmlu_offline:llama2-13b_chat:world_religions,test,11.882697714027017
