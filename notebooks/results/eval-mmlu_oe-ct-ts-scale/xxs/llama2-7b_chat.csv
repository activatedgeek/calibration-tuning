N,acc,unc_acc,unc_auroc,unc_ece,dataset,split,ts
11,0.09090909361839294,0.9090909361839294,0.09999999999999998,0.29530318758704444,mmlu_offline:llama2-7b_chat:abstract_algebra,validation,4.19015143532306
100,0.20999999344348907,0.75,0.497588908981314,0.11485104858875271,mmlu_offline:llama2-7b_chat:abstract_algebra,test,5.951617148704827
14,0.2142857164144516,0.7142857313156128,0.8181818181818182,0.24189879638808118,mmlu_offline:llama2-7b_chat:anatomy,validation,1.1204462149180472
135,0.385185182094574,0.6518518328666687,0.6395968489341984,0.1071218495015745,mmlu_offline:llama2-7b_chat:anatomy,test,6.7706919722259045
16,0.5625,0.375,0.4365079365079365,0.2508447729051113,mmlu_offline:llama2-7b_chat:astronomy,validation,1.2017970709130168
152,0.42763158679008484,0.6118420958518982,0.6001768346595933,0.04335181728789683,mmlu_offline:llama2-7b_chat:astronomy,test,8.984162423294038
11,0.4545454680919647,0.4545454680919647,0.4833333333333334,0.29563048211011017,mmlu_offline:llama2-7b_chat:business_ethics,validation,1.0932120149955153
100,0.33000001311302185,0.3400000035762787,0.6255088195386703,0.43025882720947267,mmlu_offline:llama2-7b_chat:business_ethics,test,7.094941627699882
29,0.17241379618644714,0.7241379022598267,0.7041666666666666,0.15146191161254358,mmlu_offline:llama2-7b_chat:clinical_knowledge,validation,1.8392106657847762
265,0.2792452871799469,0.6226415038108826,0.5660464129050516,0.05000411429495179,mmlu_offline:llama2-7b_chat:clinical_knowledge,test,13.853895432781428
16,0.375,0.625,0.7666666666666666,0.0430762879550457,mmlu_offline:llama2-7b_chat:college_biology,validation,1.187419201247394
144,0.3402777910232544,0.5208333134651184,0.6302900107411386,0.07516967629392941,mmlu_offline:llama2-7b_chat:college_biology,test,8.803745116107166
8,0.0,0.375,,0.24625487625598907,mmlu_offline:llama2-7b_chat:college_chemistry,validation,0.7354292278178036
100,0.14000000059604645,0.5,0.6283222591362126,0.15864361166954039,mmlu_offline:llama2-7b_chat:college_chemistry,test,7.0844368012622
11,0.27272728085517883,0.4545454680919647,0.7083333333333333,0.19268964637409558,mmlu_offline:llama2-7b_chat:college_computer_science,validation,1.408586454577744
100,0.17000000178813934,0.4399999976158142,0.47802976612331677,0.18424258410930638,mmlu_offline:llama2-7b_chat:college_computer_science,test,10.460987932048738
11,0.0,0.27272728085517883,,0.4043160785328258,mmlu_offline:llama2-7b_chat:college_mathematics,validation,1.1971534010954201
100,0.2199999988079071,0.3400000035762787,0.657925407925408,0.3204963076114655,mmlu_offline:llama2-7b_chat:college_mathematics,test,8.018553883768618
22,0.3181818127632141,0.5909090638160706,0.5333333333333333,0.12061937830664896,mmlu_offline:llama2-7b_chat:college_medicine,validation,1.6868956559337676
173,0.2947976887226105,0.6994219422340393,0.6359691417550627,0.061716249223389356,mmlu_offline:llama2-7b_chat:college_medicine,test,14.41653706971556
11,0.09090909361839294,0.7272727489471436,0.9,0.1558632254600525,mmlu_offline:llama2-7b_chat:college_physics,validation,1.114581482950598
102,0.0882352963089943,0.6372548937797546,0.6445639187574671,0.08098697662353513,mmlu_offline:llama2-7b_chat:college_physics,test,7.282017062883824
11,0.6363636255264282,0.8181818127632141,0.9107142857142857,0.22546632181514392,mmlu_offline:llama2-7b_chat:computer_security,validation,1.0429146559908986
100,0.5,0.550000011920929,0.5362,0.12472016572952273,mmlu_offline:llama2-7b_chat:computer_security,test,5.7500461540184915
26,0.1538461595773697,0.7307692170143127,0.6534090909090909,0.17108874596082246,mmlu_offline:llama2-7b_chat:conceptual_physics,validation,1.7143793269060552
235,0.3787234127521515,0.5829787254333496,0.5177774357395721,0.022799865489310442,mmlu_offline:llama2-7b_chat:conceptual_physics,test,11.60691152419895
12,0.3333333432674408,0.5833333134651184,0.703125,0.12616137663523355,mmlu_offline:llama2-7b_chat:econometrics,validation,1.2658518124371767
114,0.17543859779834747,0.7982456088066101,0.6159574468085106,0.09097909875083386,mmlu_offline:llama2-7b_chat:econometrics,test,9.058510371949524
16,0.1875,0.8125,0.41025641025641024,0.2201887182891369,mmlu_offline:llama2-7b_chat:electrical_engineering,validation,1.1635325700044632
145,0.2137930989265442,0.6896551847457886,0.39261460101867574,0.11407625017494992,mmlu_offline:llama2-7b_chat:electrical_engineering,test,8.941020700149238
41,0.3658536672592163,0.5609756112098694,0.47435897435897434,0.11492160762228618,mmlu_offline:llama2-7b_chat:elementary_mathematics,validation,3.1070109140127897
378,0.28042328357696533,0.6375661492347717,0.4965142896781354,0.05800116740206563,mmlu_offline:llama2-7b_chat:elementary_mathematics,test,24.145560977980494
14,0.4285714328289032,0.6428571343421936,0.46875,0.3384689901556287,mmlu_offline:llama2-7b_chat:formal_logic,validation,1.2017489457502961
126,0.2539682686328888,0.7142857313156128,0.38347739361702127,0.12915716805155317,mmlu_offline:llama2-7b_chat:formal_logic,test,8.814751512836665
10,0.20000000298023224,0.6000000238418579,0.25,0.12590615749359133,mmlu_offline:llama2-7b_chat:global_facts,validation,0.983626919798553
100,0.07999999821186066,0.7099999785423279,0.5842391304347826,0.11511960327625276,mmlu_offline:llama2-7b_chat:global_facts,test,5.85126576712355
32,0.3125,0.40625,0.4159090909090909,0.288869246840477,mmlu_offline:llama2-7b_chat:high_school_biology,validation,2.1255694502033293
310,0.3838709592819214,0.4612903296947479,0.5522020326455189,0.15755243685937698,mmlu_offline:llama2-7b_chat:high_school_biology,test,18.73183426912874
22,0.1818181872367859,0.5,0.3472222222222222,0.11283488707108931,mmlu_offline:llama2-7b_chat:high_school_chemistry,validation,1.7285367930307984
203,0.1428571492433548,0.546798050403595,0.5975029726516052,0.04756555915466086,mmlu_offline:llama2-7b_chat:high_school_chemistry,test,13.613883731886744
9,0.4444444477558136,0.6666666865348816,0.6,0.05777245759963985,mmlu_offline:llama2-7b_chat:high_school_computer_science,validation,1.2222843361087143
100,0.4099999964237213,0.5199999809265137,0.5307978503513848,0.08098732709884647,mmlu_offline:llama2-7b_chat:high_school_computer_science,test,9.431151447352022
18,0.8333333134651184,0.8333333134651184,0.6000000000000001,0.14572352170944217,mmlu_offline:llama2-7b_chat:high_school_european_history,validation,6.298001995310187
165,0.7090908885002136,0.7090908885002136,0.5935719373219372,0.049808513034473784,mmlu_offline:llama2-7b_chat:high_school_european_history,test,54.454553429968655
22,0.4545454680919647,0.40909090638160706,0.3958333333333333,0.2015389637513594,mmlu_offline:llama2-7b_chat:high_school_geography,validation,1.4363417220301926
198,0.3737373650074005,0.5858585834503174,0.621512641673932,0.013580039896146198,mmlu_offline:llama2-7b_chat:high_school_geography,test,10.228822028264403
21,0.523809552192688,0.4285714328289032,0.38181818181818183,0.1499764181318737,mmlu_offline:llama2-7b_chat:high_school_government_and_politics,validation,1.4837351790629327
193,0.5233160853385925,0.575129508972168,0.6316185966422729,0.025794501749345083,mmlu_offline:llama2-7b_chat:high_school_government_and_politics,test,10.47565029002726
43,0.39534884691238403,0.6279069781303406,0.33936651583710403,0.22914615758629733,mmlu_offline:llama2-7b_chat:high_school_macroeconomics,validation,2.545238718856126
390,0.2897436022758484,0.656410276889801,0.4924603047825948,0.027775257061689332,mmlu_offline:llama2-7b_chat:high_school_macroeconomics,test,19.98958367621526
29,0.0,1.0,,0.3231863708331667,mmlu_offline:llama2-7b_chat:high_school_mathematics,validation,2.4550805049948394
270,0.0555555559694767,0.9259259104728699,0.4701960784313726,0.26878977175112123,mmlu_offline:llama2-7b_chat:high_school_mathematics,test,19.484408469870687
26,0.26923078298568726,0.42307692766189575,0.46240601503759404,0.1673427843130552,mmlu_offline:llama2-7b_chat:high_school_microeconomics,validation,1.7399535817094147
238,0.3403361439704895,0.5252100825309753,0.5592907132185263,0.06047318713003851,mmlu_offline:llama2-7b_chat:high_school_microeconomics,test,12.10892596328631
17,0.0,0.0,,0.8148911174605875,mmlu_offline:llama2-7b_chat:high_school_physics,validation,1.5993271521292627
151,0.18543046712875366,0.18543046712875366,0.6290650406504066,0.6124682493557204,mmlu_offline:llama2-7b_chat:high_school_physics,test,10.489130936097354
60,0.5666666626930237,0.6000000238418579,0.5486425339366515,0.15490604341030123,mmlu_offline:llama2-7b_chat:high_school_psychology,validation,3.9055363056249917
545,0.5009174346923828,0.5100917220115662,0.5219174208144797,0.10127891280235499,mmlu_offline:llama2-7b_chat:high_school_psychology,test,33.406318864785135
23,0.17391304671764374,0.3478260934352875,0.5263157894736842,0.30976195957349695,mmlu_offline:llama2-7b_chat:high_school_statistics,validation,2.1840590997599065
216,0.19907407462596893,0.2916666567325592,0.5260787740287672,0.36001470077920844,mmlu_offline:llama2-7b_chat:high_school_statistics,test,18.585438995156437
22,0.6363636255264282,0.6363636255264282,0.6071428571428571,0.18976694887334652,mmlu_offline:llama2-7b_chat:high_school_us_history,validation,6.0430464977398515
204,0.5784313678741455,0.5882353186607361,0.5922841939298384,0.15447954979597353,mmlu_offline:llama2-7b_chat:high_school_us_history,test,52.84450797410682
26,0.7307692170143127,0.7307692170143127,0.5451127819548872,0.09607124328613281,mmlu_offline:llama2-7b_chat:high_school_world_history,validation,5.264919685665518
237,0.552742600440979,0.5400843620300293,0.5075615728071439,0.13985380241136514,mmlu_offline:llama2-7b_chat:high_school_world_history,test,43.496811538003385
23,0.30434781312942505,0.47826087474823,0.5535714285714286,0.13027494368345846,mmlu_offline:llama2-7b_chat:human_aging,validation,1.3019909369759262
223,0.33183857798576355,0.3991031348705292,0.5490658443678578,0.21003076206942847,mmlu_offline:llama2-7b_chat:human_aging,test,11.399350577965379
12,0.25,0.5,0.38888888888888895,0.12794504563013714,mmlu_offline:llama2-7b_chat:human_sexuality,validation,1.0068812710233033
131,0.4198473393917084,0.5572519302368164,0.5275119617224879,0.06301907819646005,mmlu_offline:llama2-7b_chat:human_sexuality,test,7.119937913957983
13,0.4615384638309479,0.6153846383094788,0.6428571428571429,0.09714728135329027,mmlu_offline:llama2-7b_chat:international_law,validation,1.1567113660275936
121,0.6115702390670776,0.42975205183029175,0.6030764807360551,0.17042563818703013,mmlu_offline:llama2-7b_chat:international_law,test,7.644997369032353
11,0.5454545617103577,0.6363636255264282,0.6,0.24512632868506692,mmlu_offline:llama2-7b_chat:jurisprudence,validation,0.9579777391627431
108,0.5185185074806213,0.4166666567325592,0.4177541208791209,0.16913219292958576,mmlu_offline:llama2-7b_chat:jurisprudence,test,6.059771894942969
18,0.4444444477558136,0.5,0.68125,0.09968146350648668,mmlu_offline:llama2-7b_chat:logical_fallacies,validation,1.3815978639759123
163,0.42944785952568054,0.4601227045059204,0.5264208909370199,0.13483200519362845,mmlu_offline:llama2-7b_chat:logical_fallacies,test,9.523215882014483
11,0.3636363744735718,0.6363636255264282,0.5535714285714286,0.046691840345209276,mmlu_offline:llama2-7b_chat:machine_learning,validation,1.207804974168539
112,0.1964285671710968,0.7767857313156128,0.5896464646464646,0.13548052736691066,mmlu_offline:llama2-7b_chat:machine_learning,test,8.434312973171473
11,0.27272728085517883,0.4545454680919647,0.5625,0.19138686765323984,mmlu_offline:llama2-7b_chat:management,validation,0.8318317369557917
103,0.3689320385456085,0.41747573018074036,0.5277327935222672,0.22376652712960843,mmlu_offline:llama2-7b_chat:management,test,4.970830214209855
25,0.20000000298023224,0.23999999463558197,0.48500000000000004,0.5111566877365112,mmlu_offline:llama2-7b_chat:marketing,validation,1.8609851165674627
234,0.39743590354919434,0.4188034236431122,0.5566613284526805,0.31885892152786255,mmlu_offline:llama2-7b_chat:marketing,test,12.43160831136629
11,0.7272727489471436,0.7272727489471436,0.3125,0.24436220797625458,mmlu_offline:llama2-7b_chat:medical_genetics,validation,0.9939327333122492
100,0.4099999964237213,0.46000000834465027,0.5938404299297231,0.15975697517395016,mmlu_offline:llama2-7b_chat:medical_genetics,test,5.055268805008382
86,0.41860464215278625,0.5232558250427246,0.4930555555555556,0.1551162066847779,mmlu_offline:llama2-7b_chat:miscellaneous,validation,4.483974929898977
783,0.4533844292163849,0.5300127863883972,0.5241246544688692,0.07702698036171923,mmlu_offline:llama2-7b_chat:miscellaneous,test,38.713284420315176
38,0.4736842215061188,0.5263158082962036,0.6097222222222223,0.12624665937925642,mmlu_offline:llama2-7b_chat:moral_disputes,validation,2.516798498108983
346,0.41040462255477905,0.5953757166862488,0.5293945042805854,0.05166701154212728,mmlu_offline:llama2-7b_chat:moral_disputes,test,20.376249853055924
100,0.5099999904632568,0.5699999928474426,0.5778311324529812,0.03992170810699458,mmlu_offline:llama2-7b_chat:moral_scenarios,validation,8.549719326663762
895,0.46145251393318176,0.4737430214881897,0.49280640591562597,0.05443384674008332,mmlu_offline:llama2-7b_chat:moral_scenarios,test,73.29248384805396
33,0.3636363744735718,0.6969696879386902,0.5059523809523809,0.1364613085082083,mmlu_offline:llama2-7b_chat:nutrition,validation,2.530754331033677
306,0.4084967374801636,0.5718954205513,0.6029834254143647,0.08200182342061811,mmlu_offline:llama2-7b_chat:nutrition,test,19.597451902925968
34,0.3529411852359772,0.5,0.6685606060606061,0.17169938367955825,mmlu_offline:llama2-7b_chat:philosophy,validation,2.1172471698373556
311,0.3376205861568451,0.43408361077308655,0.5628294036061026,0.16401120167453193,mmlu_offline:llama2-7b_chat:philosophy,test,15.92126561095938
35,0.2857142984867096,0.4000000059604645,0.622,0.2833698885781424,mmlu_offline:llama2-7b_chat:prehistory,validation,2.3645496270619333
324,0.34259259700775146,0.37345677614212036,0.5502685784375925,0.2941290107038286,mmlu_offline:llama2-7b_chat:prehistory,test,18.34548682300374
31,0.06451612710952759,0.6129032373428345,0.6206896551724138,0.07758047696082823,mmlu_offline:llama2-7b_chat:professional_accounting,validation,3.058677867986262
282,0.1560283750295639,0.5957446694374084,0.5254965622612682,0.025027841963666538,mmlu_offline:llama2-7b_chat:professional_accounting,test,25.600084954872727
170,0.30588236451148987,0.658823549747467,0.4950293350717079,0.09539263213382049,mmlu_offline:llama2-7b_chat:professional_law,validation,29.861157371662557
1534,0.3200782239437103,0.6088657379150391,0.475800262832617,0.042169338099493345,mmlu_offline:llama2-7b_chat:professional_law,test,272.9817446428351
31,0.25806450843811035,0.32258063554763794,0.49728260869565216,0.3228938964105421,mmlu_offline:llama2-7b_chat:professional_medicine,validation,4.64555422635749
272,0.23529411852359772,0.2389705926179886,0.4883563701923076,0.42075901960625367,mmlu_offline:llama2-7b_chat:professional_medicine,test,39.83145947800949
69,0.37681159377098083,0.5942028760910034,0.4968694096601074,0.0814835282339566,mmlu_offline:llama2-7b_chat:professional_psychology,validation,4.869203339796513
612,0.32189542055130005,0.6225489974021912,0.5176808757874136,0.04657148438341475,mmlu_offline:llama2-7b_chat:professional_psychology,test,38.38877916196361
12,0.3333333432674408,0.25,0.25,0.37130573888619745,mmlu_offline:llama2-7b_chat:public_relations,validation,1.0403208872303367
110,0.3272727131843567,0.4272727370262146,0.5028153153153153,0.19766903031956068,mmlu_offline:llama2-7b_chat:public_relations,test,6.1842982028611
27,0.7407407164573669,0.6666666865348816,0.6714285714285715,0.10757264163759023,mmlu_offline:llama2-7b_chat:security_studies,validation,2.3442843589000404
245,0.6816326379776001,0.6489796042442322,0.6419084907108858,0.10065369095121114,mmlu_offline:llama2-7b_chat:security_studies,test,18.262453543022275
22,0.5,0.5454545617103577,0.5330578512396694,0.10975918715650386,mmlu_offline:llama2-7b_chat:sociology,validation,1.431893102824688
201,0.45771142840385437,0.5273631811141968,0.6361188671719186,0.09144824031573626,mmlu_offline:llama2-7b_chat:sociology,test,10.795442725066096
11,0.5454545617103577,0.8181818127632141,0.75,0.24841748042540115,mmlu_offline:llama2-7b_chat:us_foreign_policy,validation,1.000458647031337
100,0.550000011920929,0.6499999761581421,0.6513131313131314,0.09215229272842408,mmlu_offline:llama2-7b_chat:us_foreign_policy,test,5.471846446860582
18,0.2777777910232544,0.4444444477558136,0.4076923076923077,0.2446408106221093,mmlu_offline:llama2-7b_chat:virology,validation,1.3743958021514118
166,0.34337350726127625,0.3855421543121338,0.6038950587477868,0.22476904291704478,mmlu_offline:llama2-7b_chat:virology,test,8.819156731944531
19,0.6315789222717285,0.6842105388641357,0.75,0.14839597438511096,mmlu_offline:llama2-7b_chat:world_religions,validation,1.2374941101297736
171,0.5380116701126099,0.5847952961921692,0.6129609246009906,0.1371530228190952,mmlu_offline:llama2-7b_chat:world_religions,test,8.323980771005154
