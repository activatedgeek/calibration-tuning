N,acc,unc_acc,unc_auroc,unc_ece,dataset,split,ts
11,0.09090909361839294,0.9090909361839294,0.3,0.11835008859634401,mmlu_offline:llama2-13b_chat:abstract_algebra,validation,3.1318372720852494
100,0.3499999940395355,0.6499999761581421,0.6687912087912088,0.10574095010757445,mmlu_offline:llama2-13b_chat:abstract_algebra,test,7.997777557000518
14,0.2857142984867096,0.7857142686843872,0.6749999999999999,0.22404694557189941,mmlu_offline:llama2-13b_chat:anatomy,validation,1.1024695551022887
135,0.42222222685813904,0.6518518328666687,0.7245838956365273,0.056249652085480904,mmlu_offline:llama2-13b_chat:anatomy,test,9.3546126447618
16,0.5,0.625,0.6875,0.17775549367070195,mmlu_offline:llama2-13b_chat:astronomy,validation,1.4883060920983553
152,0.5263158082962036,0.6513158082962036,0.7436631944444444,0.059790470490330136,mmlu_offline:llama2-13b_chat:astronomy,test,12.761142103932798
11,0.5454545617103577,0.6363636255264282,0.5333333333333333,0.1346667625687339,mmlu_offline:llama2-13b_chat:business_ethics,validation,1.7777586439624429
100,0.3199999928474426,0.7599999904632568,0.669577205882353,0.13200654983520507,mmlu_offline:llama2-13b_chat:business_ethics,test,10.387237029150128
29,0.24137930572032928,0.7931034564971924,0.6396103896103896,0.06390906202382053,mmlu_offline:llama2-13b_chat:clinical_knowledge,validation,2.6335876821540296
265,0.2981131970882416,0.7245283126831055,0.6920852048455153,0.023573150949658065,mmlu_offline:llama2-13b_chat:clinical_knowledge,test,21.54691634932533
16,0.25,0.8125,0.45833333333333337,0.3242337368428707,mmlu_offline:llama2-13b_chat:college_biology,validation,1.7908407826907933
144,0.3958333432674408,0.625,0.662431941923775,0.08344759295384088,mmlu_offline:llama2-13b_chat:college_biology,test,12.379801555071026
8,0.0,1.0,,0.28333956003189087,mmlu_offline:llama2-13b_chat:college_chemistry,validation,0.9389766021631658
100,0.17000000178813934,0.8100000023841858,0.830970942593905,0.1539754605293274,mmlu_offline:llama2-13b_chat:college_chemistry,test,10.120842375326902
11,0.27272728085517883,0.9090909361839294,0.9583333333333334,0.24576714905825528,mmlu_offline:llama2-13b_chat:college_computer_science,validation,1.83389160502702
100,0.23999999463558197,0.7799999713897705,0.7077850877192983,0.12904709696769717,mmlu_offline:llama2-13b_chat:college_computer_science,test,15.644175254739821
11,0.0,1.0,,0.2654714313420382,mmlu_offline:llama2-13b_chat:college_mathematics,validation,1.5834158188663423
100,0.09000000357627869,0.8999999761581421,0.7826617826617825,0.1738568502664566,mmlu_offline:llama2-13b_chat:college_mathematics,test,11.61686836881563
22,0.5,0.4545454680919647,0.487603305785124,0.249648010188883,mmlu_offline:llama2-13b_chat:college_medicine,validation,2.4678771952167153
173,0.3988439440727234,0.6878612637519836,0.7663740245261984,0.058210025288466104,mmlu_offline:llama2-13b_chat:college_medicine,test,20.71196950506419
11,0.27272728085517883,0.7272727489471436,0.9166666666666666,0.19595922665162518,mmlu_offline:llama2-13b_chat:college_physics,validation,1.5063659427687526
102,0.1568627506494522,0.8039215803146362,0.5417877906976744,0.03688160929025389,mmlu_offline:llama2-13b_chat:college_physics,test,10.861795200034976
11,0.5454545617103577,0.5454545617103577,0.6,0.1840618740428578,mmlu_offline:llama2-13b_chat:computer_security,validation,1.358424348756671
100,0.550000011920929,0.5299999713897705,0.6565656565656566,0.16014783918857572,mmlu_offline:llama2-13b_chat:computer_security,test,8.175674760714173
26,0.3461538553237915,0.6538461446762085,0.5424836601307189,0.14752631004040057,mmlu_offline:llama2-13b_chat:conceptual_physics,validation,2.245793882291764
235,0.4553191363811493,0.5957446694374084,0.6501533294392523,0.1787866990616981,mmlu_offline:llama2-13b_chat:conceptual_physics,test,17.023181660100818
12,0.25,0.4166666567325592,0.7777777777777778,0.30099795262018847,mmlu_offline:llama2-13b_chat:econometrics,validation,1.731904364656657
114,0.1315789520740509,0.5701754093170166,0.6612794612794612,0.08663158144867211,mmlu_offline:llama2-13b_chat:econometrics,test,14.10600031586364
16,0.25,0.8125,0.6875,0.09577953815460202,mmlu_offline:llama2-13b_chat:electrical_engineering,validation,1.5222924062982202
145,0.24827586114406586,0.7655172348022461,0.7217125382262997,0.021611355913096434,mmlu_offline:llama2-13b_chat:electrical_engineering,test,12.665014699101448
41,0.26829269528388977,0.707317054271698,0.49242424242424243,0.045775633032728975,mmlu_offline:llama2-13b_chat:elementary_mathematics,validation,4.111302034929395
378,0.3174603283405304,0.7063491940498352,0.6664567183462534,0.05473114416082069,mmlu_offline:llama2-13b_chat:elementary_mathematics,test,34.75578446406871
14,0.2142857164144516,0.7857142686843872,0.6515151515151515,0.10497480630874634,mmlu_offline:llama2-13b_chat:formal_logic,validation,1.5954545391723514
126,0.261904776096344,0.738095223903656,0.48012381883349625,0.09636771395092923,mmlu_offline:llama2-13b_chat:formal_logic,test,13.069949139840901
10,0.20000000298023224,0.800000011920929,0.15625,0.1253657579421997,mmlu_offline:llama2-13b_chat:global_facts,validation,1.2734038177877665
100,0.12999999523162842,0.8600000143051147,0.5614500442086648,0.0921885186433792,mmlu_offline:llama2-13b_chat:global_facts,test,8.164448508992791
32,0.3125,0.71875,0.7,0.16338252648711207,mmlu_offline:llama2-13b_chat:high_school_biology,validation,3.041794574819505
310,0.42258065938949585,0.6612903475761414,0.698835771248241,0.04268428760190163,mmlu_offline:llama2-13b_chat:high_school_biology,test,26.364302759990096
22,0.1818181872367859,0.9090909361839294,0.7638888888888888,0.25220412557775324,mmlu_offline:llama2-13b_chat:high_school_chemistry,validation,2.3308339528739452
203,0.1822660118341446,0.807881772518158,0.7156463692608271,0.12219019124073345,mmlu_offline:llama2-13b_chat:high_school_chemistry,test,19.52537698391825
9,0.6666666865348816,0.5555555820465088,0.7222222222222222,0.16147722138298884,mmlu_offline:llama2-13b_chat:high_school_computer_science,validation,1.5258468319661915
100,0.4300000071525574,0.6200000047683716,0.6721746226030192,0.09462547779083254,mmlu_offline:llama2-13b_chat:high_school_computer_science,test,15.25420375727117
18,0.7777777910232544,0.8333333134651184,0.8571428571428572,0.25639495584699845,mmlu_offline:llama2-13b_chat:high_school_european_history,validation,10.006146722007543
165,0.7575757503509521,0.7818182110786438,0.7424,0.07073756095134852,mmlu_offline:llama2-13b_chat:high_school_european_history,test,88.68182360008359
22,0.5,0.5909090638160706,0.5991735537190083,0.2121314298022877,mmlu_offline:llama2-13b_chat:high_school_geography,validation,1.6775804390199482
198,0.39393940567970276,0.7070707082748413,0.7534722222222222,0.034381956163078875,mmlu_offline:llama2-13b_chat:high_school_geography,test,16.04230412375182
21,0.523809552192688,0.761904776096344,0.8636363636363635,0.2118363011450994,mmlu_offline:llama2-13b_chat:high_school_government_and_politics,validation,1.8031010581180453
193,0.6321243643760681,0.6580311059951782,0.7302008773955208,0.06436828442805789,mmlu_offline:llama2-13b_chat:high_school_government_and_politics,test,15.748600772116333
43,0.4883720874786377,0.5348837375640869,0.7207792207792209,0.2254929390064506,mmlu_offline:llama2-13b_chat:high_school_macroeconomics,validation,3.26625616196543
390,0.3692307770252228,0.6487179398536682,0.6588188798554653,0.10305985670823316,mmlu_offline:llama2-13b_chat:high_school_macroeconomics,test,30.927845794707537
29,0.06896551698446274,0.931034505367279,0.6481481481481481,0.16112292223963243,mmlu_offline:llama2-13b_chat:high_school_mathematics,validation,3.4082079650834203
270,0.10000000149011612,0.8962963223457336,0.5638622161255905,0.12840898323942113,mmlu_offline:llama2-13b_chat:high_school_mathematics,test,28.543714153114706
26,0.3076923191547394,0.692307710647583,0.6319444444444444,0.08950423277341402,mmlu_offline:llama2-13b_chat:high_school_microeconomics,validation,2.254012465942651
238,0.36554622650146484,0.7058823704719543,0.7453756565425896,0.049804534982232494,mmlu_offline:llama2-13b_chat:high_school_microeconomics,test,18.56622658483684
17,0.29411765933036804,0.5882353186607361,0.5916666666666667,0.18682798918555762,mmlu_offline:llama2-13b_chat:high_school_physics,validation,2.2558608637191355
151,0.17218543589115143,0.807947039604187,0.6369230769230769,0.08162613617663349,mmlu_offline:llama2-13b_chat:high_school_physics,test,15.311759140808135
60,0.6000000238418579,0.699999988079071,0.7841435185185186,0.06152527530988057,mmlu_offline:llama2-13b_chat:high_school_psychology,validation,5.610766208264977
545,0.5064220428466797,0.6678898930549622,0.7226577231830181,0.019278705120086684,mmlu_offline:llama2-13b_chat:high_school_psychology,test,48.20755952782929
23,0.17391304671764374,0.52173912525177,0.5394736842105263,0.12851839221042138,mmlu_offline:llama2-13b_chat:high_school_statistics,validation,3.477891025133431
216,0.25925925374031067,0.7083333134651184,0.6989397321428572,0.10188859176856503,mmlu_offline:llama2-13b_chat:high_school_statistics,test,28.770402094814926
22,0.7727272510528564,0.7272727489471436,0.5705882352941176,0.09721415693109686,mmlu_offline:llama2-13b_chat:high_school_us_history,validation,9.359262140933424
204,0.6764705777168274,0.720588207244873,0.78645147123408,0.06481108157073748,mmlu_offline:llama2-13b_chat:high_school_us_history,test,85.2159254802391
26,0.6538461446762085,0.5769230723381042,0.5130718954248366,0.28976693290930533,mmlu_offline:llama2-13b_chat:high_school_world_history,validation,8.511838412843645
237,0.6033755540847778,0.6751055121421814,0.6998586519863116,0.09362239173696013,mmlu_offline:llama2-13b_chat:high_school_world_history,test,67.72741832677275
23,0.3913043439388275,0.6521739363670349,0.6111111111111112,0.20888729976571124,mmlu_offline:llama2-13b_chat:human_aging,validation,1.6672128182835877
223,0.3901345431804657,0.6367713212966919,0.665525693035835,0.07530861745500778,mmlu_offline:llama2-13b_chat:human_aging,test,14.42033117217943
12,0.3333333432674408,0.5833333134651184,0.703125,0.23187413811683658,mmlu_offline:llama2-13b_chat:human_sexuality,validation,1.064399405848235
131,0.5114504098892212,0.49618321657180786,0.5784748134328358,0.2520170466590474,mmlu_offline:llama2-13b_chat:human_sexuality,test,9.536056884098798
13,0.6153846383094788,0.38461539149284363,0.5499999999999999,0.2610052503072298,mmlu_offline:llama2-13b_chat:international_law,validation,1.365882711019367
121,0.6280992031097412,0.5619834661483765,0.6105263157894737,0.07940342406596038,mmlu_offline:llama2-13b_chat:international_law,test,10.5093396268785
11,0.1818181872367859,0.9090909361839294,0.9444444444444444,0.22031683813441882,mmlu_offline:llama2-13b_chat:jurisprudence,validation,1.1755968951620162
108,0.3611111044883728,0.6574074029922485,0.6183574879227053,0.020768801923151378,mmlu_offline:llama2-13b_chat:jurisprudence,test,8.053208346944302
18,0.6666666865348816,0.4444444477558136,0.763888888888889,0.19650201333893672,mmlu_offline:llama2-13b_chat:logical_fallacies,validation,1.7681909552775323
163,0.48466256260871887,0.6073619723320007,0.7781796262808922,0.121669373994956,mmlu_offline:llama2-13b_chat:logical_fallacies,test,13.196796259377152
11,0.27272728085517883,0.5454545617103577,0.625,0.2574365518309853,mmlu_offline:llama2-13b_chat:machine_learning,validation,1.3929534489288926
112,0.2946428656578064,0.7232142686843872,0.7008055235903337,0.07724026750241009,mmlu_offline:llama2-13b_chat:machine_learning,test,11.855069110170007
11,0.6363636255264282,0.4545454680919647,0.4107142857142857,0.3199467388066378,mmlu_offline:llama2-13b_chat:management,validation,1.006656948942691
103,0.42718446254730225,0.708737850189209,0.8191448382126347,0.07080758428110659,mmlu_offline:llama2-13b_chat:management,test,6.248026626184583
25,0.23999999463558197,0.4399999976158142,0.6271929824561403,0.1505608248710633,mmlu_offline:llama2-13b_chat:marketing,validation,2.2247999077662826
234,0.44871795177459717,0.6452991366386414,0.7337763012181617,0.04522739643724556,mmlu_offline:llama2-13b_chat:marketing,test,17.24589409213513
11,0.9090909361839294,0.5454545617103577,1.0,0.22693506154147058,mmlu_offline:llama2-13b_chat:medical_genetics,validation,1.0397845152765512
100,0.44999998807907104,0.6899999976158142,0.7143434343434344,0.05716969549655912,mmlu_offline:llama2-13b_chat:medical_genetics,test,6.32527089305222
86,0.5581395626068115,0.6511628031730652,0.7001096491228072,0.07448876319929613,mmlu_offline:llama2-13b_chat:miscellaneous,validation,5.5507875070907176
783,0.618135392665863,0.7139208316802979,0.7566509577379144,0.017294742595190298,mmlu_offline:llama2-13b_chat:miscellaneous,test,49.50439892616123
38,0.42105263471603394,0.5263158082962036,0.6349431818181819,0.17293737750304372,mmlu_offline:llama2-13b_chat:moral_disputes,validation,3.2247346942313015
346,0.43063583970069885,0.5982658863067627,0.6075869587435696,0.07253922616815292,mmlu_offline:llama2-13b_chat:moral_disputes,test,28.365520304068923
100,0.4300000071525574,0.5699999928474426,0.6242350061199511,0.22091705858707428,mmlu_offline:llama2-13b_chat:moral_scenarios,validation,12.512044257018715
895,0.38100558519363403,0.6178770661354065,0.49285653789555034,0.1633682941591273,mmlu_offline:llama2-13b_chat:moral_scenarios,test,106.41543010994792
33,0.3333333432674408,0.6363636255264282,0.6487603305785125,0.07851977781815964,mmlu_offline:llama2-13b_chat:nutrition,validation,3.1924997097812593
306,0.4542483687400818,0.6666666865348816,0.7237754706414509,0.03391256421999215,mmlu_offline:llama2-13b_chat:nutrition,test,27.42622107686475
34,0.3235294222831726,0.7058823704719543,0.658102766798419,0.08914918408674355,mmlu_offline:llama2-13b_chat:philosophy,validation,2.611404594965279
311,0.3633440434932709,0.6559485793113708,0.6058371323858049,0.025001279602480067,mmlu_offline:llama2-13b_chat:philosophy,test,21.068678407929838
35,0.37142857909202576,0.6000000238418579,0.5279720279720279,0.11778136662074498,mmlu_offline:llama2-13b_chat:prehistory,validation,2.951583791989833
324,0.4413580298423767,0.654321014881134,0.672217285476954,0.037584809793366335,mmlu_offline:llama2-13b_chat:prehistory,test,24.541366829071194
31,0.16129031777381897,0.8387096524238586,0.6076923076923078,0.15489903188520865,mmlu_offline:llama2-13b_chat:professional_accounting,validation,4.412728238850832
282,0.1879432648420334,0.7730496525764465,0.6703468731976601,0.07436017533566089,mmlu_offline:llama2-13b_chat:professional_accounting,test,37.3164380597882
170,0.38235294818878174,0.6294117569923401,0.5177289377289378,0.06754643391160403,mmlu_offline:llama2-13b_chat:professional_law,validation,44.777778623159975
1534,0.34810951352119446,0.6525423526763916,0.5485543071161049,0.042487786094399425,mmlu_offline:llama2-13b_chat:professional_law,test,415.5923906364478
31,0.35483869910240173,0.6774193644523621,0.7000000000000001,0.05249652939458049,mmlu_offline:llama2-13b_chat:professional_medicine,validation,6.873642859980464
272,0.2904411852359772,0.6286764740943909,0.6155637174526136,0.020869223948787215,mmlu_offline:llama2-13b_chat:professional_medicine,test,59.618897697888315
69,0.37681159377098083,0.7246376872062683,0.731216457960644,0.08226635058720906,mmlu_offline:llama2-13b_chat:professional_psychology,validation,6.5343735120259225
612,0.3464052379131317,0.6454248428344727,0.6528714622641509,0.02970699517945056,mmlu_offline:llama2-13b_chat:professional_psychology,test,53.46452508820221
12,0.1666666716337204,0.75,0.475,0.09465153018633524,mmlu_offline:llama2-13b_chat:public_relations,validation,1.2373115299269557
110,0.30909091234207153,0.6909090876579285,0.8008900928792568,0.0601270101287148,mmlu_offline:llama2-13b_chat:public_relations,test,8.274795810692012
27,0.5555555820465088,0.7037037014961243,0.8972222222222223,0.078760356814773,mmlu_offline:llama2-13b_chat:security_studies,validation,2.9800945091992617
245,0.6612244844436646,0.5142857432365417,0.665402350141306,0.12006097740056564,mmlu_offline:llama2-13b_chat:security_studies,test,25.355657311156392
22,0.5909090638160706,0.4545454680919647,0.7905982905982906,0.3133942132646388,mmlu_offline:llama2-13b_chat:sociology,validation,1.8710619662888348
201,0.447761207818985,0.6666666865348816,0.7129129129129129,0.0351522363240446,mmlu_offline:llama2-13b_chat:sociology,test,15.0794980796054
11,0.7272727489471436,0.5454545617103577,0.7916666666666667,0.22102417187257248,mmlu_offline:llama2-13b_chat:us_foreign_policy,validation,1.0897916499525309
100,0.5899999737739563,0.6399999856948853,0.7368747416287722,0.04993730306625367,mmlu_offline:llama2-13b_chat:us_foreign_policy,test,7.57341680303216
18,0.5555555820465088,0.5555555820465088,0.8125,0.17403452926211888,mmlu_offline:llama2-13b_chat:virology,validation,1.757682790979743
166,0.34939759969711304,0.6445783376693726,0.6025702426564497,0.0584422515099307,mmlu_offline:llama2-13b_chat:virology,test,12.009712947066873
19,0.6315789222717285,0.6842105388641357,0.7738095238095237,0.11576544611077562,mmlu_offline:llama2-13b_chat:world_religions,validation,1.4217095193453133
171,0.6081871390342712,0.7017543911933899,0.7832233065442019,0.050182803332457025,mmlu_offline:llama2-13b_chat:world_religions,test,9.944332690909505
