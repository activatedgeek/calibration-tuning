N,acc,unc_acc,unc_auroc,unc_ece,dataset,split,ts
11,0.09090909361839294,0.9090909361839294,0.65,0.3247117345983332,mmlu_offline:llama2-7b_chat:abstract_algebra,validation,4.98287053219974
100,0.20999999344348907,0.7900000214576721,0.6238698010849908,0.19617033600807188,mmlu_offline:llama2-7b_chat:abstract_algebra,test,5.764894735999405
14,0.2142857164144516,0.7142857313156128,0.8787878787878787,0.18281398500714982,mmlu_offline:llama2-7b_chat:anatomy,validation,0.9155066679231822
135,0.385185182094574,0.6296296119689941,0.6348470806302132,0.04072477243564746,mmlu_offline:llama2-7b_chat:anatomy,test,6.748270083684474
16,0.5625,0.625,0.6428571428571428,0.07257633656263349,mmlu_offline:llama2-7b_chat:astronomy,validation,1.2000549649819732
152,0.42763158679008484,0.6447368264198303,0.6907161803713529,0.07873649306987461,mmlu_offline:llama2-7b_chat:astronomy,test,9.015845318324864
11,0.4545454680919647,0.4545454680919647,0.5,0.16842978650873355,mmlu_offline:llama2-7b_chat:business_ethics,validation,1.0911605572327971
100,0.33000001311302185,0.49000000953674316,0.6775214834916328,0.0811427927017212,mmlu_offline:llama2-7b_chat:business_ethics,test,7.173268961720169
29,0.17241379618644714,0.7586206793785095,0.85,0.16855956562634175,mmlu_offline:llama2-7b_chat:clinical_knowledge,validation,1.7184833181090653
265,0.2792452871799469,0.7358490824699402,0.6416796377529361,0.1616715482945712,mmlu_offline:llama2-7b_chat:clinical_knowledge,test,14.13548417109996
16,0.375,0.8125,0.7833333333333333,0.22765177860856062,mmlu_offline:llama2-7b_chat:college_biology,validation,1.200052676256746
144,0.3402777910232544,0.6805555820465088,0.6870032223415682,0.10927071339554255,mmlu_offline:llama2-7b_chat:college_biology,test,8.861027546226978
8,0.0,0.875,,0.2738489583134651,mmlu_offline:llama2-7b_chat:college_chemistry,validation,0.7781185461208224
100,0.14000000059604645,0.7900000214576721,0.6166943521594684,0.19881164312362673,mmlu_offline:llama2-7b_chat:college_chemistry,test,6.70391570776701
11,0.27272728085517883,0.6363636255264282,0.5833333333333334,0.09098535234277898,mmlu_offline:llama2-7b_chat:college_computer_science,validation,1.4441946861334145
100,0.17000000178813934,0.8299999833106995,0.5970942593905032,0.23585869073867793,mmlu_offline:llama2-7b_chat:college_computer_science,test,10.387485637329519
11,0.0,0.9090909361839294,,0.29693481055173004,mmlu_offline:llama2-7b_chat:college_mathematics,validation,1.236173768993467
100,0.2199999988079071,0.7799999713897705,0.6197552447552448,0.1905535089969635,mmlu_offline:llama2-7b_chat:college_mathematics,test,7.908084424212575
22,0.3181818127632141,0.6363636255264282,0.7571428571428571,0.13833400065248666,mmlu_offline:llama2-7b_chat:college_medicine,validation,1.6297872988507152
173,0.2947976887226105,0.7109826803207397,0.7412407585985213,0.1002100137616858,mmlu_offline:llama2-7b_chat:college_medicine,test,14.420114657841623
11,0.09090909361839294,0.9090909361839294,0.9500000000000001,0.32175004482269287,mmlu_offline:llama2-7b_chat:college_physics,validation,1.1549237896688282
102,0.0882352963089943,0.9019607901573181,0.5794504181600956,0.30798392552955484,mmlu_offline:llama2-7b_chat:college_physics,test,6.996265052817762
11,0.6363636255264282,0.9090909361839294,1.0,0.3456950892101634,mmlu_offline:llama2-7b_chat:computer_security,validation,1.033954902086407
100,0.5,0.5400000214576721,0.6284,0.038225767016410866,mmlu_offline:llama2-7b_chat:computer_security,test,5.648022410925478
26,0.1538461595773697,0.8461538553237915,0.7443181818181818,0.2657159154231732,mmlu_offline:llama2-7b_chat:conceptual_physics,validation,1.5184759460389614
235,0.3787234127521515,0.6425532102584839,0.5858473141449899,0.05401460439600842,mmlu_offline:llama2-7b_chat:conceptual_physics,test,11.479206652846187
12,0.3333333432674408,0.4166666567325592,0.59375,0.25914604465166724,mmlu_offline:llama2-7b_chat:econometrics,validation,1.2472373810596764
114,0.17543859779834747,0.8245614171028137,0.6101063829787234,0.20999809053906224,mmlu_offline:llama2-7b_chat:econometrics,test,9.194514540955424
16,0.1875,0.8125,0.2948717948717948,0.25673121213912964,mmlu_offline:llama2-7b_chat:electrical_engineering,validation,1.1765657649375498
145,0.2137930989265442,0.7793103456497192,0.48613469156762884,0.14040028555639855,mmlu_offline:llama2-7b_chat:electrical_engineering,test,8.972566777840257
41,0.3658536672592163,0.6097561120986938,0.6102564102564102,0.13432187568850634,mmlu_offline:llama2-7b_chat:elementary_mathematics,validation,2.9775971081107855
378,0.28042328357696533,0.7222222089767456,0.5801886792452831,0.12952827603097947,mmlu_offline:llama2-7b_chat:elementary_mathematics,test,23.747944871895015
14,0.4285714328289032,0.5714285969734192,0.44791666666666663,0.0796366631984711,mmlu_offline:llama2-7b_chat:formal_logic,validation,1.2429286022670567
126,0.2539682686328888,0.761904776096344,0.5465425531914894,0.18330385855266026,mmlu_offline:llama2-7b_chat:formal_logic,test,8.917664679232985
10,0.20000000298023224,0.699999988079071,0.25,0.12486333847045898,mmlu_offline:llama2-7b_chat:global_facts,validation,0.9187058578245342
100,0.07999999821186066,0.8199999928474426,0.5020380434782609,0.2619703340530396,mmlu_offline:llama2-7b_chat:global_facts,test,5.842279899865389
32,0.3125,0.65625,0.46818181818181814,0.12190851010382171,mmlu_offline:llama2-7b_chat:high_school_biology,validation,2.1511819809675217
310,0.3838709592819214,0.6387096643447876,0.6568920762022086,0.07089116880970614,mmlu_offline:llama2-7b_chat:high_school_biology,test,18.78379178000614
22,0.1818181872367859,0.8181818127632141,0.5277777777777778,0.2308728315613487,mmlu_offline:llama2-7b_chat:high_school_chemistry,validation,1.741030631121248
203,0.1428571492433548,0.8325123190879822,0.7499009116131589,0.24523791538670725,mmlu_offline:llama2-7b_chat:high_school_chemistry,test,13.525484269019216
9,0.4444444477558136,0.4444444477558136,0.425,0.12568555275599164,mmlu_offline:llama2-7b_chat:high_school_computer_science,validation,1.1965322671458125
100,0.4099999964237213,0.6800000071525574,0.6165770979743697,0.11253366947174072,mmlu_offline:llama2-7b_chat:high_school_computer_science,test,9.302299344912171
18,0.8333333134651184,0.8333333134651184,0.6444444444444445,0.2274976869424184,mmlu_offline:llama2-7b_chat:high_school_european_history,validation,6.2145671909675
165,0.7090908885002136,0.7575757503509521,0.6761039886039886,0.152641152974331,mmlu_offline:llama2-7b_chat:high_school_european_history,test,53.91147986007854
22,0.4545454680919647,0.5,0.4958333333333333,0.10621569915251296,mmlu_offline:llama2-7b_chat:high_school_geography,validation,1.3632839131169021
198,0.3737373650074005,0.6363636255264282,0.6959459459459459,0.07698696911937057,mmlu_offline:llama2-7b_chat:high_school_geography,test,9.98311208980158
21,0.523809552192688,0.380952388048172,0.48636363636363633,0.17825100251606535,mmlu_offline:llama2-7b_chat:high_school_government_and_politics,validation,1.4518314078450203
193,0.5233160853385925,0.6165803074836731,0.7268080068876452,0.04611351082362039,mmlu_offline:llama2-7b_chat:high_school_government_and_politics,test,10.492540769279003
43,0.39534884691238403,0.604651153087616,0.4864253393665158,0.03173647231833876,mmlu_offline:llama2-7b_chat:high_school_macroeconomics,validation,2.5113141988404095
390,0.2897436022758484,0.7153846025466919,0.6761445321235743,0.1168689087415353,mmlu_offline:llama2-7b_chat:high_school_macroeconomics,test,19.781938650179654
29,0.0,1.0,,0.36249865951209237,mmlu_offline:llama2-7b_chat:high_school_mathematics,validation,2.3032318390905857
270,0.0555555559694767,0.9444444179534912,0.583921568627451,0.3257165635073627,mmlu_offline:llama2-7b_chat:high_school_mathematics,test,19.269961965736
26,0.26923078298568726,0.692307710647583,0.6278195488721805,0.10989422523058376,mmlu_offline:llama2-7b_chat:high_school_microeconomics,validation,1.6503775147721171
238,0.3403361439704895,0.6764705777168274,0.6260124243139106,0.09724612696831968,mmlu_offline:llama2-7b_chat:high_school_microeconomics,test,12.277627596165985
17,0.0,0.3529411852359772,,0.18613443655126233,mmlu_offline:llama2-7b_chat:high_school_physics,validation,1.5341433892026544
151,0.18543046712875366,0.7350993156433105,0.7513066202090593,0.19351689902362446,mmlu_offline:llama2-7b_chat:high_school_physics,test,10.383040466811508
60,0.5666666626930237,0.5666666626930237,0.7081447963800905,0.019127937157948787,mmlu_offline:llama2-7b_chat:high_school_psychology,validation,4.008123300038278
545,0.5009174346923828,0.5577981472015381,0.6057490303813833,0.011927964271755381,mmlu_offline:llama2-7b_chat:high_school_psychology,test,33.73334980616346
23,0.17391304671764374,0.739130437374115,0.6644736842105263,0.1802360570949057,mmlu_offline:llama2-7b_chat:high_school_statistics,validation,2.2191105759702623
216,0.19907407462596893,0.7453703880310059,0.631603710176099,0.19319571140739653,mmlu_offline:llama2-7b_chat:high_school_statistics,test,18.362659377977252
22,0.6363636255264282,0.6818181872367859,0.7098214285714286,0.11386597969315272,mmlu_offline:llama2-7b_chat:high_school_us_history,validation,6.068661894649267
204,0.5784313678741455,0.6911764740943909,0.7374852187623177,0.11211150884628296,mmlu_offline:llama2-7b_chat:high_school_us_history,test,52.62189190182835
26,0.7307692170143127,0.7692307829856873,0.5827067669172933,0.20022136431473953,mmlu_offline:llama2-7b_chat:high_school_world_history,validation,5.392639822792262
237,0.552742600440979,0.5822784900665283,0.5923952182053867,0.019360574740397797,mmlu_offline:llama2-7b_chat:high_school_world_history,test,42.60100340330973
23,0.30434781312942505,0.6521739363670349,0.5625,0.09944465626841006,mmlu_offline:llama2-7b_chat:human_aging,validation,1.2918389630503953
223,0.33183857798576355,0.6860986351966858,0.6779430437148558,0.1328135899898717,mmlu_offline:llama2-7b_chat:human_aging,test,11.306883845012635
12,0.25,0.75,0.6296296296296297,0.14182157317797342,mmlu_offline:llama2-7b_chat:human_sexuality,validation,0.8717966163530946
131,0.4198473393917084,0.6183205842971802,0.6141148325358852,0.04922071076531443,mmlu_offline:llama2-7b_chat:human_sexuality,test,7.107210433110595
13,0.4615384638309479,0.5384615659713745,0.6904761904761905,0.023328611483940748,mmlu_offline:llama2-7b_chat:international_law,validation,1.1120290230028331
121,0.6115702390670776,0.6776859760284424,0.6112708453133986,0.16413830430054466,mmlu_offline:llama2-7b_chat:international_law,test,7.6492867190390825
11,0.5454545617103577,0.6363636255264282,0.7333333333333334,0.24872579899701203,mmlu_offline:llama2-7b_chat:jurisprudence,validation,0.9534290949814022
108,0.5185185074806213,0.5092592835426331,0.5896291208791209,0.08031896584563787,mmlu_offline:llama2-7b_chat:jurisprudence,test,5.91475121723488
18,0.4444444477558136,0.5555555820465088,0.66875,0.03430030081007215,mmlu_offline:llama2-7b_chat:logical_fallacies,validation,1.3740421319380403
163,0.42944785952568054,0.5705521702766418,0.6220430107526882,0.01668237835351673,mmlu_offline:llama2-7b_chat:logical_fallacies,test,9.606488245073706
11,0.3636363744735718,0.7272727489471436,0.6964285714285714,0.15792264721610327,mmlu_offline:llama2-7b_chat:machine_learning,validation,1.1271134833805263
112,0.1964285671710968,0.8214285969734192,0.6762626262626263,0.24038403215152881,mmlu_offline:llama2-7b_chat:machine_learning,test,8.50807058205828
11,0.27272728085517883,0.7272727489471436,0.8125,0.17582094669342035,mmlu_offline:llama2-7b_chat:management,validation,0.850599785335362
103,0.3689320385456085,0.6796116232872009,0.71417004048583,0.1296227909986255,mmlu_offline:llama2-7b_chat:management,test,5.012740669772029
25,0.20000000298023224,0.4399999976158142,0.695,0.1321293830871582,mmlu_offline:llama2-7b_chat:marketing,validation,1.7700445470400155
234,0.39743590354919434,0.5811966061592102,0.6108823305117059,0.03927753165236907,mmlu_offline:llama2-7b_chat:marketing,test,12.769790838006884
11,0.7272727489471436,0.4545454680919647,0.5416666666666667,0.07792926376516168,mmlu_offline:llama2-7b_chat:medical_genetics,validation,0.856168313883245
100,0.4099999964237213,0.6600000262260437,0.6471682513435305,0.10617714047431946,mmlu_offline:llama2-7b_chat:medical_genetics,test,5.077797280158848
86,0.41860464215278625,0.5813953280448914,0.5705555555555555,0.06860371800356133,mmlu_offline:llama2-7b_chat:miscellaneous,validation,4.4466932360082865
783,0.4533844292163849,0.6079182624816895,0.6683921284717652,0.05461718699636742,mmlu_offline:llama2-7b_chat:miscellaneous,test,38.79457942210138
38,0.4736842215061188,0.5263158082962036,0.5930555555555556,0.029409069763986675,mmlu_offline:llama2-7b_chat:moral_disputes,validation,2.4828311377204955
346,0.41040462255477905,0.5953757166862488,0.5257870753935376,0.0417068090742034,mmlu_offline:llama2-7b_chat:moral_disputes,test,20.44174480671063
100,0.5099999904632568,0.49000000953674316,0.5816326530612245,0.10045937359333039,mmlu_offline:llama2-7b_chat:moral_scenarios,validation,8.460293923970312
895,0.46145251393318176,0.5385474562644958,0.502486612480283,0.048663868984030634,mmlu_offline:llama2-7b_chat:moral_scenarios,test,72.89166282117367
33,0.3636363744735718,0.6666666865348816,0.6785714285714286,0.06288129633123228,mmlu_offline:llama2-7b_chat:nutrition,validation,2.484736126847565
306,0.4084967374801636,0.601307213306427,0.6620552486187845,0.04829125116074008,mmlu_offline:llama2-7b_chat:nutrition,test,19.69810679508373
34,0.3529411852359772,0.5,0.5965909090909091,0.13641116198371436,mmlu_offline:llama2-7b_chat:philosophy,validation,2.087224392220378
311,0.3376205861568451,0.5144694447517395,0.6262829403606102,0.027436855904925655,mmlu_offline:llama2-7b_chat:philosophy,test,15.764458840712905
35,0.2857142984867096,0.5714285969734192,0.83,0.1404886620385306,mmlu_offline:llama2-7b_chat:prehistory,validation,2.2917394461110234
324,0.34259259700775146,0.5308641791343689,0.6239267436450535,0.0402166731195685,mmlu_offline:llama2-7b_chat:prehistory,test,18.652407256886363
31,0.06451612710952759,0.7096773982048035,0.5344827586206897,0.15095155469832883,mmlu_offline:llama2-7b_chat:professional_accounting,validation,2.9854058558121324
282,0.1560283750295639,0.73758864402771,0.6232811306340718,0.18148504546348085,mmlu_offline:llama2-7b_chat:professional_accounting,test,25.5485926810652
170,0.30588236451148987,0.6000000238418579,0.48712516297262065,0.04104012776823602,mmlu_offline:llama2-7b_chat:professional_law,validation,29.71788496291265
1534,0.3200782239437103,0.6342894434928894,0.5300353632889616,0.07994084330239226,mmlu_offline:llama2-7b_chat:professional_law,test,269.49980571027845
31,0.25806450843811035,0.6451612710952759,0.6630434782608696,0.11648656860474615,mmlu_offline:llama2-7b_chat:professional_medicine,validation,4.57898113084957
272,0.23529411852359772,0.5992646813392639,0.5125075120192308,0.07190300831023387,mmlu_offline:llama2-7b_chat:professional_medicine,test,38.769224317744374
69,0.37681159377098083,0.6521739363670349,0.6189624329159213,0.08181859876798545,mmlu_offline:llama2-7b_chat:professional_psychology,validation,4.667499800212681
612,0.32189542055130005,0.6683006286621094,0.5938902819399425,0.09222543015589123,mmlu_offline:llama2-7b_chat:professional_psychology,test,38.19234990281984
12,0.3333333432674408,0.6666666865348816,0.53125,0.317711224158605,mmlu_offline:llama2-7b_chat:public_relations,validation,1.0220942911691964
110,0.3272727131843567,0.5181818008422852,0.640015015015015,0.10058203014460479,mmlu_offline:llama2-7b_chat:public_relations,test,6.339859617874026
27,0.7407407164573669,0.7777777910232544,0.8607142857142858,0.22317880392074588,mmlu_offline:llama2-7b_chat:security_studies,validation,2.250408727210015
245,0.6816326379776001,0.640816330909729,0.715453707968678,0.08687443879185894,mmlu_offline:llama2-7b_chat:security_studies,test,18.518238552846014
22,0.5,0.4545454680919647,0.5950413223140496,0.10839192975651137,mmlu_offline:llama2-7b_chat:sociology,validation,1.420710044912994
201,0.45771142840385437,0.641791045665741,0.7082169924212206,0.0791451136271159,mmlu_offline:llama2-7b_chat:sociology,test,10.826868624426425
11,0.5454545617103577,0.9090909361839294,0.9333333333333333,0.35820994052019983,mmlu_offline:llama2-7b_chat:us_foreign_policy,validation,0.879817470908165
100,0.550000011920929,0.6800000071525574,0.6937373737373738,0.12422404348850245,mmlu_offline:llama2-7b_chat:us_foreign_policy,test,5.563066603150219
18,0.2777777910232544,0.6111111044883728,0.5538461538461539,0.09711190064748128,mmlu_offline:llama2-7b_chat:virology,validation,1.4024913026951253
166,0.34337350726127625,0.5361445546150208,0.6385803959439884,0.0307499080537313,mmlu_offline:llama2-7b_chat:virology,test,8.812677655834705
19,0.6315789222717285,0.6315789222717285,0.7380952380952381,0.08570876560713117,mmlu_offline:llama2-7b_chat:world_religions,validation,1.280723956413567
171,0.5380116701126099,0.707602322101593,0.7160842047330764,0.16598453159220736,mmlu_offline:llama2-7b_chat:world_religions,test,8.201984959188849
