N,acc,unc_acc,unc_auroc,unc_ece,dataset,split,ts
11,0.09090909361839294,0.5454545617103577,0.9,0.054687499999999986,mmlu:abstract_algebra,validation,2.734303864184767
100,0.20999999344348907,0.4699999988079071,0.6633514165159735,0.14171874046325683,mmlu:abstract_algebra,test,1.4514228668995202
14,0.2142857164144516,0.2142857164144516,0.7272727272727273,0.5044642771993365,mmlu:anatomy,validation,0.3553994679823518
135,0.385185182094574,0.385185182094574,0.6553521779425395,0.33055554584220603,mmlu:anatomy,test,1.8399190148338675
16,0.5625,0.5625,0.5714285714285714,0.2641601450741291,mmlu:astronomy,validation,0.31954669998958707
152,0.42763158679008484,0.42763158679008484,0.561450044208665,0.4173776723052326,mmlu:astronomy,test,2.0395035040564835
11,0.4545454680919647,0.4545454680919647,0.4666666666666667,0.35830966992811725,mmlu:business_ethics,validation,0.27725600195117295
100,0.33000001311302185,0.33000001311302185,0.6117141564902759,0.48175779938697816,mmlu:business_ethics,test,1.4408696801401675
29,0.17241379618644714,0.17241379618644714,0.8,0.46120690682838705,mmlu:clinical_knowledge,validation,0.4788758009672165
265,0.2792452871799469,0.2792452871799469,0.6939649073156927,0.3158166242095659,mmlu:clinical_knowledge,test,3.687066935002804
16,0.375,0.375,0.31666666666666665,0.1936035081744194,mmlu:college_biology,validation,0.3409420298412442
144,0.3402777910232544,0.3680555522441864,0.6117078410311493,0.23578559690051606,mmlu:college_biology,test,1.9208734950516373
8,0.0,0.0,,0.6640624850988388,mmlu:college_chemistry,validation,0.199481159215793
100,0.14000000059604645,0.14000000059604645,0.657392026578073,0.5332031315565109,mmlu:college_chemistry,test,1.4184329921845347
11,0.27272728085517883,0.6363636255264282,0.6666666666666666,0.04296875,mmlu:college_computer_science,validation,0.3591123051010072
100,0.17000000178813934,0.7200000286102295,0.6909992912827783,0.14164061546325685,mmlu:college_computer_science,test,1.4438660198356956
11,0.0,1.0,,0.09730112552642822,mmlu:college_mathematics,validation,0.3192644459195435
100,0.2199999988079071,0.7799999713897705,0.6223776223776223,0.09410156190395355,mmlu:college_mathematics,test,1.4245932230260223
22,0.3181818127632141,0.3181818127632141,0.7904761904761904,0.3625710200179707,mmlu:college_medicine,validation,0.47593556391075253
173,0.2947976887226105,0.2947976887226105,0.6892478302796528,0.3768515200973246,mmlu:college_medicine,test,2.275527840014547
11,0.09090909361839294,0.09090909361839294,0.8,0.5177556872367859,mmlu:college_physics,validation,0.34334662510082126
102,0.0882352963089943,0.09803921729326248,0.7138590203106332,0.5022977695745581,mmlu:college_physics,test,1.4042685951571912
11,0.6363636255264282,0.5454545617103577,0.7321428571428571,0.14417614720084448,mmlu:computer_security,validation,0.3589969149325043
100,0.5,0.5099999904632568,0.5926,0.11949218809604646,mmlu:computer_security,test,1.4114264249801636
26,0.1538461595773697,0.3076923191547394,0.46590909090909094,0.23783051050626314,mmlu:conceptual_physics,validation,0.5595128419809043
235,0.3787234127521515,0.4553191363811493,0.5657226412190242,0.08452462236931986,mmlu:conceptual_physics,test,3.1157389448489994
12,0.3333333432674408,0.3333333432674408,0.65625,0.2805989583333333,mmlu:econometrics,validation,0.3433384250383824
114,0.17543859779834747,0.17543859779834747,0.7196808510638298,0.43674615391513755,mmlu:econometrics,test,1.6233249001670629
16,0.1875,0.25,0.43589743589743585,0.3400878868997097,mmlu:electrical_engineering,validation,0.3083107329439372
145,0.2137930989265442,0.3379310369491577,0.579513299377476,0.2514278136450669,mmlu:electrical_engineering,test,2.03562462516129
41,0.3658536672592163,0.6341463327407837,0.6448717948717949,0.16101370061316145,mmlu:elementary_mathematics,validation,0.7636058509815484
378,0.28042328357696533,0.7195767164230347,0.547898168701443,0.04613097477211523,mmlu:elementary_mathematics,test,4.928080135956407
14,0.4285714328289032,0.5,0.75,0.025669634342193604,mmlu:formal_logic,validation,0.35383500601164997
126,0.2539682686328888,0.523809552192688,0.7433510638297873,0.001457066763015047,mmlu:formal_logic,test,1.6547788369935006
10,0.20000000298023224,0.20000000298023224,0.71875,0.532421886920929,mmlu:global_facts,validation,0.3430448640137911
100,0.07999999821186066,0.07999999821186066,0.4830163043478261,0.6628125059604646,mmlu:global_facts,test,1.4190540870185941
32,0.3125,0.3125,0.525,0.3984375223517418,mmlu:high_school_biology,validation,0.5671080308966339
310,0.3838709592819214,0.3838709592819214,0.5910070834616569,0.3627016242473356,mmlu:high_school_biology,test,4.002989703789353
22,0.1818181872367859,0.1818181872367859,0.7916666666666666,0.6047585227272727,mmlu:high_school_chemistry,validation,0.4797943611629307
203,0.1428571492433548,0.1428571492433548,0.6288149028933808,0.6466287016281353,mmlu:high_school_chemistry,test,2.714764356147498
9,0.4444444477558136,0.6666666865348816,0.44999999999999996,0.3563368055555556,mmlu:high_school_computer_science,validation,0.3199013189878315
100,0.4099999964237213,0.5,0.5260438197602315,0.12546875357627868,mmlu:high_school_computer_science,test,1.4043590859510005
18,0.8333333134651184,0.7222222089767456,0.7,0.15321181217829388,mmlu:high_school_european_history,validation,0.45217966800555587
165,0.7090908885002136,0.6424242258071899,0.5845797720797721,0.021685616897814168,mmlu:high_school_european_history,test,2.227852163137868
22,0.4545454680919647,0.4545454680919647,0.6208333333333333,0.2984730167822404,mmlu:high_school_geography,validation,0.4573991468641907
198,0.3737373650074005,0.3737373650074005,0.5543265039232782,0.33408300563542537,mmlu:high_school_geography,test,2.6552913170307875
21,0.523809552192688,0.523809552192688,0.8090909090909091,0.2739955130077544,mmlu:high_school_government_and_politics,validation,0.397077094996348
193,0.5233160853385925,0.5233160853385925,0.5369134739560912,0.2772830397353889,mmlu:high_school_government_and_politics,test,2.6057830590289086
43,0.39534884691238403,0.39534884691238403,0.5904977375565611,0.23809956256733386,mmlu:high_school_macroeconomics,validation,0.7338549240957946
390,0.2897436022758484,0.2897436022758484,0.5823935337529152,0.3342648247877757,mmlu:high_school_macroeconomics,test,4.975729328114539
29,0.0,1.0,,0.14870689860705674,mmlu:high_school_mathematics,validation,0.5492416070774198
270,0.0555555559694767,0.9444444179534912,0.6279738562091504,0.09633970702136006,mmlu:high_school_mathematics,test,3.5143754680175334
26,0.26923078298568726,0.26923078298568726,0.5451127819548872,0.37995792581484866,mmlu:high_school_microeconomics,validation,0.5762876011431217
238,0.3403361439704895,0.3403361439704895,0.5776912793897931,0.3143710777038286,mmlu:high_school_microeconomics,test,3.0690695371013135
17,0.0,0.0,,0.5900735294117647,mmlu:high_school_physics,validation,0.4453959839884192
151,0.18543046712875366,0.2715231776237488,0.6800232288037166,0.3165097339263815,mmlu:high_school_physics,test,2.044027548050508
60,0.5666666626930237,0.5666666626930237,0.6351809954751131,0.10559894045193993,mmlu:high_school_psychology,validation,1.016783569008112
545,0.5009174346923828,0.5064220428466797,0.5326909609997845,0.08819525241851807,mmlu:high_school_psychology,test,6.974718431942165
23,0.17391304671764374,0.782608687877655,0.5855263157894738,0.13552990426187933,mmlu:high_school_statistics,validation,0.39559094700962305
216,0.19907407462596893,0.7685185074806213,0.5587444548998521,0.12188947062801431,mmlu:high_school_statistics,test,2.834073610138148
22,0.6363636255264282,0.6363636255264282,0.7410714285714286,0.12588778680021118,mmlu:high_school_us_history,validation,0.5090239241253585
204,0.5784313678741455,0.5784313678741455,0.560750886874261,0.13779105655118534,mmlu:high_school_us_history,test,2.703591034980491
26,0.7307692170143127,0.6538461446762085,0.5714285714285714,0.07376804489355823,mmlu:high_school_world_history,validation,0.5592470848932862
237,0.552742600440979,0.5274261832237244,0.5309664410197321,0.06963672793867218,mmlu:high_school_world_history,test,3.129303145222366
23,0.30434781312942505,0.30434781312942505,0.5089285714285714,0.3850203622942385,mmlu:human_aging,validation,0.44932446884922683
223,0.33183857798576355,0.33183857798576355,0.6164066751315074,0.3522106048237582,mmlu:human_aging,test,2.9095146500039846
12,0.25,0.25,0.7592592592592593,0.5335286458333334,mmlu:human_sexuality,validation,0.35142482491210103
131,0.4198473393917084,0.4198473393917084,0.5013157894736842,0.3599117552961102,mmlu:human_sexuality,test,1.8145145741291344
13,0.4615384638309479,0.4615384638309479,0.32142857142857145,0.39002405680142915,mmlu:international_law,validation,0.33545431587845087
121,0.6115702390670776,0.6115702390670776,0.6441920644048303,0.24131586531962246,mmlu:international_law,test,1.701321620028466
11,0.5454545617103577,0.5454545617103577,0.6333333333333333,0.15624998916279187,mmlu:jurisprudence,validation,0.32860917691141367
108,0.5185185074806213,0.5185185074806213,0.5137362637362637,0.1898871594005161,mmlu:jurisprudence,test,1.4982564558740705
18,0.4444444477558136,0.4444444477558136,0.3875,0.3018663360012902,mmlu:logical_fallacies,validation,0.4236306508537382
163,0.42944785952568054,0.42944785952568054,0.5513056835637481,0.31115797216906865,mmlu:logical_fallacies,test,2.2363203139975667
11,0.3636363744735718,0.5454545617103577,0.625,1.6255812251841917e-08,mmlu:machine_learning,validation,0.3206869140267372
112,0.1964285671710968,0.2410714328289032,0.47297979797979794,0.30154853580253466,mmlu:machine_learning,test,1.5086488949600607
11,0.27272728085517883,0.27272728085517883,0.9583333333333333,0.44460228356448084,mmlu:management,validation,0.31049216096289456
103,0.3689320385456085,0.3689320385456085,0.677327935222672,0.3569098988783012,mmlu:management,test,1.4573810058645904
25,0.20000000298023224,0.20000000298023224,0.47500000000000003,0.5449999833106994,mmlu:marketing,validation,0.5584403458051383
234,0.39743590354919434,0.39743590354919434,0.6129794860062534,0.3522469452303698,mmlu:marketing,test,3.138271701987833
11,0.7272727489471436,0.7272727489471436,0.625,0.14133524352853952,mmlu:medical_genetics,validation,0.3226661919616163
100,0.4099999964237213,0.4099999964237213,0.5731707317073171,0.28632812440395355,mmlu:medical_genetics,test,1.4290615490172058
86,0.41860464215278625,0.41860464215278625,0.5325000000000001,0.1872274542963782,mmlu:miscellaneous,validation,1.2811534209176898
783,0.4533844292163849,0.4636015295982361,0.600691062261419,0.1480783276631001,mmlu:miscellaneous,test,9.759902809048072
38,0.4736842215061188,0.4736842215061188,0.7013888888888888,0.28577302161015966,mmlu:moral_disputes,validation,0.653449113946408
346,0.41040462255477905,0.41040462255477905,0.5363504556752278,0.31769326415365146,mmlu:moral_disputes,test,4.461820536991581
100,0.5099999904632568,0.5099999904632568,0.5580232092837134,0.07039060235023498,mmlu:moral_scenarios,validation,1.436944301938638
895,0.46145251393318176,0.46145251393318176,0.5059502878442326,0.12236817688915316,mmlu:moral_scenarios,test,11.302734005032107
33,0.3636363744735718,0.3636363744735718,0.7202380952380951,0.47076229615644977,mmlu:nutrition,validation,0.6420489919837564
306,0.4084967374801636,0.4084967374801636,0.5716906077348067,0.4197687035292582,mmlu:nutrition,test,4.016597723122686
34,0.3529411852359772,0.3529411852359772,0.49810606060606066,0.3893612377783831,mmlu:philosophy,validation,0.6686626540031284
311,0.3376205861568451,0.3376205861568451,0.623462783171521,0.40328577438734736,mmlu:philosophy,test,4.006036779144779
35,0.2857142984867096,0.2857142984867096,0.522,0.4184151666504996,mmlu:prehistory,validation,0.6524411849677563
324,0.34259259700775146,0.34259259700775146,0.591528147866176,0.3454499373465409,mmlu:prehistory,test,4.1971132720354944
31,0.06451612710952759,0.9032257795333862,0.24137931034482757,0.2133316493803455,mmlu:professional_accounting,validation,0.5478000889997929
282,0.1560283750295639,0.847517728805542,0.5869938884644766,0.16219247380892435,mmlu:professional_accounting,test,3.7922136620618403
170,0.30588236451148987,0.5058823823928833,0.5262385919165581,0.09558825387674223,mmlu:professional_law,validation,2.427128863055259
1534,0.3200782239437103,0.5195567011833191,0.523079867138698,0.07737380825711664,mmlu:professional_law,test,19.37143514305353
31,0.25806450843811035,0.25806450843811035,0.532608695652174,0.3266129109167284,mmlu:professional_medicine,validation,0.5581641830503941
272,0.23529411852359772,0.25,0.5090895432692307,0.33592315785148563,mmlu:professional_medicine,test,3.609714609803632
69,0.37681159377098083,0.4637681245803833,0.49329159212880147,0.07591713252274884,mmlu:professional_psychology,validation,1.0028937100432813
612,0.32189542055130005,0.4575163424015045,0.5258883248730964,0.08199297690313626,mmlu:professional_psychology,test,7.851049707038328
12,0.3333333432674408,0.25,0.375,0.4404296875,mmlu:public_relations,validation,0.3562900829128921
110,0.3272727131843567,0.33636364340782166,0.5512387387387387,0.3694957348433408,mmlu:public_relations,test,1.5450845169834793
27,0.7407407164573669,0.7407407164573669,0.75,0.06119792770456386,mmlu:security_studies,validation,0.5740875450428575
245,0.6816326379776001,0.6816326379776001,0.6846691232918778,0.11289859061338464,mmlu:security_studies,test,3.2537892169784755
22,0.5,0.5,0.6900826446280992,0.17684661258350723,mmlu:sociology,validation,0.41262143407948315
201,0.45771142840385437,0.45771142840385437,0.5633725568408456,0.21824470533067317,mmlu:sociology,test,2.7587059459183365
11,0.5454545617103577,0.5454545617103577,0.6666666666666666,0.26633522185412317,mmlu:us_foreign_policy,validation,0.34823694499209523
100,0.550000011920929,0.550000011920929,0.622020202020202,0.2655078011751175,mmlu:us_foreign_policy,test,1.466973596950993
18,0.2777777910232544,0.2777777910232544,0.7307692307692308,0.42903647157881,mmlu:virology,validation,0.3855844959616661
166,0.34337350726127625,0.34337350726127625,0.6170127152744246,0.37961219161389825,mmlu:virology,test,2.2231837431900203
19,0.6315789222717285,0.6315789222717285,0.5654761904761906,0.05098684524234974,mmlu:world_religions,validation,0.47423034883104265
171,0.5380116701126099,0.5497075915336609,0.5776692350027518,0.05514435670529194,mmlu:world_religions,test,2.3058012621477246
