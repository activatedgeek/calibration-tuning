N,acc,unc_acc,unc_auroc,unc_ece,dataset,split,ts
11,0.09090909361839294,0.9090909361839294,0.75,0.14914771643551913,mmlu_offline:llama2-7b_chat:abstract_algebra,validation,3.7350645361002535
100,0.20999999344348907,0.7900000214576721,0.5626883664858349,0.13710937499999998,mmlu_offline:llama2-7b_chat:abstract_algebra,test,3.2463953299447894
14,0.2142857164144516,0.5714285969734192,0.696969696969697,0.2580915135996682,mmlu_offline:llama2-7b_chat:anatomy,validation,0.5406642397865653
135,0.385185182094574,0.43703705072402954,0.4258572752548656,0.29427082891817447,mmlu_offline:llama2-7b_chat:anatomy,test,3.6837624690961093
16,0.5625,0.625,0.7142857142857143,0.11767577752470972,mmlu_offline:llama2-7b_chat:astronomy,validation,0.770659337984398
152,0.42763158679008484,0.5065789222717285,0.5541114058355437,0.17919923090621045,mmlu_offline:llama2-7b_chat:astronomy,test,5.581738387001678
11,0.4545454680919647,0.6363636255264282,0.7833333333333334,0.29900567639957776,mmlu_offline:llama2-7b_chat:business_ethics,validation,0.6651890038046986
100,0.33000001311302185,0.6800000071525574,0.6173677069199457,0.062421869635581974,mmlu_offline:llama2-7b_chat:business_ethics,test,4.674170295940712
29,0.17241379618644714,0.17241379618644714,0.42500000000000004,0.5787984872686452,mmlu_offline:llama2-7b_chat:clinical_knowledge,validation,0.9770122759509832
265,0.2792452871799469,0.3849056661128998,0.6107612848450545,0.3892541212855645,mmlu_offline:llama2-7b_chat:clinical_knowledge,test,7.922789796022698
16,0.375,0.3125,0.19166666666666665,0.4987792931497097,mmlu_offline:llama2-7b_chat:college_biology,validation,0.728404977126047
144,0.3402777910232544,0.5208333134651184,0.5132116004296455,0.3073459122743871,mmlu_offline:llama2-7b_chat:college_biology,test,5.658203358063474
8,0.0,0.5,,0.49609375,mmlu_offline:llama2-7b_chat:college_chemistry,validation,0.5733845881186426
100,0.14000000059604645,0.8100000023841858,0.7147009966777408,0.06890625715255737,mmlu_offline:llama2-7b_chat:college_chemistry,test,4.456984652904794
11,0.27272728085517883,0.5454545617103577,0.5416666666666666,0.18750001083720813,mmlu_offline:llama2-7b_chat:college_computer_science,validation,0.9874256199691445
100,0.17000000178813934,0.7400000095367432,0.7367115520907158,0.0988671863079071,mmlu_offline:llama2-7b_chat:college_computer_science,test,8.023859831970185
11,0.0,0.7272727489471436,,0.20134943181818182,mmlu_offline:llama2-7b_chat:college_mathematics,validation,0.7902102300431579
100,0.2199999988079071,0.6800000071525574,0.5337995337995338,0.17460937798023227,mmlu_offline:llama2-7b_chat:college_mathematics,test,5.630193504970521
22,0.3181818127632141,0.5454545617103577,0.6666666666666666,0.19531250812790615,mmlu_offline:llama2-7b_chat:college_medicine,validation,1.038158357143402
173,0.2947976887226105,0.47398844361305237,0.559386049501768,0.29951680188923224,mmlu_offline:llama2-7b_chat:college_medicine,test,10.270383092109114
11,0.09090909361839294,0.8181818127632141,0.7,0.11505680734461005,mmlu_offline:llama2-7b_chat:college_physics,validation,0.6908185000065714
102,0.0882352963089943,0.843137264251709,0.6768219832735961,0.03994331990971286,mmlu_offline:llama2-7b_chat:college_physics,test,4.846779283834621
11,0.6363636255264282,0.4545454680919647,0.4642857142857143,0.35582386905496766,mmlu_offline:llama2-7b_chat:computer_security,validation,0.6228533978573978
100,0.5,0.5099999904632568,0.5738,0.19207031369209293,mmlu_offline:llama2-7b_chat:computer_security,test,3.275684859137982
26,0.1538461595773697,0.4615384638309479,0.5795454545454546,0.3984375045849727,mmlu_offline:llama2-7b_chat:conceptual_physics,validation,0.8921485668979585
235,0.3787234127521515,0.4893617033958435,0.5266276743112205,0.3584275184793675,mmlu_offline:llama2-7b_chat:conceptual_physics,test,6.371951946988702
12,0.3333333432674408,0.75,0.828125,0.1184895783662796,mmlu_offline:llama2-7b_chat:econometrics,validation,0.829135028179735
114,0.17543859779834747,0.640350878238678,0.5039893617021276,0.13575932331252516,mmlu_offline:llama2-7b_chat:econometrics,test,6.555962220067158
16,0.1875,0.3125,0.46153846153846156,0.4226074256002903,mmlu_offline:llama2-7b_chat:electrical_engineering,validation,0.730352388927713
145,0.2137930989265442,0.48275861144065857,0.5423033389926429,0.25255925737578294,mmlu_offline:llama2-7b_chat:electrical_engineering,test,5.595849776873365
41,0.3658536672592163,0.6585366129875183,0.6551282051282051,0.22189404179410235,mmlu_offline:llama2-7b_chat:elementary_mathematics,validation,1.8779566958546638
378,0.28042328357696533,0.6957672238349915,0.5024105160932297,0.2109168203419478,mmlu_offline:llama2-7b_chat:elementary_mathematics,test,15.350697664078325
14,0.4285714328289032,0.6428571343421936,0.7604166666666666,0.29352679422923494,mmlu_offline:llama2-7b_chat:formal_logic,validation,0.783480775076896
126,0.2539682686328888,0.6269841194152832,0.573470744680851,0.09446303617386592,mmlu_offline:llama2-7b_chat:formal_logic,test,6.024151012999937
10,0.20000000298023224,0.699999988079071,0.625,0.19374999403953555,mmlu_offline:llama2-7b_chat:global_facts,validation,0.5176935140043497
100,0.07999999821186066,0.6700000166893005,0.720788043478261,0.10777341961860655,mmlu_offline:llama2-7b_chat:global_facts,test,3.5399926668033004
32,0.3125,0.59375,0.5113636363636362,0.1695556566119194,mmlu_offline:llama2-7b_chat:high_school_biology,validation,1.3426729820203036
310,0.3838709592819214,0.5548387169837952,0.5999384046812443,0.17255544970112463,mmlu_offline:llama2-7b_chat:high_school_biology,test,12.01619184203446
22,0.1818181872367859,0.7272727489471436,0.548611111111111,0.11505682901902632,mmlu_offline:llama2-7b_chat:high_school_chemistry,validation,1.0785126788541675
203,0.1428571492433548,0.7586206793785095,0.6735037653586999,0.04137161887925246,mmlu_offline:llama2-7b_chat:high_school_chemistry,test,8.813321063062176
9,0.4444444477558136,0.4444444477558136,0.5249999999999999,0.44097222884496057,mmlu_offline:llama2-7b_chat:high_school_computer_science,validation,0.7972112149000168
100,0.4099999964237213,0.5099999904632568,0.5167424555601489,0.25832031011581424,mmlu_offline:llama2-7b_chat:high_school_computer_science,test,7.160717620048672
18,0.8333333134651184,0.7222222089767456,0.7111111111111112,0.1306423544883728,mmlu_offline:llama2-7b_chat:high_school_european_history,validation,5.702945094788447
165,0.7090908885002136,0.6727272868156433,0.5915242165242166,0.1122396013953469,mmlu_offline:llama2-7b_chat:high_school_european_history,test,52.080934608122334
22,0.4545454680919647,0.5454545617103577,0.4708333333333333,0.3061079491268505,mmlu_offline:llama2-7b_chat:high_school_geography,validation,0.7692263070493937
198,0.3737373650074005,0.5202020406723022,0.5513295553618135,0.28446574463988794,mmlu_offline:llama2-7b_chat:high_school_geography,test,5.712072354042903
21,0.523809552192688,0.7142857313156128,0.7545454545454546,0.13839285714285715,mmlu_offline:llama2-7b_chat:high_school_government_and_politics,validation,0.8418984687887132
193,0.5233160853385925,0.46113988757133484,0.461095566078347,0.3375769118570911,mmlu_offline:llama2-7b_chat:high_school_government_and_politics,test,6.337411097949371
43,0.39534884691238403,0.5581395626068115,0.5441176470588236,0.2435501411903736,mmlu_offline:llama2-7b_chat:high_school_macroeconomics,validation,1.3854552318807691
390,0.2897436022758484,0.5538461804389954,0.6332864764704004,0.16249999434520035,mmlu_offline:llama2-7b_chat:high_school_macroeconomics,test,11.41070145694539
29,0.0,0.931034505367279,,0.15342133826222915,mmlu_offline:llama2-7b_chat:high_school_mathematics,validation,1.5075965928845108
270,0.0555555559694767,0.9333333373069763,0.5490196078431373,0.06221064969345377,mmlu_offline:llama2-7b_chat:high_school_mathematics,test,13.092705209972337
26,0.26923078298568726,0.5,0.5639097744360902,0.3016827083550967,mmlu_offline:llama2-7b_chat:high_school_microeconomics,validation,0.9540547269862145
238,0.3403361439704895,0.4663865566253662,0.5665644412990485,0.31811317300596154,mmlu_offline:llama2-7b_chat:high_school_microeconomics,test,6.881912355078384
17,0.0,0.7058823704719543,,0.1946231617647059,mmlu_offline:llama2-7b_chat:high_school_physics,validation,1.0325124149676412
151,0.18543046712875366,0.6821191906929016,0.6477932636469222,0.14851511195795425,mmlu_offline:llama2-7b_chat:high_school_physics,test,7.099414204014465
60,0.5666666626930237,0.550000011920929,0.6199095022624435,0.22753906051317851,mmlu_offline:llama2-7b_chat:high_school_psychology,validation,2.501871680142358
545,0.5009174346923828,0.5082568526268005,0.5207188644688645,0.2620986154320043,mmlu_offline:llama2-7b_chat:high_school_psychology,test,21.719380815979093
23,0.17391304671764374,0.6086956262588501,0.44078947368421056,0.24660326346107153,mmlu_offline:llama2-7b_chat:high_school_statistics,validation,1.5507905900012702
216,0.19907407462596893,0.5972222089767456,0.626629923376798,0.1753833870644923,mmlu_offline:llama2-7b_chat:high_school_statistics,test,13.625095552997664
22,0.6363636255264282,0.7272727489471436,0.6116071428571428,0.24822442369027573,mmlu_offline:llama2-7b_chat:high_school_us_history,validation,5.429637257941067
204,0.5784313678741455,0.593137264251709,0.5682893180922349,0.1975720136188993,mmlu_offline:llama2-7b_chat:high_school_us_history,test,49.56762046297081
26,0.7307692170143127,0.6153846383094788,0.7293233082706767,0.14362980539982137,mmlu_offline:llama2-7b_chat:high_school_world_history,validation,4.608164576813579
237,0.552742600440979,0.5611814260482788,0.574319458447357,0.13164227672770054,mmlu_offline:llama2-7b_chat:high_school_world_history,test,37.75630446197465
23,0.30434781312942505,0.5652173757553101,0.5669642857142857,0.15540081262588504,mmlu_offline:llama2-7b_chat:human_aging,validation,0.743277793051675
223,0.33183857798576355,0.6143497824668884,0.5297478686740431,0.14276205210407752,mmlu_offline:llama2-7b_chat:human_aging,test,6.0917078570928425
12,0.25,0.6666666865348816,0.5925925925925926,0.2197265625,mmlu_offline:llama2-7b_chat:human_sexuality,validation,0.5303898579441011
131,0.4198473393917084,0.47328245639801025,0.48636363636363633,0.2679210346163684,mmlu_offline:llama2-7b_chat:human_sexuality,test,4.065534960012883
13,0.4615384638309479,0.38461539149284363,0.5,0.28906250000000006,mmlu_offline:llama2-7b_chat:international_law,validation,0.6475521789398044
121,0.6115702390670776,0.5702479481697083,0.543271995399655,0.06191890988468139,mmlu_offline:llama2-7b_chat:international_law,test,4.948874009074643
11,0.5454545617103577,0.3636363744735718,0.3499999999999999,0.4957386472008445,mmlu_offline:llama2-7b_chat:jurisprudence,validation,0.5777322750072926
108,0.5185185074806213,0.5370370149612427,0.592032967032967,0.2938006293994409,mmlu_offline:llama2-7b_chat:jurisprudence,test,3.510866951197386
18,0.4444444477558136,0.5555555820465088,0.40625,0.38346353835529756,mmlu_offline:llama2-7b_chat:logical_fallacies,validation,0.8132781069725752
163,0.42944785952568054,0.5950919985771179,0.6397849462365591,0.10189801052304134,mmlu_offline:llama2-7b_chat:logical_fallacies,test,5.675943292211741
11,0.3636363744735718,0.4545454680919647,0.4285714285714286,0.3185369318181819,mmlu_offline:llama2-7b_chat:machine_learning,validation,0.6931548230350018
112,0.1964285671710968,0.6160714030265808,0.498989898989899,0.15876114581312448,mmlu_offline:llama2-7b_chat:machine_learning,test,5.856184225063771
11,0.27272728085517883,0.7272727489471436,0.6875,0.24644887989217582,mmlu_offline:llama2-7b_chat:management,validation,0.46172845200635493
103,0.3689320385456085,0.6213592290878296,0.6495951417004049,0.1166944741045387,mmlu_offline:llama2-7b_chat:management,test,2.4836434880271554
25,0.20000000298023224,0.4000000059604645,0.395,0.4023437452316284,mmlu_offline:llama2-7b_chat:marketing,validation,1.0253824801184237
234,0.39743590354919434,0.44017094373703003,0.5188743994509265,0.3544337632309677,mmlu_offline:llama2-7b_chat:marketing,test,7.415387475164607
11,0.7272727489471436,0.4545454680919647,0.3541666666666667,0.36363636363636365,mmlu_offline:llama2-7b_chat:medical_genetics,validation,0.49347890983335674
100,0.4099999964237213,0.5400000214576721,0.5155022736668045,0.19968749284744267,mmlu_offline:llama2-7b_chat:medical_genetics,test,2.657453104155138
86,0.41860464215278625,0.5465116500854492,0.5480555555555556,0.1870912133261215,mmlu_offline:llama2-7b_chat:miscellaneous,validation,2.328630784060806
783,0.4533844292163849,0.4661557972431183,0.48395748321705934,0.26090558826695,mmlu_offline:llama2-7b_chat:miscellaneous,test,20.910837319213897
38,0.4736842215061188,0.5526315569877625,0.6972222222222222,0.25400903193574204,mmlu_offline:llama2-7b_chat:moral_disputes,validation,1.4953465859871358
346,0.41040462255477905,0.43063583970069885,0.496565175365921,0.36355218997580463,mmlu_offline:llama2-7b_chat:moral_disputes,test,12.212925106985494
100,0.5099999904632568,0.5,0.5408163265306122,0.07550780832767484,mmlu_offline:llama2-7b_chat:moral_scenarios,validation,6.0602089199237525
895,0.46145251393318176,0.5508379936218262,0.565523494720344,0.0200811893580346,mmlu_offline:llama2-7b_chat:moral_scenarios,test,52.894330132054165
33,0.3636363744735718,0.6363636255264282,0.6448412698412699,0.16299716270331183,mmlu_offline:llama2-7b_chat:nutrition,validation,1.5147157900501043
306,0.4084967374801636,0.5228758454322815,0.5109613259668508,0.20505258479928656,mmlu_offline:llama2-7b_chat:nutrition,test,12.531290596118197
34,0.3529411852359772,0.3529411852359772,0.5,0.4927619590478785,mmlu_offline:llama2-7b_chat:philosophy,validation,1.1499514528550208
311,0.3376205861568451,0.35691317915916443,0.488580674988442,0.46445437968735537,mmlu_offline:llama2-7b_chat:philosophy,test,8.867799618048593
35,0.2857142984867096,0.37142857909202576,0.516,0.4550223248345512,mmlu_offline:llama2-7b_chat:prehistory,validation,1.3371569339651614
324,0.34259259700775146,0.4104938209056854,0.5614135262022586,0.4189332552530148,mmlu_offline:llama2-7b_chat:prehistory,test,10.828891531797126
31,0.06451612710952759,0.5806451439857483,0.48275862068965514,0.2584425518589635,mmlu_offline:llama2-7b_chat:professional_accounting,validation,2.2329056449234486
282,0.1560283750295639,0.673758864402771,0.5920550038197097,0.13149656619585998,mmlu_offline:llama2-7b_chat:professional_accounting,test,19.13943733694032
170,0.30588236451148987,0.4058823585510254,0.601205997392438,0.2636029359172372,mmlu_offline:llama2-7b_chat:professional_law,validation,25.949019633000717
1534,0.3200782239437103,0.3735332489013672,0.5182811215493456,0.3027649265768631,mmlu_offline:llama2-7b_chat:professional_law,test,237.51449845312163
31,0.25806450843811035,0.5483871102333069,0.33152173913043476,0.1674647177419355,mmlu_offline:llama2-7b_chat:professional_medicine,validation,3.811583308968693
272,0.23529411852359772,0.6691176295280457,0.589618389423077,0.06712431110003413,mmlu_offline:llama2-7b_chat:professional_medicine,test,32.986944498028606
69,0.37681159377098083,0.4202898442745209,0.40921288014311274,0.32218070393023285,mmlu_offline:llama2-7b_chat:professional_psychology,validation,3.1528765459079295
612,0.32189542055130005,0.47385621070861816,0.4866307871078222,0.2687334073524849,mmlu_offline:llama2-7b_chat:professional_psychology,test,25.062216598074883
12,0.3333333432674408,0.5,0.59375,0.22656249006589252,mmlu_offline:llama2-7b_chat:public_relations,validation,0.5641437750309706
110,0.3272727131843567,0.5545454621315002,0.5334084084084084,0.2411221677606756,mmlu_offline:llama2-7b_chat:public_relations,test,3.7474000200163573
27,0.7407407164573669,0.7777777910232544,0.7107142857142857,0.17201967592592593,mmlu_offline:llama2-7b_chat:security_studies,validation,1.527913476107642
245,0.6816326379776001,0.722449004650116,0.6712344541685858,0.16930803960683397,mmlu_offline:llama2-7b_chat:security_studies,test,12.75203962996602
22,0.5,0.6363636255264282,0.6611570247933884,0.17631392316384747,mmlu_offline:llama2-7b_chat:sociology,validation,0.7843175390735269
201,0.45771142840385437,0.5522388219833374,0.5410351017151973,0.19241682658741133,mmlu_offline:llama2-7b_chat:sociology,test,6.186008695978671
11,0.5454545617103577,0.6363636255264282,0.7,0.23011364178224045,mmlu_offline:llama2-7b_chat:us_foreign_policy,validation,0.5303966458886862
100,0.550000011920929,0.5099999904632568,0.4957575757575758,0.2319140696525574,mmlu_offline:llama2-7b_chat:us_foreign_policy,test,3.2076506200246513
18,0.2777777910232544,0.6666666865348816,0.5076923076923077,0.26410590940051615,mmlu_offline:llama2-7b_chat:virology,validation,0.9010567090008408
166,0.34337350726127625,0.6265060305595398,0.5787059391598263,0.0790192003709724,mmlu_offline:llama2-7b_chat:virology,test,5.039392156060785
19,0.6315789222717285,0.42105263471603394,0.5059523809523809,0.27837169797796957,mmlu_offline:llama2-7b_chat:world_religions,validation,0.6479098598938435
171,0.5380116701126099,0.49707603454589844,0.5069482663731426,0.19588358952985171,mmlu_offline:llama2-7b_chat:world_religions,test,4.074013222940266
