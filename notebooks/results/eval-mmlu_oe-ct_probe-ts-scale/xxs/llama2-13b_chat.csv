N,acc,unc_acc,unc_auroc,unc_ece,dataset,split,ts
11,0.09090909361839294,0.9090909361839294,1.0,0.08203127709302038,mmlu_offline:llama2-13b_chat:abstract_algebra,validation,5.955352217890322
100,0.3499999940395355,0.6499999761581421,0.5131868131868131,0.3336489616018353,mmlu_offline:llama2-13b_chat:abstract_algebra,test,5.626538315787911
14,0.2857142984867096,0.7142857313156128,0.075,0.24944197280066355,mmlu_offline:llama2-13b_chat:anatomy,validation,0.8864633738994598
135,0.42222222685813904,0.5777778029441833,0.4082321187584345,0.2994791635760554,mmlu_offline:llama2-13b_chat:anatomy,test,6.521961377933621
16,0.5,0.5,0.84375,0.4453125,mmlu_offline:llama2-13b_chat:astronomy,validation,1.0885553960688412
152,0.5263158082962036,0.4736842215061188,0.6304687499999999,0.4552323084912802,mmlu_offline:llama2-13b_chat:astronomy,test,8.76325392909348
11,0.5454545617103577,0.4545454680919647,0.6333333333333333,0.5010653409090909,mmlu_offline:llama2-13b_chat:business_ethics,validation,1.0876333829946816
100,0.3199999928474426,0.6800000071525574,0.5654871323529411,0.2260890386321328,mmlu_offline:llama2-13b_chat:business_ethics,test,7.687416934873909
29,0.24137930572032928,0.7586206793785095,0.5292207792207793,0.15005389369767286,mmlu_offline:llama2-13b_chat:clinical_knowledge,validation,1.7643944970332086
265,0.2981131970882416,0.701886773109436,0.6002790254525656,0.20026533446221986,mmlu_offline:llama2-13b_chat:clinical_knowledge,test,13.760677731130272
16,0.25,0.75,0.40625,0.1904296986758709,mmlu_offline:llama2-13b_chat:college_biology,validation,1.1576039660722017
144,0.3958333432674408,0.6041666865348816,0.5537406735228877,0.3099772110581398,mmlu_offline:llama2-13b_chat:college_biology,test,9.252695806790143
8,0.0,1.0,,0.0556640625,mmlu_offline:llama2-13b_chat:college_chemistry,validation,0.9344333889894187
100,0.17000000178813934,0.8299999833106995,0.57512402551382,0.13816848735219428,mmlu_offline:llama2-13b_chat:college_chemistry,test,7.268323825206608
11,0.27272728085517883,0.7272727489471436,0.9375,0.23224433443763037,mmlu_offline:llama2-13b_chat:college_computer_science,validation,1.6122608841396868
100,0.23999999463558197,0.7599999904632568,0.5383771929824561,0.21234375000000003,mmlu_offline:llama2-13b_chat:college_computer_science,test,13.327181802596897
11,0.0,1.0,,0.03409093618392944,mmlu_offline:llama2-13b_chat:college_mathematics,validation,1.270435362122953
100,0.09000000357627869,0.9100000262260437,0.6465201465201466,0.0631640815734863,mmlu_offline:llama2-13b_chat:college_mathematics,test,9.318137784022838
22,0.5,0.5,0.6446280991735537,0.40181107412685046,mmlu_offline:llama2-13b_chat:college_medicine,validation,1.6711703487671912
173,0.3988439440727234,0.6011560559272766,0.6276477146042363,0.3085480691396702,mmlu_offline:llama2-13b_chat:college_medicine,test,17.31152370199561
11,0.27272728085517883,0.7272727489471436,0.25,0.26171877167441626,mmlu_offline:llama2-13b_chat:college_physics,validation,1.1187265501357615
102,0.1568627506494522,0.843137264251709,0.5352470930232558,0.12855116165045533,mmlu_offline:llama2-13b_chat:college_physics,test,8.003118670079857
11,0.5454545617103577,0.4545454680919647,0.7833333333333333,0.5421875,mmlu_offline:llama2-13b_chat:computer_security,validation,1.0289947581477463
100,0.550000011920929,0.44999998807907104,0.5078787878787878,0.47462913002630674,mmlu_offline:llama2-13b_chat:computer_security,test,5.3633158267475665
26,0.3461538553237915,0.6538461446762085,0.6601307189542484,0.2644230815080496,mmlu_offline:llama2-13b_chat:conceptual_physics,validation,1.6806412832811475
235,0.4553191363811493,0.5446808338165283,0.5795852803738317,0.3745492910727476,mmlu_offline:llama2-13b_chat:conceptual_physics,test,10.907845824025571
12,0.25,0.75,0.3333333333333333,0.14746095240116117,mmlu_offline:llama2-13b_chat:econometrics,validation,1.3798653669655323
114,0.1315789520740509,0.8684210777282715,0.603030303030303,0.0611979302607085,mmlu_offline:llama2-13b_chat:econometrics,test,10.839632081333548
16,0.25,0.75,0.7708333333333334,0.15136716142296794,mmlu_offline:llama2-13b_chat:electrical_engineering,validation,1.1886626142077148
145,0.24827586114406586,0.751724123954773,0.591360856269113,0.14908404432494066,mmlu_offline:llama2-13b_chat:electrical_engineering,test,9.192042905371636
41,0.26829269528388977,0.7317073345184326,0.5454545454545455,0.2502929404377937,mmlu_offline:llama2-13b_chat:elementary_mathematics,validation,3.08601515321061
378,0.3174603283405304,0.682539701461792,0.37501614987080095,0.2909655530228574,mmlu_offline:llama2-13b_chat:elementary_mathematics,test,25.635116446297616
14,0.2142857164144516,0.7857142686843872,0.5909090909090909,0.19475448131561277,mmlu_offline:llama2-13b_chat:formal_logic,validation,1.2656839699484408
126,0.261904776096344,0.738095223903656,0.621700879765396,0.21276662368623034,mmlu_offline:llama2-13b_chat:formal_logic,test,9.862788715399802
10,0.20000000298023224,0.800000011920929,0.5,0.16835939884185797,mmlu_offline:llama2-13b_chat:global_facts,validation,0.894316011108458
100,0.12999999523162842,0.8700000047683716,0.44120247568523435,0.10677084116020588,mmlu_offline:llama2-13b_chat:global_facts,test,5.828652669675648
32,0.3125,0.6875,0.38863636363636367,0.2664794921875,mmlu_offline:llama2-13b_chat:high_school_biology,validation,2.1534282537177205
310,0.42258065938949585,0.5774193406105042,0.5265682971555291,0.3652595935329314,mmlu_offline:llama2-13b_chat:high_school_biology,test,19.815892928745598
22,0.1818181872367859,0.8181818127632141,0.5277777777777778,0.15662201245625815,mmlu_offline:llama2-13b_chat:high_school_chemistry,validation,1.758611287921667
203,0.1822660118341446,0.8177340030670166,0.5221426245522631,0.13002231200130618,mmlu_offline:llama2-13b_chat:high_school_chemistry,test,14.569639334920794
9,0.6666666865348816,0.3333333432674408,0.6111111111111112,0.6085069311989678,mmlu_offline:llama2-13b_chat:high_school_computer_science,validation,1.327898554969579
100,0.4300000071525574,0.5699999928474426,0.5875152998776011,0.37197971221097964,mmlu_offline:llama2-13b_chat:high_school_computer_science,test,11.855737566947937
18,0.7777777910232544,0.2222222238779068,0.8571428571428571,0.6922742989328172,mmlu_offline:llama2-13b_chat:high_school_european_history,validation,9.506697528064251
165,0.7575757503509521,0.24242424964904785,0.6670000000000001,0.6731297334035238,mmlu_offline:llama2-13b_chat:high_school_european_history,test,86.40381387621164
22,0.5,0.5,0.5909090909090908,0.417080974036997,mmlu_offline:llama2-13b_chat:high_school_geography,validation,1.3752620569430292
198,0.39393940567970276,0.6060606241226196,0.5791132478632479,0.2928101069431014,mmlu_offline:llama2-13b_chat:high_school_geography,test,9.444616209249943
21,0.523809552192688,0.4761904776096344,0.8863636363636364,0.4345238123621259,mmlu_offline:llama2-13b_chat:high_school_government_and_politics,validation,1.5642675734125078
193,0.6321243643760681,0.3678756356239319,0.5690948972523666,0.5301975212566594,mmlu_offline:llama2-13b_chat:high_school_government_and_politics,test,10.770314794965088
43,0.4883720874786377,0.5116279125213623,0.5541125541125542,0.4009810907896175,mmlu_offline:llama2-13b_chat:high_school_macroeconomics,validation,2.335701474919915
390,0.3692307770252228,0.6307692527770996,0.587327800361337,0.2871498480979643,mmlu_offline:llama2-13b_chat:high_school_macroeconomics,test,19.545101010240614
29,0.06896551698446274,0.931034505367279,0.7777777777777778,0.05967134442822684,mmlu_offline:llama2-13b_chat:high_school_mathematics,validation,2.4663686179555953
270,0.10000000149011612,0.8999999761581421,0.3962810547172687,0.08273880414243014,mmlu_offline:llama2-13b_chat:high_school_mathematics,test,21.815923960879445
26,0.3076923191547394,0.692307710647583,0.7777777777777778,0.22145434067799494,mmlu_offline:llama2-13b_chat:high_school_microeconomics,validation,1.4718952584080398
238,0.36554622650146484,0.6344537734985352,0.5814493415543883,0.26898962309380536,mmlu_offline:llama2-13b_chat:high_school_microeconomics,test,11.641376006416976
17,0.29411765933036804,0.7058823704719543,0.5583333333333333,0.2653952416251687,mmlu_offline:llama2-13b_chat:high_school_physics,validation,1.6104497890919447
151,0.17218543589115143,0.8278145790100098,0.582,0.14054633787014342,mmlu_offline:llama2-13b_chat:high_school_physics,test,11.716050706803799
60,0.6000000238418579,0.4000000059604645,0.5578703703703703,0.516992183526357,mmlu_offline:llama2-13b_chat:high_school_psychology,validation,4.099963769782335
545,0.5064220428466797,0.4935779869556427,0.5442796724314423,0.4121057813777889,mmlu_offline:llama2-13b_chat:high_school_psychology,test,36.09978306805715
23,0.17391304671764374,0.8260869383811951,0.8026315789473684,0.13400135610414587,mmlu_offline:llama2-13b_chat:high_school_statistics,validation,2.5352727719582617
216,0.25925925374031067,0.7407407164573669,0.5113281249999999,0.22093025917230652,mmlu_offline:llama2-13b_chat:high_school_statistics,test,22.752325835172087
22,0.7727272510528564,0.22727273404598236,0.4588235294117647,0.6516335254365748,mmlu_offline:llama2-13b_chat:high_school_us_history,validation,9.010458432603627
204,0.6764705777168274,0.3235294222831726,0.6270311813790075,0.5598766830037623,mmlu_offline:llama2-13b_chat:high_school_us_history,test,81.59057191573083
26,0.6538461446762085,0.3461538553237915,0.2647058823529412,0.6150841506627891,mmlu_offline:llama2-13b_chat:high_school_world_history,validation,7.5728029501624405
237,0.6033755540847778,0.39662447571754456,0.484860883797054,0.5552313820722234,mmlu_offline:llama2-13b_chat:high_school_world_history,test,62.606256500817835
23,0.3913043439388275,0.6086956262588501,0.6666666666666666,0.3344089518422666,mmlu_offline:llama2-13b_chat:human_aging,validation,1.2760967328213155
223,0.3901345431804657,0.6098654866218567,0.45085361730899254,0.3307174887892377,mmlu_offline:llama2-13b_chat:human_aging,test,10.85208266414702
12,0.3333333432674408,0.6666666865348816,0.71875,0.2522786458333333,mmlu_offline:llama2-13b_chat:human_sexuality,validation,0.8985886760056019
131,0.5114504098892212,0.4885496199131012,0.49020522388059706,0.43591962515852833,mmlu_offline:llama2-13b_chat:human_sexuality,test,6.8788087191060185
13,0.6153846383094788,0.38461539149284363,0.35,0.4813701923076924,mmlu_offline:llama2-13b_chat:international_law,validation,1.0502762836404145
121,0.6280992031097412,0.3719008266925812,0.555701754385965,0.5156572820726505,mmlu_offline:llama2-13b_chat:international_law,test,8.050039138644934
11,0.1818181872367859,0.8181818127632141,0.0,0.14914773810993542,mmlu_offline:llama2-13b_chat:jurisprudence,validation,0.8641528091393411
108,0.3611111044883728,0.6388888955116272,0.5081753994797473,0.25553385527045636,mmlu_offline:llama2-13b_chat:jurisprudence,test,5.945135699119419
18,0.6666666865348816,0.3333333432674408,0.5069444444444444,0.5262586938010322,mmlu_offline:llama2-13b_chat:logical_fallacies,validation,1.3164536356925964
163,0.48466256260871887,0.5153374075889587,0.6444394213381555,0.3463621501542308,mmlu_offline:llama2-13b_chat:logical_fallacies,test,9.450117751024663
11,0.27272728085517883,0.7272727489471436,0.8541666666666666,0.2240766828710382,mmlu_offline:llama2-13b_chat:machine_learning,validation,1.1461326256394386
112,0.2946428656578064,0.7053571343421936,0.5663598005370157,0.2374093330332211,mmlu_offline:llama2-13b_chat:machine_learning,test,9.505625540856272
11,0.6363636255264282,0.3636363744735718,0.26785714285714285,0.557883538983085,mmlu_offline:llama2-13b_chat:management,validation,0.7286911550909281
103,0.42718446254730225,0.5728155374526978,0.5743451463790447,0.3143962465443657,mmlu_offline:llama2-13b_chat:management,test,4.65664628893137
25,0.23999999463558197,0.7599999904632568,0.662280701754386,0.08671875000000001,mmlu_offline:llama2-13b_chat:marketing,validation,1.7014636141248047
234,0.44871795177459717,0.5512820482254028,0.6461055740125508,0.2777944810879536,mmlu_offline:llama2-13b_chat:marketing,test,12.955593624152243
11,0.9090909361839294,0.09090909361839294,0.9,0.7709516991268505,mmlu_offline:llama2-13b_chat:medical_genetics,validation,0.7992504546418786
100,0.44999998807907104,0.550000011920929,0.6008080808080807,0.3107421886920929,mmlu_offline:llama2-13b_chat:medical_genetics,test,4.693416041787714
86,0.5581395626068115,0.44186046719551086,0.4026864035087719,0.5407366192057019,mmlu_offline:llama2-13b_chat:miscellaneous,validation,4.0876776752993464
783,0.618135392665863,0.38186463713645935,0.3904267669089803,0.5980020987335593,mmlu_offline:llama2-13b_chat:miscellaneous,test,36.82912815734744
38,0.42105263471603394,0.5789473652839661,0.6207386363636362,0.30057566730599655,mmlu_offline:llama2-13b_chat:moral_disputes,validation,2.394536986015737
346,0.43063583970069885,0.5693641901016235,0.5008687357339965,0.3133693049753332,mmlu_offline:llama2-13b_chat:moral_disputes,test,19.783098021987826
100,0.4300000071525574,0.5699999928474426,0.6099551203590371,0.4290834407216495,mmlu_offline:llama2-13b_chat:moral_scenarios,validation,10.286075702868402
895,0.38100558519363403,0.618994414806366,0.5995823496405772,0.37940388266102165,mmlu_offline:llama2-13b_chat:moral_scenarios,test,90.47665904276073
33,0.3333333432674408,0.6666666865348816,0.5227272727272727,0.2642045273925319,mmlu_offline:llama2-13b_chat:nutrition,validation,2.5534443869255483
306,0.4542483687400818,0.5490196347236633,0.467065868263473,0.38222527562403213,mmlu_offline:llama2-13b_chat:nutrition,test,21.076076648198068
34,0.3235294222831726,0.6764705777168274,0.38537549407114624,0.2302389916251688,mmlu_offline:llama2-13b_chat:philosophy,validation,2.1358510022982955
311,0.3633440434932709,0.6366559267044067,0.4962903369983016,0.26708198724452326,mmlu_offline:llama2-13b_chat:philosophy,test,15.78217723313719
35,0.37142857909202576,0.6285714507102966,0.34265734265734266,0.27142855099269325,mmlu_offline:llama2-13b_chat:prehistory,validation,2.4039933122694492
324,0.4413580298423767,0.5586419701576233,0.6043928447243364,0.3197578957051406,mmlu_offline:llama2-13b_chat:prehistory,test,18.239822443109006
31,0.16129031777381897,0.8387096524238586,0.49615384615384617,0.1357106585656443,mmlu_offline:llama2-13b_chat:professional_accounting,validation,3.6406832891516387
282,0.1879432648420334,0.8120567202568054,0.5808684188844031,0.14853448225251326,mmlu_offline:llama2-13b_chat:professional_accounting,test,31.98708302807063
170,0.38235294818878174,0.6176470518112183,0.38249084249084253,0.3270909754668965,mmlu_offline:llama2-13b_chat:professional_law,validation,42.66540807811543
1534,0.34810951352119446,0.6518904566764832,0.3951058052434457,0.2928617745443615,mmlu_offline:llama2-13b_chat:professional_law,test,391.43957115197554
31,0.35483869910240173,0.6451612710952759,0.49090909090909096,0.28654234640059933,mmlu_offline:llama2-13b_chat:professional_medicine,validation,6.207041339017451
272,0.2904411852359772,0.7095588445663452,0.39748803043221614,0.24401136619203231,mmlu_offline:llama2-13b_chat:professional_medicine,test,55.46261095767841
69,0.37681159377098083,0.6231883764266968,0.7061717352415027,0.28379753016043396,mmlu_offline:llama2-13b_chat:professional_psychology,validation,5.084015590138733
612,0.3464052379131317,0.6535947918891907,0.527629716981132,0.25519558689952676,mmlu_offline:llama2-13b_chat:professional_psychology,test,41.635594895109534
12,0.1666666716337204,0.8333333134651184,0.3,0.046875009934107426,mmlu_offline:llama2-13b_chat:public_relations,validation,0.9201996931806207
110,0.30909091234207153,0.6909090876579285,0.44156346749226005,0.16544977081156217,mmlu_offline:llama2-13b_chat:public_relations,test,6.166318427305669
27,0.5555555820465088,0.4444444477558136,0.3777777777777778,0.46455438931783044,mmlu_offline:llama2-13b_chat:security_studies,validation,2.4177242228761315
245,0.6612244844436646,0.33877551555633545,0.5406812434924885,0.563775506798102,mmlu_offline:llama2-13b_chat:security_studies,test,20.636251350864768
22,0.5909090638160706,0.40909090638160706,0.6282051282051282,0.4335937581279061,mmlu_offline:llama2-13b_chat:sociology,validation,1.3529861262068152
201,0.447761207818985,0.5522388219833374,0.5396396396396397,0.29557292111477446,mmlu_offline:llama2-13b_chat:sociology,test,10.622720084153116
11,0.7272727489471436,0.27272728085517883,0.3333333333333333,0.6207386417822405,mmlu_offline:llama2-13b_chat:us_foreign_policy,validation,0.8862217827700078
100,0.5899999737739563,0.4099999964237213,0.5173625465068209,0.4972656124830247,mmlu_offline:llama2-13b_chat:us_foreign_policy,test,5.456719808746129
18,0.5555555820465088,0.4444444477558136,0.48750000000000004,0.47460938162273836,mmlu_offline:llama2-13b_chat:virology,validation,1.3819910371676087
166,0.34939759969711304,0.650602400302887,0.51772030651341,0.25430629734533383,mmlu_offline:llama2-13b_chat:virology,test,8.639503244310617
19,0.6315789222717285,0.3684210479259491,0.3928571428571429,0.5968338564822547,mmlu_offline:llama2-13b_chat:world_religions,validation,1.1158753288909793
171,0.6081871390342712,0.39181286096572876,0.3012342135476464,0.5689418824792606,mmlu_offline:llama2-13b_chat:world_religions,test,7.857048312202096
