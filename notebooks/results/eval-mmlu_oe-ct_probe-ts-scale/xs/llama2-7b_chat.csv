N,acc,unc_acc,unc_auroc,unc_ece,dataset,split,ts
11,0.09090909361839294,0.7272727489471436,0.8,0.1452414718541232,mmlu_offline:llama2-7b_chat:abstract_algebra,validation,1.8144029066897929
100,0.20999999344348907,0.6100000143051147,0.6024713682941532,0.09160157620906827,mmlu_offline:llama2-7b_chat:abstract_algebra,test,3.238743994385004
14,0.2142857164144516,0.2142857164144516,0.9090909090909091,0.45507814202989855,mmlu_offline:llama2-7b_chat:anatomy,validation,0.5071498299948871
135,0.385185182094574,0.42222222685813904,0.6990268767377202,0.23582174248165555,mmlu_offline:llama2-7b_chat:anatomy,test,3.621630661189556
16,0.5625,0.5625,0.6111111111111112,0.2199706956744194,mmlu_offline:llama2-7b_chat:astronomy,validation,0.7410849779844284
152,0.42763158679008484,0.5131579041481018,0.6389036251105217,0.11320416864595914,mmlu_offline:llama2-7b_chat:astronomy,test,5.536766530945897
11,0.4545454680919647,0.4545454680919647,0.5166666666666666,0.15198864720084448,mmlu_offline:llama2-7b_chat:business_ethics,validation,0.6407101470977068
100,0.33000001311302185,0.4099999964237213,0.7191316146540027,0.221718755364418,mmlu_offline:llama2-7b_chat:business_ethics,test,4.651648876722902
29,0.17241379618644714,0.17241379618644714,0.7166666666666669,0.4498922352133126,mmlu_offline:llama2-7b_chat:clinical_knowledge,validation,0.9756266521289945
265,0.2792452871799469,0.324528306722641,0.6765246922314985,0.30617629019719256,mmlu_offline:llama2-7b_chat:clinical_knowledge,test,7.893816311843693
16,0.375,0.4375,0.5499999999999999,0.2592773512005806,mmlu_offline:llama2-7b_chat:college_biology,validation,0.714225810021162
144,0.3402777910232544,0.375,0.564983888292159,0.26166447500387835,mmlu_offline:llama2-7b_chat:college_biology,test,5.635213966947049
8,0.0,0.125,,0.6279296949505806,mmlu_offline:llama2-7b_chat:college_chemistry,validation,0.44689937587827444
100,0.14000000059604645,0.4699999988079071,0.7321428571428572,0.13867185056209566,mmlu_offline:llama2-7b_chat:college_chemistry,test,4.438591608311981
11,0.27272728085517883,0.3636363744735718,0.5,0.3778409090909091,mmlu_offline:llama2-7b_chat:college_computer_science,validation,0.9673463650979102
100,0.17000000178813934,0.5099999904632568,0.6895818568391212,0.1472265815734863,mmlu_offline:llama2-7b_chat:college_computer_science,test,8.006069920025766
11,0.0,0.7272727489471436,,0.2194602218541232,mmlu_offline:llama2-7b_chat:college_mathematics,validation,0.7695193109102547
100,0.2199999988079071,0.6399999856948853,0.581002331002331,0.08062498629093172,mmlu_offline:llama2-7b_chat:college_mathematics,test,5.612980813253671
22,0.3181818127632141,0.4545454680919647,0.4380952380952381,0.29669744860042224,mmlu_offline:llama2-7b_chat:college_medicine,validation,1.0197048308327794
173,0.2947976887226105,0.3988439440727234,0.721150755384121,0.24482931153622664,mmlu_offline:llama2-7b_chat:college_medicine,test,10.237524658907205
11,0.09090909361839294,0.3636363744735718,1.0,0.29865055734461,mmlu_offline:llama2-7b_chat:college_physics,validation,0.6614391203038394
102,0.0882352963089943,0.529411792755127,0.7198327359617682,0.10650274040652256,mmlu_offline:llama2-7b_chat:college_physics,test,4.812796650920063
11,0.6363636255264282,0.7272727489471436,0.6785714285714286,0.12748579545454547,mmlu_offline:llama2-7b_chat:computer_security,validation,0.6017867517657578
100,0.5,0.5799999833106995,0.626,0.03648435473442078,mmlu_offline:llama2-7b_chat:computer_security,test,3.2503040693700314
26,0.1538461595773697,0.26923078298568726,0.5,0.3556190156019651,mmlu_offline:llama2-7b_chat:conceptual_physics,validation,0.8987344899214804
235,0.3787234127521515,0.4936170279979706,0.6011235955056179,0.15320812565215092,mmlu_offline:llama2-7b_chat:conceptual_physics,test,6.376789187081158
12,0.3333333432674408,0.9166666865348816,0.90625,0.3496093700329463,mmlu_offline:llama2-7b_chat:econometrics,validation,0.8135527940467
114,0.17543859779834747,0.6052631735801697,0.5106382978723405,0.07757673922337982,mmlu_offline:llama2-7b_chat:econometrics,test,6.4760703281499445
16,0.1875,0.125,0.3846153846153846,0.4699707068502903,mmlu_offline:llama2-7b_chat:electrical_engineering,validation,0.7177582532167435
145,0.2137930989265442,0.35172414779663086,0.5021222410865874,0.24143320691996603,mmlu_offline:llama2-7b_chat:electrical_engineering,test,5.601104501634836
41,0.3658536672592163,0.6097561120986938,0.6397435897435898,0.06307164924900709,mmlu_offline:llama2-7b_chat:elementary_mathematics,validation,1.853660306893289
378,0.28042328357696533,0.6137565970420837,0.5327934239733629,0.05017154730817002,mmlu_offline:llama2-7b_chat:elementary_mathematics,test,15.299455760978162
14,0.4285714328289032,0.6428571343421936,0.7083333333333333,0.07924107142857145,mmlu_offline:llama2-7b_chat:formal_logic,validation,0.7648795158602297
126,0.2539682686328888,0.4682539701461792,0.6963098404255319,0.12509302582059587,mmlu_offline:llama2-7b_chat:formal_logic,test,6.0035069598816335
10,0.20000000298023224,0.4000000059604645,0.625,0.1515624761581421,mmlu_offline:llama2-7b_chat:global_facts,validation,0.5092229759320617
100,0.07999999821186066,0.28999999165534973,0.6786684782608695,0.28972654581069945,mmlu_offline:llama2-7b_chat:global_facts,test,3.532853131182492
32,0.3125,0.28125,0.4340909090909092,0.3599853701889515,mmlu_offline:llama2-7b_chat:high_school_biology,validation,1.3459972641430795
310,0.3838709592819214,0.448387086391449,0.6105855954947425,0.2012853026390076,mmlu_offline:llama2-7b_chat:high_school_biology,test,11.973926295060664
22,0.1818181872367859,0.27272728085517883,0.41666666666666663,0.37109376083720813,mmlu_offline:llama2-7b_chat:high_school_chemistry,validation,1.0624601561576128
203,0.1428571492433548,0.37438422441482544,0.7270114942528736,0.24747923031229102,mmlu_offline:llama2-7b_chat:high_school_chemistry,test,8.781664846930653
9,0.4444444477558136,0.5555555820465088,0.29999999999999993,0.3398437632454766,mmlu_offline:llama2-7b_chat:high_school_computer_science,validation,0.7978437468409538
100,0.4099999964237213,0.4699999988079071,0.5529144274493591,0.15757812917232514,mmlu_offline:llama2-7b_chat:high_school_computer_science,test,7.139450021088123
18,0.8333333134651184,0.8333333134651184,0.6222222222222222,0.16319445768992105,mmlu_offline:llama2-7b_chat:high_school_european_history,validation,5.6821498312056065
165,0.7090908885002136,0.7333333492279053,0.6793981481481481,0.05710227453347411,mmlu_offline:llama2-7b_chat:high_school_european_history,test,52.05243491427973
22,0.4545454680919647,0.5454545617103577,0.6291666666666667,0.1337002840909091,mmlu_offline:llama2-7b_chat:high_school_geography,validation,0.7600106690078974
198,0.3737373650074005,0.40909090638160706,0.5838600697471665,0.2200323704517249,mmlu_offline:llama2-7b_chat:high_school_geography,test,5.699363606981933
21,0.523809552192688,0.4285714328289032,0.5363636363636364,0.2165178826877049,mmlu_offline:llama2-7b_chat:high_school_government_and_politics,validation,0.8397914129309356
193,0.5233160853385925,0.5595855116844177,0.5470297029702971,0.1091523408272106,mmlu_offline:llama2-7b_chat:high_school_government_and_politics,test,6.302862016949803
43,0.39534884691238403,0.4883720874786377,0.5158371040723981,0.11210029069767445,mmlu_offline:llama2-7b_chat:high_school_macroeconomics,validation,1.375575115904212
390,0.2897436022758484,0.4307692348957062,0.648477684419028,0.19022437104812034,mmlu_offline:llama2-7b_chat:high_school_macroeconomics,test,11.649590563960373
29,0.0,0.7586206793785095,,0.16985453819406443,mmlu_offline:llama2-7b_chat:high_school_mathematics,validation,1.4944452922791243
270,0.0555555559694767,0.7925925850868225,0.6176470588235294,0.1648292740186056,mmlu_offline:llama2-7b_chat:high_school_mathematics,test,13.038744549732655
26,0.26923078298568726,0.42307692766189575,0.7180451127819549,0.22355769230769235,mmlu_offline:llama2-7b_chat:high_school_microeconomics,validation,0.9605645788833499
238,0.3403361439704895,0.42016807198524475,0.6645828418652198,0.21589415689476402,mmlu_offline:llama2-7b_chat:high_school_microeconomics,test,6.85435332916677
17,0.0,0.47058823704719543,,0.17141544818878174,mmlu_offline:llama2-7b_chat:high_school_physics,validation,1.0256360787898302
151,0.18543046712875366,0.4503311216831207,0.681184668989547,0.1798427353631582,mmlu_offline:llama2-7b_chat:high_school_physics,test,7.058506128843874
60,0.5666666626930237,0.550000011920929,0.46040723981900455,0.09960935413837435,mmlu_offline:llama2-7b_chat:high_school_psychology,validation,2.501881239004433
545,0.5009174346923828,0.515596330165863,0.5724049235078646,0.1314435060964812,mmlu_offline:llama2-7b_chat:high_school_psychology,test,21.65607732301578
23,0.17391304671764374,0.43478259444236755,0.48684210526315785,0.24065897775732947,mmlu_offline:llama2-7b_chat:high_school_statistics,validation,1.5327030811458826
216,0.19907407462596893,0.41203704476356506,0.6143970963839226,0.2183702309374456,mmlu_offline:llama2-7b_chat:high_school_statistics,test,13.593154430389404
22,0.6363636255264282,0.6818181872367859,0.7053571428571428,0.10316051136363638,mmlu_offline:llama2-7b_chat:high_school_us_history,validation,5.414622323587537
204,0.5784313678741455,0.5980392098426819,0.658651951123374,0.09080118086992524,mmlu_offline:llama2-7b_chat:high_school_us_history,test,49.532839679159224
26,0.7307692170143127,0.692307710647583,0.8120300751879699,0.2662259615384615,mmlu_offline:llama2-7b_chat:high_school_world_history,validation,4.5900712898001075
237,0.552742600440979,0.5569620132446289,0.5914230159873253,0.10502374976999146,mmlu_offline:llama2-7b_chat:high_school_world_history,test,37.71550257969648
23,0.30434781312942505,0.3478260934352875,0.5580357142857142,0.30570651655611786,mmlu_offline:llama2-7b_chat:human_aging,validation,0.7392378239892423
223,0.33183857798576355,0.4573991000652313,0.6300108833665881,0.1693000045057904,mmlu_offline:llama2-7b_chat:human_aging,test,6.089321837294847
12,0.25,0.1666666716337204,0.4814814814814815,0.4482421974341075,mmlu_offline:llama2-7b_chat:human_sexuality,validation,0.5488633019849658
131,0.4198473393917084,0.4580152630805969,0.5998803827751196,0.1592616930262733,mmlu_offline:llama2-7b_chat:human_sexuality,test,4.036515450105071
13,0.4615384638309479,0.38461539149284363,0.4642857142857143,0.19350960621467006,mmlu_offline:llama2-7b_chat:international_law,validation,0.6381070367060602
121,0.6115702390670776,0.6033057570457458,0.6344163312248418,0.08648631641687438,mmlu_offline:llama2-7b_chat:international_law,test,4.916870004031807
11,0.5454545617103577,0.5454545617103577,0.43333333333333335,0.2148437554186041,mmlu_offline:llama2-7b_chat:jurisprudence,validation,0.5251289331354201
108,0.5185185074806213,0.5370370149612427,0.6189903846153846,0.12243201942355544,mmlu_offline:llama2-7b_chat:jurisprudence,test,3.492614195216447
18,0.4444444477558136,0.4444444477558136,0.7625,0.194010423289405,mmlu_offline:llama2-7b_chat:logical_fallacies,validation,0.7573015238158405
163,0.42944785952568054,0.453987717628479,0.6156682027649769,0.1860381693927788,mmlu_offline:llama2-7b_chat:logical_fallacies,test,5.702802765648812
11,0.3636363744735718,0.4545454680919647,0.32142857142857145,0.17578124999999997,mmlu_offline:llama2-7b_chat:machine_learning,validation,0.696042045019567
112,0.1964285671710968,0.4285714328289032,0.527020202020202,0.18868583760091237,mmlu_offline:llama2-7b_chat:machine_learning,test,5.827920829877257
11,0.27272728085517883,0.3636363744735718,0.4583333333333333,0.19566759196194733,mmlu_offline:llama2-7b_chat:management,validation,0.47742443392053246
103,0.3689320385456085,0.4660194218158722,0.624089068825911,0.14608616157642845,mmlu_offline:llama2-7b_chat:management,test,2.5018681241199374
25,0.20000000298023224,0.2800000011920929,0.29500000000000004,0.4225000023841858,mmlu_offline:llama2-7b_chat:marketing,validation,1.022135698236525
234,0.39743590354919434,0.4059829115867615,0.5098756958743231,0.23582733339733553,mmlu_offline:llama2-7b_chat:marketing,test,7.391141651198268
11,0.7272727489471436,0.6363636255264282,0.7708333333333334,0.1644176190549677,mmlu_offline:llama2-7b_chat:medical_genetics,validation,0.4806701256893575
100,0.4099999964237213,0.46000000834465027,0.6054154609342703,0.1728515589237213,mmlu_offline:llama2-7b_chat:medical_genetics,test,2.660864843055606
86,0.41860464215278625,0.5116279125213623,0.6083333333333334,0.11273619463277419,mmlu_offline:llama2-7b_chat:miscellaneous,validation,2.365926668047905
783,0.4533844292163849,0.5121328234672546,0.6606917204159537,0.11585550008301229,mmlu_offline:llama2-7b_chat:miscellaneous,test,20.931065103039145
38,0.4736842215061188,0.5263158082962036,0.7000000000000001,0.12068256578947369,mmlu_offline:llama2-7b_chat:moral_disputes,validation,1.4716093786992133
346,0.41040462255477905,0.424855500459671,0.5720104943385805,0.215092149084014,mmlu_offline:llama2-7b_chat:moral_disputes,test,12.167300086002797
100,0.5099999904632568,0.5099999904632568,0.595438175270108,0.08593749046325685,mmlu_offline:llama2-7b_chat:moral_scenarios,validation,6.020760590210557
895,0.46145251393318176,0.46145251393318176,0.5781951714506747,0.1374039680598168,mmlu_offline:llama2-7b_chat:moral_scenarios,test,52.798984955996275
33,0.3636363744735718,0.5454545617103577,0.7301587301587302,0.16548294370824645,mmlu_offline:llama2-7b_chat:nutrition,validation,1.5147553347051144
306,0.4084967374801636,0.44771242141723633,0.5729060773480663,0.1729090221567092,mmlu_offline:llama2-7b_chat:nutrition,test,12.471845427993685
34,0.3529411852359772,0.3529411852359772,0.4696969696969697,0.30422794117647056,mmlu_offline:llama2-7b_chat:philosophy,validation,1.1379959150217474
311,0.3376205861568451,0.35691317915916443,0.5545076282940361,0.30495125802766854,mmlu_offline:llama2-7b_chat:philosophy,test,8.869057463947684
35,0.2857142984867096,0.3142857253551483,0.6579999999999999,0.30691964966910223,mmlu_offline:llama2-7b_chat:prehistory,validation,1.338460040744394
324,0.34259259700775146,0.39814814925193787,0.6603011462166392,0.23255447307486593,mmlu_offline:llama2-7b_chat:prehistory,test,10.760758032090962
31,0.06451612710952759,0.3870967626571655,0.6379310344827586,0.22794859447786886,mmlu_offline:llama2-7b_chat:professional_accounting,validation,2.215319549664855
282,0.1560283750295639,0.41134750843048096,0.5933919022154316,0.19642065518291285,mmlu_offline:llama2-7b_chat:professional_accounting,test,19.128681004047394
170,0.30588236451148987,0.30588236451148987,0.5560625814863103,0.365946709057864,mmlu_offline:llama2-7b_chat:professional_law,validation,25.89950949512422
1534,0.3200782239437103,0.32203391194343567,0.6186583820367771,0.3558201635210268,mmlu_offline:llama2-7b_chat:professional_law,test,237.21291613485664
31,0.25806450843811035,0.29032257199287415,0.5135869565217391,0.3378276305813943,mmlu_offline:llama2-7b_chat:professional_medicine,validation,3.7885296368040144
272,0.23529411852359772,0.37132352590560913,0.6104266826923077,0.26235063812311965,mmlu_offline:llama2-7b_chat:professional_medicine,test,32.944507592823356
69,0.37681159377098083,0.4057970941066742,0.5644007155635062,0.20199275967003646,mmlu_offline:llama2-7b_chat:professional_psychology,validation,3.135397265665233
612,0.32189542055130005,0.3839869201183319,0.5088373799767598,0.21432033940857528,mmlu_offline:llama2-7b_chat:professional_psychology,test,24.930538669228554
12,0.3333333432674408,0.5,0.71875,0.2526041716337204,mmlu_offline:llama2-7b_chat:public_relations,validation,0.549254709854722
110,0.3272727131843567,0.4636363685131073,0.5602477477477477,0.14804688746278938,mmlu_offline:llama2-7b_chat:public_relations,test,3.7351174326613545
27,0.7407407164573669,0.7407407164573669,0.7464285714285714,0.14091434743669295,mmlu_offline:llama2-7b_chat:security_studies,validation,1.4977966891601682
245,0.6816326379776001,0.6979591846466064,0.6905803777061262,0.07726402404356976,mmlu_offline:llama2-7b_chat:security_studies,test,12.689085783902556
22,0.5,0.6818181872367859,0.5826446280991735,0.16938921267336068,mmlu_offline:llama2-7b_chat:sociology,validation,0.7978089600801468
201,0.45771142840385437,0.611940324306488,0.6482349421619467,0.03943173743006007,mmlu_offline:llama2-7b_chat:sociology,test,6.149154907092452
11,0.5454545617103577,0.5454545617103577,0.44999999999999996,0.2599431818181818,mmlu_offline:llama2-7b_chat:us_foreign_policy,validation,0.5172238298691809
100,0.550000011920929,0.550000011920929,0.48262626262626257,0.07828126668930055,mmlu_offline:llama2-7b_chat:us_foreign_policy,test,3.175562467891723
18,0.2777777910232544,0.5555555820465088,0.576923076923077,0.27495661046769887,mmlu_offline:llama2-7b_chat:virology,validation,0.8772573601454496
166,0.34337350726127625,0.4518072307109833,0.5525511025269596,0.15961501971784847,mmlu_offline:llama2-7b_chat:virology,test,5.020189558155835
19,0.6315789222717285,0.6315789222717285,0.6607142857142857,0.08984374058874031,mmlu_offline:llama2-7b_chat:world_religions,validation,0.6522464798763394
171,0.5380116701126099,0.5614035129547119,0.5975509080902587,0.07805646093268147,mmlu_offline:llama2-7b_chat:world_religions,test,4.199247566983104
