N,acc,unc_acc,unc_auroc,unc_ece,dataset,split,ts
11,0.09090909361839294,0.9090909361839294,0.4,0.1843039881099354,mmlu_offline:llama2-13b_chat:abstract_algebra,validation,4.645863291807473
100,0.3499999940395355,0.6299999952316284,0.5896703296703296,0.17175782322883604,mmlu_offline:llama2-13b_chat:abstract_algebra,test,5.197656232863665
14,0.2857142984867096,0.2857142984867096,0.35,0.3434709821428571,mmlu_offline:llama2-13b_chat:anatomy,validation,0.6971747661009431
135,0.42222222685813904,0.4888888895511627,0.6273054430949168,0.11640624867545234,mmlu_offline:llama2-13b_chat:anatomy,test,5.3263141149654984
16,0.5,0.75,0.9375,0.12036133185029028,mmlu_offline:llama2-13b_chat:astronomy,validation,1.0173952621407807
152,0.5263158082962036,0.5789473652839661,0.6143229166666667,0.05995580083445498,mmlu_offline:llama2-13b_chat:astronomy,test,8.305029288865626
11,0.5454545617103577,0.7272727489471436,0.8,0.088778403672305,mmlu_offline:llama2-13b_chat:business_ethics,validation,0.9586941269226372
100,0.3199999928474426,0.5199999809265137,0.5471047794117647,0.11957031369209291,mmlu_offline:llama2-13b_chat:business_ethics,test,7.360329871997237
29,0.24137930572032928,0.7241379022598267,0.6915584415584415,0.1858835939703317,mmlu_offline:llama2-13b_chat:clinical_knowledge,validation,1.4811848322860897
265,0.2981131970882416,0.6264150738716125,0.664522934531101,0.08862028751733167,mmlu_offline:llama2-13b_chat:clinical_knowledge,test,12.288001401815563
16,0.25,0.5625,0.3541666666666667,0.1870117299258709,mmlu_offline:llama2-13b_chat:college_biology,validation,1.0937946359626949
144,0.3958333432674408,0.5763888955116272,0.5695704779189352,0.10424805184205375,mmlu_offline:llama2-13b_chat:college_biology,test,8.826898775063455
8,0.0,0.75,,0.15185546875,mmlu_offline:llama2-13b_chat:college_chemistry,validation,0.809090499766171
100,0.17000000178813934,0.7300000190734863,0.7494684620836286,0.06410156071186066,mmlu_offline:llama2-13b_chat:college_chemistry,test,6.981113645713776
11,0.27272728085517883,0.9090909361839294,0.75,0.37819602272727276,mmlu_offline:llama2-13b_chat:college_computer_science,validation,1.5368737713433802
100,0.23999999463558197,0.7599999904632568,0.7176535087719298,0.07300781369209289,mmlu_offline:llama2-13b_chat:college_computer_science,test,12.955441330559552
11,0.0,1.0,,0.17755682360042224,mmlu_offline:llama2-13b_chat:college_mathematics,validation,1.185092082247138
100,0.09000000357627869,0.7900000214576721,0.6288156288156288,0.015742175579071066,mmlu_offline:llama2-13b_chat:college_mathematics,test,9.023275249637663
22,0.5,0.5909090638160706,0.6363636363636364,0.06640623916279184,mmlu_offline:llama2-13b_chat:college_medicine,validation,1.5869383658282459
173,0.3988439440727234,0.6242774724960327,0.7045707915273132,0.030324254766365043,mmlu_offline:llama2-13b_chat:college_medicine,test,16.688543940894306
11,0.27272728085517883,0.7272727489471436,0.875,0.14524147727272727,mmlu_offline:llama2-13b_chat:college_physics,validation,1.0044040558859706
102,0.1568627506494522,0.7843137383460999,0.5933866279069767,0.03584558940401265,mmlu_offline:llama2-13b_chat:college_physics,test,7.69989512488246
11,0.5454545617103577,0.6363636255264282,0.8333333333333333,0.1786221428350969,mmlu_offline:llama2-13b_chat:computer_security,validation,0.8657672493718565
100,0.550000011920929,0.5299999713897705,0.5434343434343434,0.12273437500000002,mmlu_offline:llama2-13b_chat:computer_security,test,5.041594112291932
26,0.3461538553237915,0.5769230723381042,0.6666666666666667,0.051231973446332506,mmlu_offline:llama2-13b_chat:conceptual_physics,validation,1.2332684812135994
235,0.4553191363811493,0.6000000238418579,0.6356965537383177,0.054371678575556324,mmlu_offline:llama2-13b_chat:conceptual_physics,test,9.431008801795542
12,0.25,0.5,0.5925925925925926,0.07942706843217216,mmlu_offline:llama2-13b_chat:econometrics,validation,1.272754009347409
114,0.1315789520740509,0.5964912176132202,0.6622895622895624,0.08079770364259418,mmlu_offline:llama2-13b_chat:econometrics,test,10.381778457202017
16,0.25,0.875,0.9166666666666667,0.2524413876235485,mmlu_offline:llama2-13b_chat:electrical_engineering,validation,1.1164818610996008
145,0.24827586114406586,0.682758629322052,0.6174821610601426,0.10396013177674392,mmlu_offline:llama2-13b_chat:electrical_engineering,test,8.744146619923413
41,0.26829269528388977,0.7560975551605225,0.543939393939394,0.19778963269256966,mmlu_offline:llama2-13b_chat:elementary_mathematics,validation,2.8801994090899825
378,0.3174603283405304,0.6693121790885925,0.41698966408268734,0.24693079521416358,mmlu_offline:llama2-13b_chat:elementary_mathematics,test,24.479211651720107
14,0.2142857164144516,0.7857142686843872,0.5757575757575757,0.1291852763720921,mmlu_offline:llama2-13b_chat:formal_logic,validation,1.1836240398697555
126,0.261904776096344,0.6904761791229248,0.6081785597914631,0.012865816789960093,mmlu_offline:llama2-13b_chat:formal_logic,test,9.47997564310208
10,0.20000000298023224,0.800000011920929,0.75,0.10664063096046449,mmlu_offline:llama2-13b_chat:global_facts,validation,0.7008207868784666
100,0.12999999523162842,0.8299999833106995,0.56631299734748,0.07398437380790711,mmlu_offline:llama2-13b_chat:global_facts,test,5.409192483872175
32,0.3125,0.75,0.725,0.17431641370058057,mmlu_offline:llama2-13b_chat:high_school_biology,validation,2.0603625453077257
310,0.42258065938949585,0.5838709473609924,0.5819651157831891,0.04688759773008284,mmlu_offline:llama2-13b_chat:high_school_biology,test,18.875569096766412
22,0.1818181872367859,0.6818181872367859,0.4861111111111111,0.128018474036997,mmlu_offline:llama2-13b_chat:high_school_chemistry,validation,1.6817200118675828
203,0.1822660118341446,0.7635468244552612,0.6416476717681537,0.0852447798099424,mmlu_offline:llama2-13b_chat:high_school_chemistry,test,14.006765996105969
9,0.6666666865348816,0.8888888955116272,0.888888888888889,0.4318576256434122,mmlu_offline:llama2-13b_chat:high_school_computer_science,validation,1.149257973767817
100,0.4300000071525574,0.6000000238418579,0.5956752345981233,0.07949218750000002,mmlu_offline:llama2-13b_chat:high_school_computer_science,test,11.535056584049016
18,0.7777777910232544,0.4444444477558136,0.8392857142857142,0.22916664679845175,mmlu_offline:llama2-13b_chat:high_school_european_history,validation,9.307102219667286
165,0.7575757503509521,0.6484848260879517,0.7866,0.15532671682762375,mmlu_offline:llama2-13b_chat:high_school_european_history,test,86.07523513119668
22,0.5,0.5,0.5909090909090908,0.1914062608372082,mmlu_offline:llama2-13b_chat:high_school_geography,validation,1.1115588708780706
198,0.39393940567970276,0.5959596037864685,0.6689102564102565,0.0731139586429403,mmlu_offline:llama2-13b_chat:high_school_geography,test,8.413448201026767
21,0.523809552192688,0.8095238208770752,0.8909090909090909,0.23400299038205824,mmlu_offline:llama2-13b_chat:high_school_government_and_politics,validation,1.2592414990067482
193,0.6321243643760681,0.575129508972168,0.6120410990533364,0.05450534079358982,mmlu_offline:llama2-13b_chat:high_school_government_and_politics,test,9.884439641609788
43,0.4883720874786377,0.5581395626068115,0.672077922077922,0.039335017980531205,mmlu_offline:llama2-13b_chat:high_school_macroeconomics,validation,1.9907770729623735
390,0.3692307770252228,0.6512820720672607,0.6235179539295391,0.10591947222367308,mmlu_offline:llama2-13b_chat:high_school_macroeconomics,test,17.20474313898012
29,0.06896551698446274,0.8275862336158752,0.7037037037037037,0.08836207924218013,mmlu_offline:llama2-13b_chat:high_school_mathematics,validation,2.3585065053775907
270,0.10000000149011612,0.8518518805503845,0.6547020271300106,0.04105903020611518,mmlu_offline:llama2-13b_chat:high_school_mathematics,test,21.067225876264274
26,0.3076923191547394,0.6538461446762085,0.5763888888888888,0.10036057004561791,mmlu_offline:llama2-13b_chat:high_school_microeconomics,validation,1.266475364100188
238,0.36554622650146484,0.6344537734985352,0.6133439902565273,0.09957654836798918,mmlu_offline:llama2-13b_chat:high_school_microeconomics,test,10.485747678205371
17,0.29411765933036804,0.8823529481887817,0.9166666666666667,0.2051930147058824,mmlu_offline:llama2-13b_chat:high_school_physics,validation,1.4849106539040804
151,0.17218543589115143,0.6821191906929016,0.5447692307692308,0.035828844995688156,mmlu_offline:llama2-13b_chat:high_school_physics,test,11.320069311186671
60,0.6000000238418579,0.6166666746139526,0.6035879629629629,0.07923175692558292,mmlu_offline:llama2-13b_chat:high_school_psychology,validation,3.8726914720609784
545,0.5064220428466797,0.5871559381484985,0.6267240450406767,0.025516045531001674,mmlu_offline:llama2-13b_chat:high_school_psychology,test,34.40306026628241
23,0.17391304671764374,0.8695651888847351,0.7039473684210527,0.1705162913902946,mmlu_offline:llama2-13b_chat:high_school_statistics,validation,2.447036776691675
216,0.25925925374031067,0.7175925970077515,0.5858258928571428,0.07877603890719237,mmlu_offline:llama2-13b_chat:high_school_statistics,test,22.081545360852033
22,0.7727272510528564,0.6363636255264282,0.4764705882352941,0.06640625270930206,mmlu_offline:llama2-13b_chat:high_school_us_history,validation,8.861570301931351
204,0.6764705777168274,0.6617646813392639,0.6306543697848045,0.049555770030208675,mmlu_offline:llama2-13b_chat:high_school_us_history,test,81.49948114901781
26,0.6538461446762085,0.3461538553237915,0.39215686274509803,0.38296274267710173,mmlu_offline:llama2-13b_chat:high_school_world_history,validation,7.512317260261625
237,0.6033755540847778,0.5063291192054749,0.6721841987799435,0.1905821441095087,mmlu_offline:llama2-13b_chat:high_school_world_history,test,61.98474355507642
23,0.3913043439388275,0.6521739363670349,0.6349206349206349,0.1428328933923141,mmlu_offline:llama2-13b_chat:human_aging,validation,1.0651503200642765
223,0.3901345431804657,0.6188340783119202,0.5735716700473292,0.10898964421096939,mmlu_offline:llama2-13b_chat:human_aging,test,9.09476823778823
12,0.3333333432674408,0.6666666865348816,0.65625,0.10774738589922585,mmlu_offline:llama2-13b_chat:human_sexuality,validation,0.7160685299895704
131,0.5114504098892212,0.5343511700630188,0.5431436567164181,0.0694179544012055,mmlu_offline:llama2-13b_chat:human_sexuality,test,6.254707783926278
13,0.6153846383094788,0.3076923191547394,0.41250000000000003,0.2854567261842581,mmlu_offline:llama2-13b_chat:international_law,validation,0.9471992142498493
121,0.6280992031097412,0.41322314739227295,0.4833333333333334,0.1919873544007293,mmlu_offline:llama2-13b_chat:international_law,test,7.582877574022859
11,0.1818181872367859,0.5454545617103577,0.25,0.1306818290190263,mmlu_offline:llama2-13b_chat:jurisprudence,validation,0.7408638522028923
108,0.3611111044883728,0.5555555820465088,0.5486807878112225,0.02419704088458305,mmlu_offline:llama2-13b_chat:jurisprudence,test,5.478856062982231
18,0.6666666865348816,0.4444444477558136,0.5208333333333334,0.2701822883552975,mmlu_offline:llama2-13b_chat:logical_fallacies,validation,1.1109489621594548
163,0.48466256260871887,0.5828220844268799,0.6235684147076552,0.01857265665487282,mmlu_offline:llama2-13b_chat:logical_fallacies,test,8.945222212933004
11,0.27272728085517883,0.6363636255264282,0.6041666666666666,0.15980113094503232,mmlu_offline:llama2-13b_chat:machine_learning,validation,1.029707645997405
112,0.2946428656578064,0.6875,0.6359800537015727,0.129568912088871,mmlu_offline:llama2-13b_chat:machine_learning,test,9.169554266147316
11,0.6363636255264282,0.3636363744735718,0.25,0.22052556818181818,mmlu_offline:llama2-13b_chat:management,validation,0.5953025789931417
103,0.42718446254730225,0.6019417643547058,0.6259630200308167,0.12545509766606452,mmlu_offline:llama2-13b_chat:management,test,3.8780348100699484
25,0.23999999463558197,0.3199999928474426,0.5394736842105263,0.36031250476837157,mmlu_offline:llama2-13b_chat:marketing,validation,1.5238299677148461
234,0.44871795177459717,0.5213675498962402,0.6747877445551864,0.11905713570423614,mmlu_offline:llama2-13b_chat:marketing,test,11.77373027894646
11,0.9090909361839294,0.8181818127632141,1.0,0.23153410174629904,mmlu_offline:llama2-13b_chat:medical_genetics,validation,0.6248582149855793
100,0.44999998807907104,0.5799999833106995,0.6608080808080808,0.05769529283046725,mmlu_offline:llama2-13b_chat:medical_genetics,test,3.942680197302252
86,0.5581395626068115,0.6279069781303406,0.6833881578947368,0.1551144656746887,mmlu_offline:llama2-13b_chat:miscellaneous,validation,3.4098401279188693
783,0.618135392665863,0.5427841544151306,0.6091171674175627,0.14996906989378947,mmlu_offline:llama2-13b_chat:miscellaneous,test,31.418718670960516
38,0.42105263471603394,0.5789473652839661,0.5383522727272727,0.0725740210006112,mmlu_offline:llama2-13b_chat:moral_disputes,validation,2.2507707052864134
346,0.43063583970069885,0.5202311873435974,0.5475590229278098,0.026259949958393338,mmlu_offline:llama2-13b_chat:moral_disputes,test,18.621811917982996
100,0.4300000071525574,0.6600000262260437,0.6540187678498572,0.07164062798023223,mmlu_offline:llama2-13b_chat:moral_scenarios,validation,9.975683552213013
895,0.38100558519363403,0.6804469227790833,0.7193723069756609,0.05454345202312788,mmlu_offline:llama2-13b_chat:moral_scenarios,test,87.80604002717882
33,0.3333333432674408,0.6666666865348816,0.6900826446280992,0.10381156206130981,mmlu_offline:llama2-13b_chat:nutrition,validation,2.318082614336163
306,0.4542483687400818,0.5620915293693542,0.5730625080773705,0.05713848019737044,mmlu_offline:llama2-13b_chat:nutrition,test,20.13708402076736
34,0.3235294222831726,0.47058823704719543,0.5079051383399209,0.09064799196579876,mmlu_offline:llama2-13b_chat:philosophy,validation,1.6492112101987004
311,0.3633440434932709,0.5273311734199524,0.57864038616251,0.024643299663948466,mmlu_offline:llama2-13b_chat:philosophy,test,13.527236296329647
35,0.37142857909202576,0.2857142984867096,0.4388111888111888,0.31551339115415306,mmlu_offline:llama2-13b_chat:prehistory,validation,2.0476936027407646
324,0.4413580298423767,0.5586419701576233,0.6731058996252367,0.03841145538989407,mmlu_offline:llama2-13b_chat:prehistory,test,16.97494865814224
31,0.16129031777381897,0.774193525314331,0.6076923076923078,0.12777217549662437,mmlu_offline:llama2-13b_chat:professional_accounting,validation,3.520617305766791
282,0.1879432648420334,0.7482269406318665,0.5827634506055862,0.06819314462073309,mmlu_offline:llama2-13b_chat:professional_accounting,test,31.027834184933454
170,0.38235294818878174,0.6176470518112183,0.42205128205128206,0.15305607914924627,mmlu_offline:llama2-13b_chat:professional_law,validation,41.988502827938646
1534,0.34810951352119446,0.6473272442817688,0.40160955056179776,0.17806287045739932,mmlu_offline:llama2-13b_chat:professional_law,test,388.55593105824664
31,0.35483869910240173,0.6774193644523621,0.7454545454545454,0.06224797425731535,mmlu_offline:llama2-13b_chat:professional_medicine,validation,6.1735633560456336
272,0.2904411852359772,0.5808823704719543,0.49337574604840295,0.05948413842741185,mmlu_offline:llama2-13b_chat:professional_medicine,test,54.135766556020826
69,0.37681159377098083,0.6666666865348816,0.6650268336314848,0.09816577123559041,mmlu_offline:llama2-13b_chat:professional_psychology,validation,4.851246217731386
612,0.3464052379131317,0.6192810535430908,0.5772936320754718,0.05591936234165637,mmlu_offline:llama2-13b_chat:professional_psychology,test,39.465213134884834
12,0.1666666716337204,0.4166666567325592,0.3,0.18587239583333334,mmlu_offline:llama2-13b_chat:public_relations,validation,0.808501270134002
110,0.30909091234207153,0.4636363685131073,0.5899767801857585,0.14098011526194484,mmlu_offline:llama2-13b_chat:public_relations,test,5.742350632790476
27,0.5555555820465088,0.4444444477558136,0.5055555555555555,0.1429398126072354,mmlu_offline:llama2-13b_chat:security_studies,validation,2.2596047669649124
245,0.6612244844436646,0.41632652282714844,0.604975457385096,0.1720663199619371,mmlu_offline:llama2-13b_chat:security_studies,test,19.746042936109006
22,0.5909090638160706,0.5,0.6239316239316239,0.0719104978171262,mmlu_offline:llama2-13b_chat:sociology,validation,1.1835908177308738
201,0.447761207818985,0.611940324306488,0.6416916916916917,0.06864114928601393,mmlu_offline:llama2-13b_chat:sociology,test,9.590683284215629
11,0.7272727489471436,0.3636363744735718,0.37500000000000006,0.2212357900359414,mmlu_offline:llama2-13b_chat:us_foreign_policy,validation,0.7192683941684663
100,0.5899999737739563,0.5,0.5680033071517157,0.0971484559774399,mmlu_offline:llama2-13b_chat:us_foreign_policy,test,4.938510286156088
18,0.5555555820465088,0.4444444477558136,0.5375000000000001,0.23480902446640864,mmlu_offline:llama2-13b_chat:virology,validation,1.2587988190352917
166,0.34939759969711304,0.5903614163398743,0.5577905491698595,0.05609939112720723,mmlu_offline:llama2-13b_chat:virology,test,7.716532732360065
19,0.6315789222717285,0.4736842215061188,0.43452380952380953,0.22697369048469945,mmlu_offline:llama2-13b_chat:world_religions,validation,0.8866057060658932
171,0.6081871390342712,0.5321637392044067,0.5642221584385764,0.12187042884659346,mmlu_offline:llama2-13b_chat:world_religions,test,6.208713169209659
