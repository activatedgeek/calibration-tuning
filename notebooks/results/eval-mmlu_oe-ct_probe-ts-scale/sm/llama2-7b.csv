N,acc,unc_acc,unc_auroc,unc_ece,dataset,split,ts
11,0.1818181872367859,0.8181818127632141,0.4722222222222222,0.20241477814587677,mmlu_offline:llama2-7b:abstract_algebra,validation,3.3949600881896913
100,0.2199999988079071,0.7799999713897705,0.5923659673659672,0.17210937500000006,mmlu_offline:llama2-7b:abstract_algebra,test,4.308665970806032
14,0.2857142984867096,0.7142857313156128,0.5,0.2064732142857143,mmlu_offline:llama2-7b:anatomy,validation,0.7895320663228631
135,0.4148148000240326,0.585185170173645,0.5,0.07737268518518514,mmlu_offline:llama2-7b:anatomy,test,5.1073601199314
16,0.4375,0.5625,0.5555555555555556,0.031249999999999993,mmlu_offline:llama2-7b:astronomy,validation,0.906624504365027
152,0.5065789222717285,0.4934210479259491,0.5198268398268399,0.03207234489290336,mmlu_offline:llama2-7b:astronomy,test,6.379492834210396
11,0.6363636255264282,0.3636363744735718,0.5,0.14417613636363635,mmlu_offline:llama2-7b:business_ethics,validation,0.8280636100098491
100,0.30000001192092896,0.699999988079071,0.5,0.19218749999999996,mmlu_offline:llama2-7b:business_ethics,test,5.189956923015416
29,0.24137930572032928,0.7586206793785095,0.42857142857142855,0.24905712440096095,mmlu_offline:llama2-7b:clinical_knowledge,validation,1.4245695299468935
265,0.35849055647850037,0.6415094137191772,0.4988235294117648,0.13957842183562946,mmlu_offline:llama2-7b:clinical_knowledge,test,10.420809222385287
16,0.3125,0.6875,0.5,0.1796875,mmlu_offline:llama2-7b:college_biology,validation,0.9158107037656009
144,0.2986111044883728,0.7013888955116272,0.5,0.19357638888888884,mmlu_offline:llama2-7b:college_biology,test,6.5994890639558434
8,0.125,0.875,0.5714285714285714,0.32324218749999994,mmlu_offline:llama2-7b:college_chemistry,validation,0.5708181369118392
100,0.14000000059604645,0.8600000143051147,0.6852159468438538,0.2409765589237213,mmlu_offline:llama2-7b:college_chemistry,test,5.131175386253744
11,0.0,1.0,,0.42223011363636365,mmlu_offline:llama2-7b:college_computer_science,validation,1.1520303329452872
100,0.17000000178813934,0.8299999833106995,0.5644932671863926,0.23898435950279234,mmlu_offline:llama2-7b:college_computer_science,test,8.638260619249195
11,0.0,1.0,,0.21981533007188275,mmlu_offline:llama2-7b:college_mathematics,validation,0.9521243069320917
100,0.1599999964237213,0.8399999737739563,0.6391369047619048,0.1282031273841858,mmlu_offline:llama2-7b:college_mathematics,test,6.142833499703556
22,0.3636363744735718,0.6363636255264282,0.5714285714285714,0.11470172080126678,mmlu_offline:llama2-7b:college_medicine,validation,1.3016218319535255
173,0.36994218826293945,0.6300578117370605,0.5002866972477065,0.12247110722381946,mmlu_offline:llama2-7b:college_medicine,test,11.247375351842493
11,0.27272728085517883,0.7272727489471436,0.75,0.09943181818181815,mmlu_offline:llama2-7b:college_physics,validation,0.9318193830549717
102,0.14705882966518402,0.8529411554336548,0.7195402298850575,0.2017080398166881,mmlu_offline:llama2-7b:college_physics,test,5.69382932363078
11,0.7272727489471436,0.27272728085517883,0.5,0.2350852272727273,mmlu_offline:llama2-7b:computer_security,validation,0.7931461329571903
100,0.4300000071525574,0.5699999928474426,0.4883720930232558,0.07515625000000004,mmlu_offline:llama2-7b:computer_security,test,4.253560143057257
26,0.3076923191547394,0.692307710647583,0.5277777777777778,0.17187500000000006,mmlu_offline:llama2-7b:conceptual_physics,validation,1.3060167636722326
235,0.4127659499645233,0.5872340202331543,0.5264828925743315,0.06896606455457974,mmlu_offline:llama2-7b:conceptual_physics,test,9.122991365846246
12,0.0833333358168602,0.9166666865348816,0.5,0.40885416666666663,mmlu_offline:llama2-7b:econometrics,validation,0.9903023303486407
114,0.14912280440330505,0.8508771657943726,0.5824742268041236,0.297560309108935,mmlu_offline:llama2-7b:econometrics,test,7.087706481106579
16,0.125,0.875,0.6428571428571428,0.281005859375,mmlu_offline:llama2-7b:electrical_engineering,validation,0.8887117872945964
145,0.1862068921327591,0.8068965673446655,0.41666666666666663,0.29574351228516677,mmlu_offline:llama2-7b:electrical_engineering,test,8.726074323989451
41,0.26829269528388977,0.7317073345184326,0.5378787878787878,0.09603657955076639,mmlu_offline:llama2-7b:elementary_mathematics,validation,2.2741556307300925
378,0.32275131344795227,0.6772486567497253,0.44592085040983603,0.2178922686627302,mmlu_offline:llama2-7b:elementary_mathematics,test,17.90720142237842
14,0.5,0.5,0.6428571428571428,0.11607142857142856,mmlu_offline:llama2-7b:formal_logic,validation,0.9516718941740692
126,0.30158731341362,0.6984127163887024,0.5482954545454546,0.1704488808200473,mmlu_offline:llama2-7b:formal_logic,test,6.711841273587197
10,0.30000001192092896,0.699999988079071,0.5,0.19218749999999996,mmlu_offline:llama2-7b:global_facts,validation,0.7387719890102744
100,0.17000000178813934,0.8299999833106995,0.5301204819277108,0.3087890625,mmlu_offline:llama2-7b:global_facts,test,4.377379131037742
32,0.15625,0.84375,0.537037037037037,0.31860351562500006,mmlu_offline:llama2-7b:high_school_biology,validation,1.583441385999322
310,0.41290321946144104,0.5870967507362366,0.4949776785714286,0.08523183484231275,mmlu_offline:llama2-7b:high_school_biology,test,13.883917181752622
22,0.13636364042758942,0.8636363744735718,0.2631578947368421,0.341796875,mmlu_offline:llama2-7b:high_school_chemistry,validation,1.3235120428726077
203,0.16748768091201782,0.8325123190879822,0.5531674208144796,0.2657019895285808,mmlu_offline:llama2-7b:high_school_chemistry,test,10.072217050008476
9,0.2222222238779068,0.7777777910232544,0.5714285714285714,0.23697916666666666,mmlu_offline:llama2-7b:high_school_computer_science,validation,1.040213467553258
100,0.3700000047683716,0.6299999952316284,0.4886314886314887,0.15027344584465027,mmlu_offline:llama2-7b:high_school_computer_science,test,7.724106926936656
18,0.7222222089767456,0.2777777910232544,0.5,0.2300347222222222,mmlu_offline:llama2-7b:high_school_european_history,validation,6.0284068789333105
165,0.6727272868156433,0.3272727131843567,0.5,0.18053977272727273,mmlu_offline:llama2-7b:high_school_european_history,test,53.254769368097186
22,0.4545454680919647,0.5454545617103577,0.5,0.037642045454545414,mmlu_offline:llama2-7b:high_school_geography,validation,1.1239451570436358
198,0.3636363744735718,0.6363636255264282,0.503968253968254,0.12709122474747475,mmlu_offline:llama2-7b:high_school_geography,test,7.738094750326127
21,0.4285714328289032,0.5714285969734192,0.5,0.0636160714285714,mmlu_offline:llama2-7b:high_school_government_and_politics,validation,1.1863844990730286
193,0.48704662919044495,0.5129533410072327,0.5,0.005140867875647714,mmlu_offline:llama2-7b:high_school_government_and_politics,test,8.108323250431567
43,0.41860464215278625,0.5813953280448914,0.5144444444444445,0.07567223837209298,mmlu_offline:llama2-7b:high_school_macroeconomics,validation,2.046799398958683
390,0.34358975291252136,0.656410276889801,0.5120539878731344,0.1476562503056648,mmlu_offline:llama2-7b:high_school_macroeconomics,test,15.211980174761266
29,0.03448275849223137,0.9655172228813171,0.9642857142857143,0.12122845855252495,mmlu_offline:llama2-7b:high_school_mathematics,validation,1.8318268349394202
270,0.08148147910833359,0.9185185432434082,0.5468291788856304,0.0969039186283394,mmlu_offline:llama2-7b:high_school_mathematics,test,14.419567617122084
26,0.3076923191547394,0.692307710647583,0.4375,0.23662860576923075,mmlu_offline:llama2-7b:high_school_microeconomics,validation,1.28750732075423
238,0.32773110270500183,0.6722689270973206,0.503125,0.16276588760504204,mmlu_offline:llama2-7b:high_school_microeconomics,test,9.32517223013565
17,0.23529411852359772,0.7647058963775635,0.7307692307692308,0.16452207284815168,mmlu_offline:llama2-7b:high_school_physics,validation,1.210243167821318
151,0.17880794405937195,0.8211920261383057,0.46580047789725204,0.2506725819695074,mmlu_offline:llama2-7b:high_school_physics,test,8.022156223189086
60,0.5,0.5,0.5,0.0078125,mmlu_offline:llama2-7b:high_school_psychology,validation,2.912795389071107
545,0.5064220428466797,0.4935779869556427,0.507475351543559,0.018219605279624993,mmlu_offline:llama2-7b:high_school_psychology,test,25.12913193833083
23,0.1304347813129425,0.8695651888847351,0.725,0.24082879916481348,mmlu_offline:llama2-7b:high_school_statistics,validation,1.8069333252497017
216,0.24074074625968933,0.7592592835426331,0.5796200750469043,0.17446107776076702,mmlu_offline:llama2-7b:high_school_statistics,test,14.828692792914808
22,0.5909090638160706,0.40909090638160706,0.5,0.09872159090909088,mmlu_offline:llama2-7b:high_school_us_history,validation,5.764654697850347
204,0.5980392098426819,0.4019607901573181,0.5,0.1058517156862745,mmlu_offline:llama2-7b:high_school_us_history,test,50.88095057476312
26,0.5384615659713745,0.4615384638309479,0.5,0.046274038461538436,mmlu_offline:llama2-7b:high_school_world_history,validation,4.674043283332139
237,0.4345991611480713,0.5654008388519287,0.5,0.05758834388185652,mmlu_offline:llama2-7b:high_school_world_history,test,39.36367349186912
23,0.21739129722118378,0.782608687877655,0.5,0.27479619565217395,mmlu_offline:llama2-7b:human_aging,validation,1.1058464259840548
223,0.340807169675827,0.6591928005218506,0.4967776584317937,0.156074831838565,mmlu_offline:llama2-7b:human_aging,test,8.398203720804304
12,0.4166666567325592,0.5833333134651184,0.5,0.07552083333333337,mmlu_offline:llama2-7b:human_sexuality,validation,0.7135629178956151
131,0.47328245639801025,0.5267175436019897,0.4838709677419355,0.028625959658440742,mmlu_offline:llama2-7b:human_sexuality,test,5.219314446207136
13,0.23076923191547394,0.7692307829856873,0.5,0.2614182692307693,mmlu_offline:llama2-7b:international_law,validation,0.8352615633048117
121,0.4876033067703247,0.5123966932296753,0.5080645161290323,0.0033251549586776567,mmlu_offline:llama2-7b:international_law,test,5.549747119192034
11,0.4545454680919647,0.5454545617103577,0.5,0.037642045454545414,mmlu_offline:llama2-7b:jurisprudence,validation,0.728239051066339
108,0.37037035822868347,0.6296296119689941,0.5,0.12181712962962965,mmlu_offline:llama2-7b:jurisprudence,test,4.484569967724383
18,0.4444444477558136,0.5555555820465088,0.55,0.03819444444444445,mmlu_offline:llama2-7b:logical_fallacies,validation,1.0961445728316903
163,0.42944785952568054,0.5705521702766418,0.5,0.06273964723926384,mmlu_offline:llama2-7b:logical_fallacies,test,6.961695122998208
11,0.27272728085517883,0.7272727489471436,0.625,0.15909090909090906,mmlu_offline:llama2-7b:machine_learning,validation,0.8611301369965076
112,0.2857142984867096,0.7142857313156128,0.58125,0.17138671928218435,mmlu_offline:llama2-7b:machine_learning,test,6.439022897742689
11,0.4545454680919647,0.5454545617103577,0.5,0.037642045454545414,mmlu_offline:llama2-7b:management,validation,0.7317451578564942
103,0.4563106894493103,0.5436893105506897,0.5089285714285714,0.033108313106796086,mmlu_offline:llama2-7b:management,test,3.649082721211016
25,0.2800000011920929,0.7200000286102295,0.5,0.21218749999999997,mmlu_offline:llama2-7b:marketing,validation,1.4520651022903621
234,0.4572649598121643,0.5427350401878357,0.5039370078740157,0.033954326923076976,mmlu_offline:llama2-7b:marketing,test,9.90337424678728
11,0.7272727489471436,0.27272728085517883,0.6666666666666667,0.2947443181818181,mmlu_offline:llama2-7b:medical_genetics,validation,0.6970640700310469
100,0.4699999988079071,0.5299999713897705,0.5373344038538739,0.013085921406745894,mmlu_offline:llama2-7b:medical_genetics,test,3.8035691492259502
86,0.5465116500854492,0.45348837971687317,0.5624659028914348,0.10655888083369233,mmlu_offline:llama2-7b:miscellaneous,validation,3.363322388846427
783,0.5874840617179871,0.4125159680843353,0.5171759321577601,0.10450089335593837,mmlu_offline:llama2-7b:miscellaneous,test,28.55351250199601
38,0.42105263471603394,0.5789473652839661,0.5,0.07113486842105265,mmlu_offline:llama2-7b:moral_disputes,validation,1.9053439418785274
346,0.41040462255477905,0.589595377445221,0.5,0.08178287572254339,mmlu_offline:llama2-7b:moral_disputes,test,14.513882644940168
100,0.3499999940395355,0.6499999761581421,0.5,0.14218750000000002,mmlu_offline:llama2-7b:moral_scenarios,validation,6.580002656206489
895,0.3173184394836426,0.6826815605163574,0.5,0.17486906424581006,mmlu_offline:llama2-7b:moral_scenarios,test,57.423977514728904
33,0.3636363744735718,0.6363636255264282,0.5476190476190477,0.1112689393939394,mmlu_offline:llama2-7b:nutrition,validation,1.8260353561490774
306,0.38235294818878174,0.6176470518112183,0.5037308370641704,0.10802185476994987,mmlu_offline:llama2-7b:nutrition,test,14.118359716143459
34,0.29411765933036804,0.7058823704719543,0.5,0.19806985294117652,mmlu_offline:llama2-7b:philosophy,validation,1.6415798161178827
311,0.3247588276863098,0.6752411723136902,0.5,0.16742865755627012,mmlu_offline:llama2-7b:philosophy,test,11.88074653595686
35,0.3142857253551483,0.6857143044471741,0.5,0.17790178571428572,mmlu_offline:llama2-7b:prehistory,validation,1.7833704110234976
324,0.4166666567325592,0.5833333134651184,0.5026455026455027,0.07529177268346154,mmlu_offline:llama2-7b:prehistory,test,13.27245662221685
31,0.12903225421905518,0.8709677457809448,0.5648148148148149,0.2968749788499648,mmlu_offline:llama2-7b:professional_accounting,validation,2.421459499746561
282,0.152482271194458,0.847517728805542,0.5424734844799067,0.24125943708081615,mmlu_offline:llama2-7b:professional_accounting,test,20.569892356172204
170,0.3117647171020508,0.6882352828979492,0.5,0.18042279411764706,mmlu_offline:llama2-7b:professional_law,validation,27.097538262140006
1534,0.2777053415775299,0.7222946286201477,0.5,0.2144821544980443,mmlu_offline:llama2-7b:professional_law,test,248.8864126917906
31,0.4193548262119293,0.5806451439857483,0.5,0.07283266129032262,mmlu_offline:llama2-7b:professional_medicine,validation,4.045872996095568
272,0.27941176295280457,0.720588207244873,0.5076530612244898,0.20902745863970587,mmlu_offline:llama2-7b:professional_medicine,test,34.87784491898492
69,0.3913043439388275,0.6086956262588501,0.5,0.10088315217391308,mmlu_offline:llama2-7b:professional_psychology,validation,3.610521259251982
612,0.30882352590560913,0.6911764740943909,0.49971856354835076,0.18374693627450986,mmlu_offline:llama2-7b:professional_psychology,test,28.43659876100719
12,0.4166666567325592,0.5833333134651184,0.5,0.07552083333333337,mmlu_offline:llama2-7b:public_relations,validation,0.8151597301475704
110,0.3181818127632141,0.6818181872367859,0.5,0.17400568181818177,mmlu_offline:llama2-7b:public_relations,test,4.658522091340274
27,0.5925925970077515,0.40740740299224854,0.5,0.10040509259259262,mmlu_offline:llama2-7b:security_studies,validation,1.7370800701901317
245,0.5306122303009033,0.4693877696990967,0.5,0.03842474489795916,mmlu_offline:llama2-7b:security_studies,test,13.455057201907039
22,0.3181818127632141,0.6818181872367859,0.5,0.17400568181818177,mmlu_offline:llama2-7b:sociology,validation,1.0384630630724132
201,0.3781094551086426,0.6218905448913574,0.504,0.11273709577114428,mmlu_offline:llama2-7b:sociology,test,8.142258792184293
11,0.6363636255264282,0.3636363744735718,0.5,0.14417613636363635,mmlu_offline:llama2-7b:us_foreign_policy,validation,0.8139924299903214
100,0.5799999833106995,0.41999998688697815,0.5,0.08781250000000002,mmlu_offline:llama2-7b:us_foreign_policy,test,4.317202966660261
18,0.5,0.5,0.45679012345679015,0.10980902446640863,mmlu_offline:llama2-7b:virology,validation,1.132253848016262
166,0.3313252925872803,0.6686747074127197,0.4954954954954955,0.1662509412650603,mmlu_offline:llama2-7b:virology,test,6.555759973824024
19,0.7368420958518982,0.2631579041481018,0.6,0.2787828947368421,mmlu_offline:llama2-7b:world_religions,validation,0.9886651327833533
171,0.5847952961921692,0.4152046740055084,0.5070422535211268,0.09683388157894737,mmlu_offline:llama2-7b:world_religions,test,6.1026874599047005
