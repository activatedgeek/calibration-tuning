N,acc,unc_acc,unc_auroc,unc_ece,dataset,split,ts
11,0.09090909361839294,0.9090909361839294,0.30000000000000004,0.25142045454545453,mmlu_offline:llama2-13b_chat:abstract_algebra,validation,2.789068599231541
100,0.3499999940395355,0.6100000143051147,0.5718681318681319,0.1023437476158142,mmlu_offline:llama2-13b_chat:abstract_algebra,test,5.181763065047562
14,0.2857142984867096,0.5,0.375,0.12444196428571427,mmlu_offline:llama2-13b_chat:anatomy,validation,0.6730364738032222
135,0.42222222685813904,0.6962962746620178,0.7379667116509222,0.17248263579827772,mmlu_offline:llama2-13b_chat:anatomy,test,5.356431399937719
16,0.5,0.5625,0.9375,0.2746581956744194,mmlu_offline:llama2-13b_chat:astronomy,validation,0.9973562662489712
152,0.5263158082962036,0.5197368264198303,0.6254340277777779,0.1410361951903293,mmlu_offline:llama2-13b_chat:astronomy,test,8.314734541345388
11,0.5454545617103577,0.6363636255264282,0.5333333333333333,0.3483664772727273,mmlu_offline:llama2-13b_chat:business_ethics,validation,0.9391598352231085
100,0.3199999928474426,0.47999998927116394,0.49862132352941174,0.16710937559604647,mmlu_offline:llama2-13b_chat:business_ethics,test,7.338595161680132
29,0.24137930572032928,0.7586206793785095,0.7792207792207791,0.133081906828387,mmlu_offline:llama2-13b_chat:clinical_knowledge,validation,1.4617275428026915
265,0.2981131970882416,0.6905660629272461,0.6858581734041105,0.08503831759938654,mmlu_offline:llama2-13b_chat:clinical_knowledge,test,12.266432176809758
16,0.25,0.75,0.625,0.22924804314970967,mmlu_offline:llama2-13b_chat:college_biology,validation,1.073249306064099
144,0.3958333432674408,0.5902777910232544,0.5907441016333937,0.06640626821253037,mmlu_offline:llama2-13b_chat:college_biology,test,8.815671957097948
8,0.0,1.0,,0.2592773362994194,mmlu_offline:llama2-13b_chat:college_chemistry,validation,0.6294253366068006
100,0.17000000178813934,0.7900000214576721,0.6906449326718639,0.09980469882488249,mmlu_offline:llama2-13b_chat:college_chemistry,test,6.966807794757187
11,0.27272728085517883,0.5454545617103577,0.47916666666666663,0.20738635821775958,mmlu_offline:llama2-13b_chat:college_computer_science,validation,1.5379233555868268
100,0.23999999463558197,0.5899999737739563,0.643640350877193,0.19472656905651095,mmlu_offline:llama2-13b_chat:college_computer_science,test,12.940000234171748
11,0.0,0.9090909361839294,,0.13991478356448084,mmlu_offline:llama2-13b_chat:college_mathematics,validation,1.158895188011229
100,0.09000000357627869,0.7799999713897705,0.7142857142857143,0.05675780951976776,mmlu_offline:llama2-13b_chat:college_mathematics,test,9.004991790745407
22,0.5,0.5454545617103577,0.7024793388429751,0.13068181005391205,mmlu_offline:llama2-13b_chat:college_medicine,validation,1.5680584032088518
173,0.3988439440727234,0.6994219422340393,0.7063823857302117,0.11777457405376988,mmlu_offline:llama2-13b_chat:college_medicine,test,16.68398949597031
11,0.27272728085517883,0.6363636255264282,0.6666666666666666,0.29225851730866864,mmlu_offline:llama2-13b_chat:college_physics,validation,0.9788436670787632
102,0.1568627506494522,0.7941176295280457,0.6311773255813954,0.08030789798381281,mmlu_offline:llama2-13b_chat:college_physics,test,7.673534851055592
11,0.5454545617103577,0.6363636255264282,0.7666666666666667,0.1012073972008445,mmlu_offline:llama2-13b_chat:computer_security,validation,0.8179908450692892
100,0.550000011920929,0.5,0.5496969696969697,0.1606640601158142,mmlu_offline:llama2-13b_chat:computer_security,test,5.021109801251441
26,0.3461538553237915,0.6153846383094788,0.6830065359477124,0.11478366301609923,mmlu_offline:llama2-13b_chat:conceptual_physics,validation,1.2290105847641826
235,0.4553191363811493,0.6297872066497803,0.6606673481308412,0.03655252000118823,mmlu_offline:llama2-13b_chat:conceptual_physics,test,9.43045308208093
12,0.25,0.5833333134651184,0.7777777777777778,0.32454427083333337,mmlu_offline:llama2-13b_chat:econometrics,validation,1.2577414140105247
114,0.1315789520740509,0.4912280738353729,0.6208754208754208,0.18602659880069264,mmlu_offline:llama2-13b_chat:econometrics,test,10.392322952859104
16,0.25,0.6875,0.75,0.1455078125,mmlu_offline:llama2-13b_chat:electrical_engineering,validation,1.0928985099308193
145,0.24827586114406586,0.7241379022598267,0.6392711518858307,0.07190194541010363,mmlu_offline:llama2-13b_chat:electrical_engineering,test,8.71809790469706
41,0.26829269528388977,0.7560975551605225,0.665151515151515,0.10184832317073174,mmlu_offline:llama2-13b_chat:elementary_mathematics,validation,2.8632602686993778
378,0.3174603283405304,0.6693121790885925,0.6048126614987079,0.051318624820658756,mmlu_offline:llama2-13b_chat:elementary_mathematics,test,24.534337904769927
14,0.2142857164144516,0.5714285969734192,0.5909090909090908,0.29380581634385244,mmlu_offline:llama2-13b_chat:formal_logic,validation,1.1663191481493413
126,0.261904776096344,0.5873016119003296,0.5966112740306289,0.0482700779324486,mmlu_offline:llama2-13b_chat:formal_logic,test,9.480112673714757
10,0.20000000298023224,0.800000011920929,0.75,0.2222656190395355,mmlu_offline:llama2-13b_chat:global_facts,validation,0.6974330800585449
100,0.12999999523162842,0.8500000238418579,0.568081343943413,0.10394531846046448,mmlu_offline:llama2-13b_chat:global_facts,test,5.422432115767151
32,0.3125,0.65625,0.6045454545454545,0.18176270090043548,mmlu_offline:llama2-13b_chat:high_school_biology,validation,2.034582889638841
310,0.42258065938949585,0.6322580575942993,0.6333105889376947,0.036063498643136814,mmlu_offline:llama2-13b_chat:high_school_biology,test,18.91780239297077
22,0.1818181872367859,0.7272727489471436,0.6805555555555556,0.07972300594503232,mmlu_offline:llama2-13b_chat:high_school_chemistry,validation,1.6572127332910895
203,0.1822660118341446,0.7783251404762268,0.7162162162162162,0.07131310959754908,mmlu_offline:llama2-13b_chat:high_school_chemistry,test,13.995865968987346
9,0.6666666865348816,1.0,1.0,0.32942708333333337,mmlu_offline:llama2-13b_chat:high_school_computer_science,validation,1.1324772317893803
100,0.4300000071525574,0.5099999904632568,0.5505915952672379,0.14437501370906827,mmlu_offline:llama2-13b_chat:high_school_computer_science,test,11.53918988024816
18,0.7777777910232544,0.4444444477558136,0.7857142857142857,0.19900173279974198,mmlu_offline:llama2-13b_chat:high_school_european_history,validation,9.30441523808986
165,0.7575757503509521,0.5272727012634277,0.7927000000000001,0.11304451660676439,mmlu_offline:llama2-13b_chat:high_school_european_history,test,86.27973403362557
22,0.5,0.5909090638160706,0.6363636363636364,0.15056818994608795,mmlu_offline:llama2-13b_chat:high_school_geography,validation,1.0929142241366208
198,0.39393940567970276,0.6161616444587708,0.6857371794871795,0.0778487968926478,mmlu_offline:llama2-13b_chat:high_school_geography,test,8.38798921694979
21,0.523809552192688,0.523809552192688,0.5727272727272728,0.11867559239977883,mmlu_offline:llama2-13b_chat:high_school_government_and_politics,validation,1.24851763388142
193,0.6321243643760681,0.5440414547920227,0.6631262987762641,0.08719236739558878,mmlu_offline:llama2-13b_chat:high_school_government_and_politics,test,9.861182873137295
43,0.4883720874786377,0.604651153087616,0.7922077922077921,0.11019257334775705,mmlu_offline:llama2-13b_chat:high_school_macroeconomics,validation,1.9611977376043797
390,0.3692307770252228,0.6333333253860474,0.638648938572719,0.05120192552224184,mmlu_offline:llama2-13b_chat:high_school_macroeconomics,test,17.211848278995603
29,0.06896551698446274,0.6896551847457886,0.4444444444444445,0.14466594005453173,mmlu_offline:llama2-13b_chat:high_school_mathematics,validation,2.336004386190325
270,0.10000000149011612,0.7629629373550415,0.5835238530711782,0.07265625022075796,mmlu_offline:llama2-13b_chat:high_school_mathematics,test,21.047713652718812
26,0.3076923191547394,0.6538461446762085,0.638888888888889,0.08067906361359814,mmlu_offline:llama2-13b_chat:high_school_microeconomics,validation,1.2544377399608493
238,0.36554622650146484,0.6386554837226868,0.6684935677856436,0.029181977780927116,mmlu_offline:llama2-13b_chat:high_school_microeconomics,test,10.498167458921671
17,0.29411765933036804,0.8235294222831726,0.8916666666666667,0.14338234242270972,mmlu_offline:llama2-13b_chat:high_school_physics,validation,1.4555993941612542
151,0.17218543589115143,0.7417218685150146,0.48553846153846153,0.08764486162867767,mmlu_offline:llama2-13b_chat:high_school_physics,test,11.316749858204275
60,0.6000000238418579,0.5666666626930237,0.6221064814814814,0.12779946625232697,mmlu_offline:llama2-13b_chat:high_school_psychology,validation,3.851544543169439
545,0.5064220428466797,0.6385321021080017,0.7060839933193255,0.012980218546106212,mmlu_offline:llama2-13b_chat:high_school_psychology,test,34.41016606986523
23,0.17391304671764374,0.739130437374115,0.4671052631578948,0.16202445133872656,mmlu_offline:llama2-13b_chat:high_school_statistics,validation,2.4198427041992545
216,0.25925925374031067,0.6712962985038757,0.6758370535714286,0.07649739721307049,mmlu_offline:llama2-13b_chat:high_school_statistics,test,22.0813308050856
22,0.7727272510528564,0.5909090638160706,0.5058823529411764,0.06640625270930203,mmlu_offline:llama2-13b_chat:high_school_us_history,validation,8.869756842032075
204,0.6764705777168274,0.6421568393707275,0.6462999560825649,0.058938425837778564,mmlu_offline:llama2-13b_chat:high_school_us_history,test,81.70678445696831
26,0.6538461446762085,0.38461539149284363,0.4183006535947713,0.34074520147763765,mmlu_offline:llama2-13b_chat:high_school_world_history,validation,7.512360379099846
237,0.6033755540847778,0.5316455960273743,0.6789540246987056,0.18782964210470016,mmlu_offline:llama2-13b_chat:high_school_world_history,test,62.08139857277274
23,0.3913043439388275,0.6521739363670349,0.6825396825396826,0.09680705744287244,mmlu_offline:llama2-13b_chat:human_aging,validation,1.0434082658030093
223,0.3901345431804657,0.6547085046768188,0.6392832995267073,0.06435677903650054,mmlu_offline:llama2-13b_chat:human_aging,test,9.122697109822184
12,0.3333333432674408,0.5833333134651184,0.453125,0.16927083333333337,mmlu_offline:llama2-13b_chat:human_sexuality,validation,0.6974509079009295
131,0.5114504098892212,0.5648854970932007,0.5532882462686567,0.1343034392094794,mmlu_offline:llama2-13b_chat:human_sexuality,test,6.261602325830609
13,0.6153846383094788,0.38461539149284363,0.4,0.20943509615384615,mmlu_offline:llama2-13b_chat:international_law,validation,0.9243362960405648
121,0.6280992031097412,0.4958677589893341,0.5128654970760234,0.11457256886584702,mmlu_offline:llama2-13b_chat:international_law,test,7.544593614991754
11,0.1818181872367859,0.8181818127632141,0.8333333333333334,0.1218039664355191,mmlu_offline:llama2-13b_chat:jurisprudence,validation,0.7201450881548226
108,0.3611111044883728,0.5833333134651184,0.4704570791527313,0.07468894289599522,mmlu_offline:llama2-13b_chat:jurisprudence,test,5.47226061206311
18,0.6666666865348816,0.5,0.576388888888889,0.22374131944444445,mmlu_offline:llama2-13b_chat:logical_fallacies,validation,1.09890810912475
163,0.48466256260871887,0.5398772954940796,0.5912447257383966,0.06726899176287505,mmlu_offline:llama2-13b_chat:logical_fallacies,test,8.938881461974233
11,0.27272728085517883,0.5454545617103577,0.41666666666666663,0.15127840367230502,mmlu_offline:llama2-13b_chat:machine_learning,validation,1.0131382518447936
112,0.2946428656578064,0.6785714030265808,0.6166091292673571,0.08164759459240095,mmlu_offline:llama2-13b_chat:machine_learning,test,9.178039743099362
11,0.6363636255264282,0.5454545617103577,0.6428571428571428,0.1424005627632141,mmlu_offline:llama2-13b_chat:management,validation,0.5579134458675981
103,0.42718446254730225,0.6601941585540771,0.6587057010785824,0.12837530397674415,mmlu_offline:llama2-13b_chat:management,test,3.875042590778321
25,0.23999999463558197,0.3199999928474426,0.5701754385964913,0.32093749284744266,mmlu_offline:llama2-13b_chat:marketing,validation,1.504963104147464
234,0.44871795177459717,0.5897436141967773,0.7216315983757844,0.07047944063814278,mmlu_offline:llama2-13b_chat:marketing,test,11.748125434853137
11,0.9090909361839294,0.6363636255264282,0.9,0.13387783549048685,mmlu_offline:llama2-13b_chat:medical_genetics,validation,0.6020703292451799
100,0.44999998807907104,0.6000000238418579,0.689090909090909,0.08269530892372132,mmlu_offline:llama2-13b_chat:medical_genetics,test,3.9451981647871435
86,0.5581395626068115,0.6511628031730652,0.7179276315789473,0.08525617316711784,mmlu_offline:llama2-13b_chat:miscellaneous,validation,3.4108996661379933
783,0.618135392665863,0.6015325784683228,0.646058486967578,0.07816989333541308,mmlu_offline:llama2-13b_chat:miscellaneous,test,31.477633879054338
38,0.42105263471603394,0.5789473652839661,0.7286931818181818,0.10197368107343976,mmlu_offline:llama2-13b_chat:moral_disputes,validation,2.2292610122822225
346,0.43063583970069885,0.6069363951683044,0.6206690968555173,0.044594464102232396,mmlu_offline:llama2-13b_chat:moral_disputes,test,18.690615468192846
100,0.4300000071525574,0.6000000238418579,0.6819665442676458,0.08167968928813936,mmlu_offline:llama2-13b_chat:moral_scenarios,validation,9.973437773995101
895,0.38100558519363403,0.6413407921791077,0.7332913389161206,0.059501569564116084,mmlu_offline:llama2-13b_chat:moral_scenarios,test,87.94902370823547
33,0.3333333432674408,0.7272727489471436,0.7107438016528925,0.08321495850880942,mmlu_offline:llama2-13b_chat:nutrition,validation,2.3034208370372653
306,0.4542483687400818,0.6307189464569092,0.6549993538103649,0.06567862240317598,mmlu_offline:llama2-13b_chat:nutrition,test,20.12331549404189
34,0.3235294222831726,0.7647058963775635,0.6818181818181819,0.1389016614240758,mmlu_offline:llama2-13b_chat:philosophy,validation,1.67535598622635
311,0.3633440434932709,0.6816720366477966,0.6906006972378654,0.05316769865931424,mmlu_offline:llama2-13b_chat:philosophy,test,13.503225850872695
35,0.37142857909202576,0.6571428775787354,0.4475524475524475,0.2621651836803981,mmlu_offline:llama2-13b_chat:prehistory,validation,2.0155866988934577
324,0.4413580298423767,0.6141975522041321,0.6712127651354169,0.020785101401953054,mmlu_offline:llama2-13b_chat:prehistory,test,16.833244892768562
31,0.16129031777381897,0.8064516186714172,0.5884615384615384,0.25541834677419356,mmlu_offline:llama2-13b_chat:professional_accounting,validation,3.492938709910959
282,0.1879432648420334,0.7517730593681335,0.5993243799950563,0.08897106677082413,mmlu_offline:llama2-13b_chat:professional_accounting,test,31.04146609734744
170,0.38235294818878174,0.6235294342041016,0.5093772893772893,0.08092832775676954,mmlu_offline:llama2-13b_chat:professional_law,validation,42.030559070874006
1534,0.34810951352119446,0.6531942486763,0.5305983146067416,0.07137181472902657,mmlu_offline:llama2-13b_chat:professional_law,test,389.16132623981684
31,0.35483869910240173,0.4838709533214569,0.5477272727272726,0.15990424540735063,mmlu_offline:llama2-13b_chat:professional_medicine,validation,6.158880852162838
272,0.2904411852359772,0.529411792755127,0.5800157408014691,0.11904010132831688,mmlu_offline:llama2-13b_chat:professional_medicine,test,54.250134313013405
69,0.37681159377098083,0.6521739363670349,0.6529516994633274,0.12199955487596814,mmlu_offline:llama2-13b_chat:professional_psychology,validation,4.84139617299661
612,0.3464052379131317,0.6503267884254456,0.6198997641509434,0.029373472032983343,mmlu_offline:llama2-13b_chat:professional_psychology,test,39.58766286633909
12,0.1666666716337204,0.6666666865348816,0.65,0.2568359375,mmlu_offline:llama2-13b_chat:public_relations,validation,0.7868468253873289
110,0.30909091234207153,0.5363636612892151,0.600812693498452,0.1262783977118406,mmlu_offline:llama2-13b_chat:public_relations,test,5.747682393994182
27,0.5555555820465088,0.48148149251937866,0.55,0.17173031524375634,mmlu_offline:llama2-13b_chat:security_studies,validation,2.2460449119098485
245,0.6612244844436646,0.559183657169342,0.7169418414398334,0.09248525881376424,mmlu_offline:llama2-13b_chat:security_studies,test,19.82851057872176
22,0.5909090638160706,0.40909090638160706,0.47863247863247865,0.17915484851056881,mmlu_offline:llama2-13b_chat:sociology,validation,1.1645852932706475
201,0.447761207818985,0.6616915464401245,0.7236236236236234,0.05433767678132694,mmlu_offline:llama2-13b_chat:sociology,test,9.638491595163941
11,0.7272727489471436,0.3636363744735718,0.4166666666666667,0.36257101730866864,mmlu_offline:llama2-13b_chat:us_foreign_policy,validation,0.6896148719824851
100,0.5899999737739563,0.5099999904632568,0.6335262505167425,0.12355470418930053,mmlu_offline:llama2-13b_chat:us_foreign_policy,test,4.956335274036974
18,0.5555555820465088,0.6111111044883728,0.7,0.19227431217829385,mmlu_offline:llama2-13b_chat:virology,validation,1.2465905179269612
166,0.34939759969711304,0.6385542154312134,0.5948275862068965,0.05880554576954209,mmlu_offline:llama2-13b_chat:virology,test,7.718684860039502
19,0.6315789222717285,0.5789473652839661,0.6190476190476191,0.12129935151652282,mmlu_offline:llama2-13b_chat:world_religions,validation,0.8521554782055318
171,0.6081871390342712,0.5906432867050171,0.5917049368541907,0.10380117760764226,mmlu_offline:llama2-13b_chat:world_religions,test,6.226431851275265
