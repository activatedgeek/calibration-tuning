N,acc,unc_acc,unc_auroc,unc_ece,dataset,split,ts
11,0.09090909361839294,0.7272727489471436,0.8,0.12535510821775955,mmlu_offline:llama2-7b_chat:abstract_algebra,validation,3.8188809799030423
100,0.20999999344348907,0.5899999737739563,0.5949367088607596,0.12863280951976774,mmlu_offline:llama2-7b_chat:abstract_algebra,test,3.2599927741102874
14,0.2142857164144516,0.3571428656578064,0.7878787878787878,0.28097098640033175,mmlu_offline:llama2-7b_chat:anatomy,validation,0.5384272979572415
135,0.385185182094574,0.43703705072402954,0.5396200185356812,0.16226853220551102,mmlu_offline:llama2-7b_chat:anatomy,test,3.6628670440986753
16,0.5625,0.375,0.4444444444444445,0.3088379018008709,mmlu_offline:llama2-7b_chat:astronomy,validation,0.7827823022380471
152,0.42763158679008484,0.6184210777282715,0.6875331564986737,0.026033081898563806,mmlu_offline:llama2-7b_chat:astronomy,test,5.5987715548835695
11,0.4545454680919647,0.6363636255264282,0.5333333333333333,0.27450284090909094,mmlu_offline:llama2-7b_chat:business_ethics,validation,0.7315059909597039
100,0.33000001311302185,0.4000000059604645,0.6451831750339213,0.22753904223442076,mmlu_offline:llama2-7b_chat:business_ethics,test,4.700329448096454
29,0.17241379618644714,0.6206896305084229,0.8666666666666667,0.05468749383400224,mmlu_offline:llama2-7b_chat:clinical_knowledge,validation,1.016855170018971
265,0.2792452871799469,0.501886785030365,0.652080090561766,0.10878536633725437,mmlu_offline:llama2-7b_chat:clinical_knowledge,test,8.15544731123373
16,0.375,0.6875,0.7,0.0927734486758709,mmlu_offline:llama2-7b_chat:college_biology,validation,0.7525967531837523
144,0.3402777910232544,0.4930555522441864,0.5671321160042965,0.09388562126292124,mmlu_offline:llama2-7b_chat:college_biology,test,5.6747059831395745
8,0.0,0.625,,0.3876952975988388,mmlu_offline:llama2-7b_chat:college_chemistry,validation,0.4824123838916421
100,0.14000000059604645,0.6200000047683716,0.6594684385382061,0.06699217736721039,mmlu_offline:llama2-7b_chat:college_chemistry,test,4.502307710237801
11,0.27272728085517883,0.5454545617103577,0.625,0.14879260821775955,mmlu_offline:llama2-7b_chat:college_computer_science,validation,1.0056814369745553
100,0.17000000178813934,0.7099999785423279,0.7143869596031183,0.12371094465255736,mmlu_offline:llama2-7b_chat:college_computer_science,test,8.017955132294446
11,0.0,0.7272727489471436,,0.22549715367230502,mmlu_offline:llama2-7b_chat:college_mathematics,validation,0.7947125770151615
100,0.2199999988079071,0.6399999856948853,0.5935314685314684,0.0739452928304672,mmlu_offline:llama2-7b_chat:college_mathematics,test,5.639581408817321
22,0.3181818127632141,0.40909090638160706,0.680952380952381,0.1946022727272727,mmlu_offline:llama2-7b_chat:college_medicine,validation,1.0580699960701168
173,0.2947976887226105,0.5086705088615417,0.7012214721954355,0.12010025392377999,mmlu_offline:llama2-7b_chat:college_medicine,test,10.294758262578398
11,0.09090909361839294,0.3636363744735718,0.8,0.22549716450951318,mmlu_offline:llama2-7b_chat:college_physics,validation,0.7103082188405097
102,0.0882352963089943,0.529411792755127,0.7491039426523296,0.0661764740943909,mmlu_offline:llama2-7b_chat:college_physics,test,4.850881772115827
11,0.6363636255264282,0.6363636255264282,0.6785714285714285,0.17223010821775958,mmlu_offline:llama2-7b_chat:computer_security,validation,0.628299224190414
100,0.5,0.6000000238418579,0.657,0.10460936665534976,mmlu_offline:llama2-7b_chat:computer_security,test,3.2956397379748523
26,0.1538461595773697,0.19230769574642181,0.32954545454545453,0.42983773121467006,mmlu_offline:llama2-7b_chat:conceptual_physics,validation,0.9790201280266047
235,0.3787234127521515,0.38723403215408325,0.5399030321686933,0.22890622717268927,mmlu_offline:llama2-7b_chat:conceptual_physics,test,6.484090686775744
12,0.3333333432674408,0.5833333134651184,0.78125,0.06803384423255918,mmlu_offline:llama2-7b_chat:econometrics,validation,0.845694846007973
114,0.17543859779834747,0.640350878238678,0.5994680851063829,0.06342516604222746,mmlu_offline:llama2-7b_chat:econometrics,test,6.543255203869194
16,0.1875,0.25,0.23076923076923073,0.2946777194738388,mmlu_offline:llama2-7b_chat:electrical_engineering,validation,0.7490760101936758
145,0.2137930989265442,0.5103448033332825,0.5440011318619129,0.1069773731560543,mmlu_offline:llama2-7b_chat:electrical_engineering,test,5.616976961027831
41,0.3658536672592163,0.5853658318519592,0.6628205128205129,0.11242378048780491,mmlu_offline:llama2-7b_chat:elementary_mathematics,validation,1.9023940251208842
378,0.28042328357696533,0.60317462682724,0.6128780521642619,0.017557440138367766,mmlu_offline:llama2-7b_chat:elementary_mathematics,test,15.365150636993349
14,0.4285714328289032,0.5,0.47916666666666663,0.06277903488704137,mmlu_offline:llama2-7b_chat:formal_logic,validation,0.8004589779302478
126,0.2539682686328888,0.5555555820465088,0.6484375,0.029606891056847915,mmlu_offline:llama2-7b_chat:formal_logic,test,6.029140415135771
10,0.20000000298023224,0.800000011920929,0.875,0.22304688096046452,mmlu_offline:llama2-7b_chat:global_facts,validation,0.5531598469242454
100,0.07999999821186066,0.47999998927116394,0.7839673913043478,0.2284765636920929,mmlu_offline:llama2-7b_chat:global_facts,test,3.580764683894813
32,0.3125,0.5,0.4613636363636364,0.11633302830159661,mmlu_offline:llama2-7b_chat:high_school_biology,validation,1.3560670521110296
310,0.3838709592819214,0.625806450843811,0.6440670509041313,0.040776213138334204,mmlu_offline:llama2-7b_chat:high_school_biology,test,12.017812232021242
22,0.1818181872367859,0.40909090638160706,0.5208333333333333,0.2112926109270616,mmlu_offline:llama2-7b_chat:high_school_chemistry,validation,1.0882374090142548
203,0.1428571492433548,0.5418719053268433,0.751684502576298,0.06169182825558289,mmlu_offline:llama2-7b_chat:high_school_chemistry,test,8.856497860047966
9,0.4444444477558136,0.4444444477558136,0.44999999999999996,0.13585068119896782,mmlu_offline:llama2-7b_chat:high_school_computer_science,validation,0.8373212083242834
100,0.4099999964237213,0.5,0.5983877635386523,0.10378904342651367,mmlu_offline:llama2-7b_chat:high_school_computer_science,test,7.162516723852605
18,0.8333333134651184,0.7777777910232544,0.788888888888889,0.170789940489663,mmlu_offline:llama2-7b_chat:high_school_european_history,validation,5.705897855106741
165,0.7090908885002136,0.7212121486663818,0.6758368945868946,0.12850379943847662,mmlu_offline:llama2-7b_chat:high_school_european_history,test,51.96561095584184
22,0.4545454680919647,0.5909090638160706,0.625,0.03284800323573028,mmlu_offline:llama2-7b_chat:high_school_geography,validation,0.7910674158483744
198,0.3737373650074005,0.47979798913002014,0.6082170880557978,0.11257101971693709,mmlu_offline:llama2-7b_chat:high_school_geography,test,5.732100839260966
21,0.523809552192688,0.6666666865348816,0.6,0.12127974771317983,mmlu_offline:llama2-7b_chat:high_school_government_and_politics,validation,0.8816792722791433
193,0.5233160853385925,0.5544041395187378,0.5938441670253981,0.0422806094347504,mmlu_offline:llama2-7b_chat:high_school_government_and_politics,test,6.325724054127932
43,0.39534884691238403,0.41860464215278625,0.4445701357466063,0.18259449892265855,mmlu_offline:llama2-7b_chat:high_school_macroeconomics,validation,1.4073089538142085
390,0.2897436022758484,0.5589743852615356,0.6486853455161178,0.030088147291770315,mmlu_offline:llama2-7b_chat:high_school_macroeconomics,test,11.513518924824893
29,0.0,0.6896551847457886,,0.07839438627506123,mmlu_offline:llama2-7b_chat:high_school_mathematics,validation,1.5350876492448151
270,0.0555555559694767,0.6629629731178284,0.6081045751633988,0.10869503948423598,mmlu_offline:llama2-7b_chat:high_school_mathematics,test,13.107465209905058
26,0.26923078298568726,0.38461539149284363,0.6691729323308271,0.2115384775858659,mmlu_offline:llama2-7b_chat:high_school_microeconomics,validation,0.9658564450219274
238,0.3403361439704895,0.45378151535987854,0.684398836203507,0.13540572629255407,mmlu_offline:llama2-7b_chat:high_school_microeconomics,test,6.940883104689419
17,0.0,0.47058823704719543,,0.26608455882352944,mmlu_offline:llama2-7b_chat:high_school_physics,validation,1.1012222883291543
151,0.18543046712875366,0.41721853613853455,0.6788617886178863,0.18871585856999784,mmlu_offline:llama2-7b_chat:high_school_physics,test,7.116900736931711
60,0.5666666626930237,0.5833333134651184,0.5073529411764706,0.07499999503294626,mmlu_offline:llama2-7b_chat:high_school_psychology,validation,2.533026484772563
545,0.5009174346923828,0.5449541211128235,0.5788623141564319,0.0506880905650078,mmlu_offline:llama2-7b_chat:high_school_psychology,test,21.71321739209816
23,0.17391304671764374,0.52173912525177,0.5460526315789473,0.21263587733973627,mmlu_offline:llama2-7b_chat:high_school_statistics,validation,1.5673419223167002
216,0.19907407462596893,0.49537035822868347,0.6166151364430702,0.10979092121124265,mmlu_offline:llama2-7b_chat:high_school_statistics,test,13.61872996808961
22,0.6363636255264282,0.6363636255264282,0.8125,0.11576702378012921,mmlu_offline:llama2-7b_chat:high_school_us_history,validation,5.432911369018257
204,0.5784313678741455,0.6029411554336548,0.7182696097753252,0.047411172997717806,mmlu_offline:llama2-7b_chat:high_school_us_history,test,49.434157584793866
26,0.7307692170143127,0.7307692170143127,0.5112781954887218,0.09074521064758301,mmlu_offline:llama2-7b_chat:high_school_world_history,validation,4.618691547773778
237,0.552742600440979,0.5907173156738281,0.6270704306495751,0.05272614100814369,mmlu_offline:llama2-7b_chat:high_school_world_history,test,37.691036868374795
23,0.30434781312942505,0.47826087474823,0.6160714285714286,0.17883830744287244,mmlu_offline:llama2-7b_chat:human_aging,validation,0.7828550771810114
223,0.33183857798576355,0.4798206388950348,0.6301922728097225,0.11114421500218821,mmlu_offline:llama2-7b_chat:human_aging,test,6.229217112995684
12,0.25,0.5,0.537037037037037,0.12955729166666666,mmlu_offline:llama2-7b_chat:human_sexuality,validation,0.5624514012597501
131,0.4198473393917084,0.580152690410614,0.6233253588516746,0.011480210391619701,mmlu_offline:llama2-7b_chat:human_sexuality,test,4.1113027720712125
13,0.4615384638309479,0.4615384638309479,0.5714285714285714,0.2430889377227196,mmlu_offline:llama2-7b_chat:international_law,validation,0.6663080872967839
121,0.6115702390670776,0.6115702390670776,0.6726566992524441,0.1123450393519126,mmlu_offline:llama2-7b_chat:international_law,test,4.98068971093744
11,0.5454545617103577,0.5454545617103577,0.75,0.15909090909090906,mmlu_offline:llama2-7b_chat:jurisprudence,validation,0.5699177077040076
108,0.5185185074806213,0.5092592835426331,0.6201923076923077,0.11809172288135245,mmlu_offline:llama2-7b_chat:jurisprudence,test,3.5339016392827034
18,0.4444444477558136,0.5,0.65,0.12651909059948394,mmlu_offline:llama2-7b_chat:logical_fallacies,validation,0.8090938958339393
163,0.42944785952568054,0.6134969592094421,0.6665130568356376,0.042034110042946465,mmlu_offline:llama2-7b_chat:logical_fallacies,test,5.69216961087659
11,0.3636363744735718,0.6363636255264282,0.4107142857142857,0.0564630627632141,mmlu_offline:llama2-7b_chat:machine_learning,validation,0.7194675449281931
112,0.1964285671710968,0.6071428656578064,0.6053030303030303,0.05552453867026741,mmlu_offline:llama2-7b_chat:machine_learning,test,5.859002371318638
11,0.27272728085517883,0.7272727489471436,0.875,0.14204545454545456,mmlu_offline:llama2-7b_chat:management,validation,0.5136401816271245
103,0.3689320385456085,0.5728155374526978,0.6852226720647773,0.05055369393339434,mmlu_offline:llama2-7b_chat:management,test,2.6455455967225134
25,0.20000000298023224,0.2800000011920929,0.37,0.3328125023841858,mmlu_offline:llama2-7b_chat:marketing,validation,1.0631827539764345
234,0.39743590354919434,0.44017094373703003,0.5326012354152369,0.1721754940147074,mmlu_offline:llama2-7b_chat:marketing,test,7.46212510112673
11,0.7272727489471436,0.6363636255264282,0.4166666666666667,0.1338778409090909,mmlu_offline:llama2-7b_chat:medical_genetics,validation,0.5126148429699242
100,0.4099999964237213,0.6100000143051147,0.6521289789169078,0.03367188215255739,mmlu_offline:llama2-7b_chat:medical_genetics,test,2.7375763640739024
86,0.41860464215278625,0.6511628031730652,0.7413888888888889,0.1086028398469437,mmlu_offline:llama2-7b_chat:miscellaneous,validation,2.412225504871458
783,0.4533844292163849,0.6922094225883484,0.7444451757272608,0.10406191367299159,mmlu_offline:llama2-7b_chat:miscellaneous,test,21.404903997667134
38,0.4736842215061188,0.5,0.5541666666666667,0.1205797603255824,mmlu_offline:llama2-7b_chat:moral_disputes,validation,1.5139293833635747
346,0.41040462255477905,0.4450867176055908,0.604408312620823,0.17298141518080165,mmlu_offline:llama2-7b_chat:moral_disputes,test,12.244707945268601
100,0.5099999904632568,0.5600000023841858,0.6206482593037216,0.011953113079071098,mmlu_offline:llama2-7b_chat:moral_scenarios,validation,6.060356000903994
895,0.46145251393318176,0.4960893988609314,0.636381903489295,0.05412449996564642,mmlu_offline:llama2-7b_chat:moral_scenarios,test,52.84887858014554
33,0.3636363744735718,0.5151515007019043,0.6706349206349207,0.12239584055813875,mmlu_offline:llama2-7b_chat:nutrition,validation,1.5430012219585478
306,0.4084967374801636,0.5392156839370728,0.6238232044198895,0.05897671081661401,mmlu_offline:llama2-7b_chat:nutrition,test,12.538478689733893
34,0.3529411852359772,0.38235294818878174,0.5037878787878788,0.2289751662927515,mmlu_offline:llama2-7b_chat:philosophy,validation,1.190214591100812
311,0.3376205861568451,0.34726688265800476,0.5930651872399445,0.27096311839063836,mmlu_offline:llama2-7b_chat:philosophy,test,8.959481403697282
35,0.2857142984867096,0.5142857432365417,0.74,0.05792412928172522,mmlu_offline:llama2-7b_chat:prehistory,validation,1.3803622000850737
324,0.34259259700775146,0.5216049551963806,0.6910713530431841,0.058545531498061285,mmlu_offline:llama2-7b_chat:prehistory,test,10.853417098987848
31,0.06451612710952759,0.4516128897666931,0.6551724137931034,0.16582660136684294,mmlu_offline:llama2-7b_chat:professional_accounting,validation,2.2430679160170257
282,0.1560283750295639,0.41489362716674805,0.6233766233766234,0.17986759205236508,mmlu_offline:llama2-7b_chat:professional_accounting,test,19.105387966148555
170,0.30588236451148987,0.3176470696926117,0.5856421121251629,0.35585939814062684,mmlu_offline:llama2-7b_chat:professional_law,validation,25.90429292805493
1534,0.3200782239437103,0.3305084705352783,0.5959983441154589,0.3447049304046407,mmlu_offline:llama2-7b_chat:professional_law,test,237.0904250270687
31,0.25806450843811035,0.5161290168762207,0.5135869565217391,0.10534274385821435,mmlu_offline:llama2-7b_chat:professional_medicine,validation,3.818837830796838
272,0.23529411852359772,0.5404411554336548,0.6135066105769231,0.06452492142424861,mmlu_offline:llama2-7b_chat:professional_medicine,test,32.93190594390035
69,0.37681159377098083,0.4492753744125366,0.6194096601073344,0.16117526137310526,mmlu_offline:llama2-7b_chat:professional_psychology,validation,3.169014635961503
612,0.32189542055130005,0.40522876381874084,0.5786924347134732,0.1906275407356374,mmlu_offline:llama2-7b_chat:professional_psychology,test,25.049217250198126
12,0.3333333432674408,0.5,0.546875,0.12760416666666666,mmlu_offline:llama2-7b_chat:public_relations,validation,0.5982852349989116
110,0.3272727131843567,0.5090909004211426,0.6266891891891891,0.0903409112583507,mmlu_offline:llama2-7b_chat:public_relations,test,3.7582033476792276
27,0.7407407164573669,0.7777777910232544,0.6857142857142857,0.22771990740740744,mmlu_offline:llama2-7b_chat:security_studies,validation,1.5467190728522837
245,0.6816326379776001,0.7061224579811096,0.7034392752955627,0.06434948979591837,mmlu_offline:llama2-7b_chat:security_studies,test,12.759708862286061
22,0.5,0.5,0.628099173553719,0.18288350918076257,mmlu_offline:llama2-7b_chat:sociology,validation,0.8004913530312479
201,0.45771142840385437,0.5920398235321045,0.6315815715995212,0.05253030678526086,mmlu_offline:llama2-7b_chat:sociology,test,6.2372939409688115
11,0.5454545617103577,0.5454545617103577,0.6666666666666666,0.14026988094503232,mmlu_offline:llama2-7b_chat:us_foreign_policy,validation,0.5683772889897227
100,0.550000011920929,0.6200000047683716,0.6206060606060606,0.04984373867511748,mmlu_offline:llama2-7b_chat:us_foreign_policy,test,3.233552621677518
18,0.2777777910232544,0.6111111044883728,0.5692307692307692,0.03385416004392838,mmlu_offline:llama2-7b_chat:virology,validation,0.872648507822305
166,0.34337350726127625,0.5301204919815063,0.5857073877353935,0.05445221030568503,mmlu_offline:llama2-7b_chat:virology,test,5.101160624995828
19,0.6315789222717285,0.7368420958518982,0.7142857142857142,0.14206413846266897,mmlu_offline:llama2-7b_chat:world_religions,validation,0.7307680319063365
171,0.5380116701126099,0.5497075915336609,0.636557512383049,0.08664565860179435,mmlu_offline:llama2-7b_chat:world_religions,test,4.381471197586507
