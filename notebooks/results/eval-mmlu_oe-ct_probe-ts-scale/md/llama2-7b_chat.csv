N,acc,unc_acc,unc_auroc,unc_ece,dataset,split,ts
11,0.09090909361839294,0.7272727489471436,0.7,0.18714491345665674,mmlu_offline:llama2-7b_chat:abstract_algebra,validation,3.377292675897479
100,0.20999999344348907,0.5699999928474426,0.5925256178420735,0.038437500000000034,mmlu_offline:llama2-7b_chat:abstract_algebra,test,4.16202737390995
14,0.2142857164144516,0.2142857164144516,0.5,0.3091517857142857,mmlu_offline:llama2-7b_chat:anatomy,validation,0.6490369350649416
135,0.385185182094574,0.40740740299224854,0.5383456904541242,0.11507522618329086,mmlu_offline:llama2-7b_chat:anatomy,test,5.04982187692076
16,0.5625,0.5625,0.5317460317460317,0.0361328125,mmlu_offline:llama2-7b_chat:astronomy,validation,0.8666105568408966
152,0.42763158679008484,0.4736842215061188,0.5362511052166224,0.05062703396144669,mmlu_offline:llama2-7b_chat:astronomy,test,6.5026453346945345
11,0.4545454680919647,0.4545454680919647,0.5,0.06889204545454547,mmlu_offline:llama2-7b_chat:business_ethics,validation,0.7718763039447367
100,0.33000001311302185,0.3400000035762787,0.5146992311171416,0.18285158634185789,mmlu_offline:llama2-7b_chat:business_ethics,test,5.1128343869932
29,0.17241379618644714,0.24137930572032928,0.5416666666666667,0.28313578202806666,mmlu_offline:llama2-7b_chat:clinical_knowledge,validation,1.3112176163122058
265,0.2792452871799469,0.34716981649398804,0.540965048818452,0.17635614197209198,mmlu_offline:llama2-7b_chat:clinical_knowledge,test,10.355730643030256
16,0.375,0.5625,0.5833333333333334,0.03173828125,mmlu_offline:llama2-7b_chat:college_biology,validation,0.810055139940232
144,0.3402777910232544,0.4166666567325592,0.5011815252416757,0.10630967219670612,mmlu_offline:llama2-7b_chat:college_biology,test,6.298602957744151
8,0.0,0.25,,0.2744140625,mmlu_offline:llama2-7b_chat:college_chemistry,validation,0.6412084833718836
100,0.14000000059604645,0.5299999713897705,0.6038205980066446,0.016054676771163918,mmlu_offline:llama2-7b_chat:college_chemistry,test,4.932171574328095
11,0.27272728085517883,0.3636363744735718,0.5,0.31498578461733734,mmlu_offline:llama2-7b_chat:college_computer_science,validation,1.2403326840139925
100,0.17000000178813934,0.46000000834465027,0.5974486180014175,0.13417968273162842,mmlu_offline:llama2-7b_chat:college_computer_science,test,8.434048110153526
11,0.0,0.7272727489471436,,0.14240055734461005,mmlu_offline:llama2-7b_chat:college_mathematics,validation,0.9173043090850115
100,0.2199999988079071,0.6399999856948853,0.6200466200466199,0.08144531488418576,mmlu_offline:llama2-7b_chat:college_mathematics,test,5.968786703888327
22,0.3181818127632141,0.5,0.6,0.023970186710357666,mmlu_offline:llama2-7b_chat:college_medicine,validation,1.2174828927963972
173,0.2947976887226105,0.4046242833137512,0.5578592092574735,0.12016797548084591,mmlu_offline:llama2-7b_chat:college_medicine,test,11.174243485089391
11,0.09090909361839294,0.4545454680919647,0.75,0.0664062337441878,mmlu_offline:llama2-7b_chat:college_physics,validation,0.7825586437247694
102,0.0882352963089943,0.5392156839370728,0.6356033452807647,0.015127123570909718,mmlu_offline:llama2-7b_chat:college_physics,test,5.151280319318175
11,0.6363636255264282,0.6363636255264282,0.4642857142857143,0.11292613636363635,mmlu_offline:llama2-7b_chat:computer_security,validation,0.7361706937663257
100,0.5,0.5699999928474426,0.5586,0.04597658634185786,mmlu_offline:llama2-7b_chat:computer_security,test,4.09446051903069
26,0.1538461595773697,0.26923078298568726,0.5909090909090908,0.252103342459752,mmlu_offline:llama2-7b_chat:conceptual_physics,validation,1.2859526919201016
235,0.3787234127521515,0.45957446098327637,0.5237417269509004,0.061835103085700505,mmlu_offline:llama2-7b_chat:conceptual_physics,test,9.222732097376138
12,0.3333333432674408,0.3333333432674408,0.453125,0.18782554070154828,mmlu_offline:llama2-7b_chat:econometrics,validation,0.8932658107951283
114,0.17543859779834747,0.37719297409057617,0.5186170212765957,0.17129248798939223,mmlu_offline:llama2-7b_chat:econometrics,test,6.886456792708486
16,0.1875,0.125,0.358974358974359,0.39794921875,mmlu_offline:llama2-7b_chat:electrical_engineering,validation,0.8125138273462653
145,0.2137930989265442,0.32413792610168457,0.4770797962648557,0.20325967073440548,mmlu_offline:llama2-7b_chat:electrical_engineering,test,6.3005497846752405
41,0.3658536672592163,0.6341463327407837,0.6230769230769231,0.08574696866477406,mmlu_offline:llama2-7b_chat:elementary_mathematics,validation,2.176817175000906
378,0.28042328357696533,0.5925925970077515,0.5515919811320755,0.052734393606740926,mmlu_offline:llama2-7b_chat:elementary_mathematics,test,16.972931700758636
14,0.4285714328289032,0.5,0.5625,0.021763384342193604,mmlu_offline:llama2-7b_chat:formal_logic,validation,0.8896131101064384
126,0.2539682686328888,0.3650793731212616,0.601063829787234,0.15814111440900774,mmlu_offline:llama2-7b_chat:formal_logic,test,6.391072737053037
10,0.20000000298023224,0.4000000059604645,0.6875,0.12148437499999998,mmlu_offline:llama2-7b_chat:global_facts,validation,0.6413562968373299
100,0.07999999821186066,0.33000001311302185,0.6868206521739131,0.19171874046325682,mmlu_offline:llama2-7b_chat:global_facts,test,4.081871592905372
32,0.3125,0.375,0.5181818181818182,0.1461181640625,mmlu_offline:llama2-7b_chat:high_school_biology,validation,1.59477355517447
310,0.3838709592819214,0.48709678649902344,0.5591755026617976,0.03781499401215584,mmlu_offline:llama2-7b_chat:high_school_biology,test,13.28519064327702
22,0.1818181872367859,0.5454545617103577,0.6041666666666667,0.03302556818181822,mmlu_offline:llama2-7b_chat:high_school_chemistry,validation,1.1940287868492305
203,0.1428571492433548,0.546798050403595,0.6884661117717004,0.019242582062782364,mmlu_offline:llama2-7b_chat:high_school_chemistry,test,9.706774417776614
9,0.4444444477558136,0.4444444477558136,0.5,0.08420136902067399,mmlu_offline:llama2-7b_chat:high_school_computer_science,validation,0.9593042209744453
100,0.4099999964237213,0.46000000834465027,0.48739148408433236,0.07386718451976777,mmlu_offline:llama2-7b_chat:high_school_computer_science,test,7.5793908927589655
18,0.8333333134651184,0.7777777910232544,0.5777777777777777,0.2549913393126594,mmlu_offline:llama2-7b_chat:high_school_european_history,validation,5.956814765930176
165,0.7090908885002136,0.7333333492279053,0.5402421652421652,0.2101089199384053,mmlu_offline:llama2-7b_chat:high_school_european_history,test,52.508972169831395
22,0.4545454680919647,0.5454545617103577,0.5708333333333333,0.023970170454545414,mmlu_offline:llama2-7b_chat:high_school_geography,validation,0.9701301329769194
198,0.3737373650074005,0.4191919267177582,0.5183631211857018,0.10347614444867531,mmlu_offline:llama2-7b_chat:high_school_geography,test,7.524093393236399
21,0.523809552192688,0.4761904776096344,0.45454545454545453,0.04687497161683585,mmlu_offline:llama2-7b_chat:high_school_government_and_politics,validation,1.0182471061125398
193,0.5233160853385925,0.5440414547920227,0.5345996556177357,0.02110993460670041,mmlu_offline:llama2-7b_chat:high_school_government_and_politics,test,7.627067012712359
43,0.39534884691238403,0.39534884691238403,0.4909502262443439,0.13126815335695136,mmlu_offline:llama2-7b_chat:high_school_macroeconomics,validation,1.863944421056658
390,0.2897436022758484,0.3769230842590332,0.5486246445800453,0.14726562041502733,mmlu_offline:llama2-7b_chat:high_school_macroeconomics,test,14.645219773985445
29,0.0,0.7586206793785095,,0.19921873150200675,mmlu_offline:llama2-7b_chat:high_school_mathematics,validation,1.6339640258811414
270,0.0555555559694767,0.7407407164573669,0.569281045751634,0.19361977886270595,mmlu_offline:llama2-7b_chat:high_school_mathematics,test,13.932744011748582
26,0.26923078298568726,0.4615384638309479,0.631578947368421,0.06490382781395543,mmlu_offline:llama2-7b_chat:high_school_microeconomics,validation,1.2558501837775111
238,0.3403361439704895,0.39915966987609863,0.5232759298576708,0.1240972936654291,mmlu_offline:llama2-7b_chat:high_school_microeconomics,test,9.002654785756022
17,0.0,0.29411765933036804,,0.22840074931873994,mmlu_offline:llama2-7b_chat:high_school_physics,validation,1.183872350025922
151,0.18543046712875366,0.37748345732688904,0.5753484320557491,0.15883693118758546,mmlu_offline:llama2-7b_chat:high_school_physics,test,7.642148660030216
60,0.5666666626930237,0.5833333134651184,0.5113122171945701,0.0599609216054281,mmlu_offline:llama2-7b_chat:high_school_psychology,validation,2.780352749861777
545,0.5009174346923828,0.5137614607810974,0.5157495690583926,0.010923144139281152,mmlu_offline:llama2-7b_chat:high_school_psychology,test,24.3602436799556
23,0.17391304671764374,0.3913043439388275,0.5723684210526316,0.17119565735692563,mmlu_offline:llama2-7b_chat:high_school_statistics,validation,1.6566694062203169
216,0.19907407462596893,0.3472222089767456,0.5901330824035488,0.2010814365413454,mmlu_offline:llama2-7b_chat:high_school_statistics,test,14.376416018698364
22,0.6363636255264282,0.6363636255264282,0.5,0.11292613636363635,mmlu_offline:llama2-7b_chat:high_school_us_history,validation,5.667953944765031
204,0.5784313678741455,0.5784313678741455,0.5146826960977533,0.055147056486092416,mmlu_offline:llama2-7b_chat:high_school_us_history,test,49.92275507003069
26,0.7307692170143127,0.7307692170143127,0.5,0.20733173076923073,mmlu_offline:llama2-7b_chat:high_school_world_history,validation,4.662926516961306
237,0.552742600440979,0.5654008388519287,0.5640933314129338,0.04410601165224226,mmlu_offline:llama2-7b_chat:high_school_world_history,test,39.263220097869635
23,0.30434781312942505,0.3478260934352875,0.4017857142857143,0.17476222566936328,mmlu_offline:llama2-7b_chat:human_aging,validation,0.9830688019283116
223,0.33183857798576355,0.5022421479225159,0.5975875204063124,0.02070486091177559,mmlu_offline:llama2-7b_chat:human_aging,test,8.158672148827463
12,0.25,0.3333333432674408,0.5,0.1874999801317851,mmlu_offline:llama2-7b_chat:human_sexuality,validation,0.7410761187784374
131,0.4198473393917084,0.5114504098892212,0.6017942583732058,0.012195853786614141,mmlu_offline:llama2-7b_chat:human_sexuality,test,5.287033964879811
13,0.4615384638309479,0.4615384638309479,0.5,0.061899038461538436,mmlu_offline:llama2-7b_chat:international_law,validation,0.799386250320822
121,0.6115702390670776,0.6033057570457458,0.4905117883841288,0.07948091453757167,mmlu_offline:llama2-7b_chat:international_law,test,5.458525033667684
11,0.5454545617103577,0.5454545617103577,0.4166666666666667,0.02308235927061597,mmlu_offline:llama2-7b_chat:jurisprudence,validation,0.7736622467637062
108,0.5185185074806213,0.5185185074806213,0.5013736263736264,0.004521120477605778,mmlu_offline:llama2-7b_chat:jurisprudence,test,4.316170571837574
18,0.4444444477558136,0.4444444477558136,0.48124999999999996,0.07747397157880997,mmlu_offline:llama2-7b_chat:logical_fallacies,validation,1.057503854855895
163,0.42944785952568054,0.48466256260871887,0.5907834101382489,0.03788824461720475,mmlu_offline:llama2-7b_chat:logical_fallacies,test,6.750592963304371
11,0.3636363744735718,0.4545454680919647,0.3571428571428572,0.0873579491268505,mmlu_offline:llama2-7b_chat:machine_learning,validation,1.0398069070652127
112,0.1964285671710968,0.375,0.5025252525252526,0.17368859531623973,mmlu_offline:llama2-7b_chat:machine_learning,test,6.191288029309362
11,0.27272728085517883,0.3636363744735718,0.41666666666666663,0.15767044912685046,mmlu_offline:llama2-7b_chat:management,validation,0.7081595431081951
103,0.3689320385456085,0.42718446254730225,0.5157894736842106,0.09507739196703274,mmlu_offline:llama2-7b_chat:management,test,3.666664392221719
25,0.20000000298023224,0.23999999463558197,0.575,0.28156251668930055,mmlu_offline:llama2-7b_chat:marketing,validation,1.2670131688937545
234,0.39743590354919434,0.39743590354919434,0.5017158544955388,0.12586802855516094,mmlu_offline:llama2-7b_chat:marketing,test,9.256799555849284
11,0.7272727489471436,0.6363636255264282,0.4375,0.10724432360042224,mmlu_offline:llama2-7b_chat:medical_genetics,validation,0.6667575538158417
100,0.4099999964237213,0.5199999809265137,0.5820587019429516,0.014062499403953564,mmlu_offline:llama2-7b_chat:medical_genetics,test,3.8322646198794246
86,0.41860464215278625,0.44186046719551086,0.5766666666666667,0.07985100358031516,mmlu_offline:llama2-7b_chat:miscellaneous,validation,3.265365953091532
783,0.4533844292163849,0.5031928420066833,0.5455475845728576,0.021955836397782456,mmlu_offline:llama2-7b_chat:miscellaneous,test,28.18858928605914
38,0.4736842215061188,0.5,0.525,0.02354031801223755,mmlu_offline:llama2-7b_chat:moral_disputes,validation,1.7126901671290398
346,0.41040462255477905,0.4219653308391571,0.5090444628555647,0.10126898984688554,mmlu_offline:llama2-7b_chat:moral_disputes,test,14.801186490803957
100,0.5099999904632568,0.5099999904632568,0.5,0.013437499999999991,mmlu_offline:llama2-7b_chat:moral_scenarios,validation,6.423238093033433
895,0.46145251393318176,0.46145251393318176,0.5,0.06198498603351954,mmlu_offline:llama2-7b_chat:moral_scenarios,test,56.4312386312522
33,0.3636363744735718,0.6666666865348816,0.7361111111111112,0.14287405303030304,mmlu_offline:llama2-7b_chat:nutrition,validation,1.9196243891492486
306,0.4084967374801636,0.4771241843700409,0.5377900552486189,0.04624948041890958,mmlu_offline:llama2-7b_chat:nutrition,test,14.115661820862442
34,0.3529411852359772,0.3529411852359772,0.5227272727272727,0.17015162986867566,mmlu_offline:llama2-7b_chat:philosophy,validation,1.7253412147983909
311,0.3376205861568451,0.3408360183238983,0.4907073509015257,0.18256381700276564,mmlu_offline:llama2-7b_chat:philosophy,test,12.668501453008503
35,0.2857142984867096,0.37142857909202576,0.516,0.1527901666504996,mmlu_offline:llama2-7b_chat:prehistory,validation,1.7698531760834157
324,0.34259259700775146,0.4104938209056854,0.5306644672841856,0.11933352034768942,mmlu_offline:llama2-7b_chat:prehistory,test,13.851944904774427
31,0.06451612710952759,0.4516128897666931,0.47413793103448276,0.07232861365041426,mmlu_offline:llama2-7b_chat:professional_accounting,validation,2.379825796931982
282,0.1560283750295639,0.40780141949653625,0.5528074866310161,0.11492964731040578,mmlu_offline:llama2-7b_chat:professional_accounting,test,20.25417813612148
170,0.30588236451148987,0.30588236451148987,0.5,0.2175551470588235,mmlu_offline:llama2-7b_chat:professional_law,validation,27.076411209069192
1534,0.3200782239437103,0.3200782239437103,0.5004793863854267,0.20335164374758086,mmlu_offline:llama2-7b_chat:professional_law,test,248.07417538529262
31,0.25806450843811035,0.29032257199287415,0.5434782608695652,0.228704652478618,mmlu_offline:llama2-7b_chat:professional_medicine,validation,4.0018968670628965
272,0.23529411852359772,0.41911765933036804,0.5984825721153847,0.10953297772828272,mmlu_offline:llama2-7b_chat:professional_medicine,test,34.52172628697008
69,0.37681159377098083,0.4202898442745209,0.5031305903398926,0.10394020788911462,mmlu_offline:llama2-7b_chat:professional_psychology,validation,3.445217594038695
612,0.32189542055130005,0.36274510622024536,0.5114427252155831,0.16089667175330363,mmlu_offline:llama2-7b_chat:professional_psychology,test,27.52169784437865
12,0.3333333432674408,0.3333333432674408,0.5625,0.18912760416666669,mmlu_offline:llama2-7b_chat:public_relations,validation,0.7271873909048736
110,0.3272727131843567,0.4000000059604645,0.5812687687687688,0.12230113744735716,mmlu_offline:llama2-7b_chat:public_relations,test,4.585263779386878
27,0.7407407164573669,0.7407407164573669,0.5428571428571428,0.2164351586942319,mmlu_offline:llama2-7b_chat:security_studies,validation,1.7055234368890524
245,0.6816326379776001,0.6857143044471741,0.5287885766927684,0.1620854572373994,mmlu_offline:llama2-7b_chat:security_studies,test,13.80581328785047
22,0.5,0.5454545617103577,0.49173553719008267,0.023615045980973637,mmlu_offline:llama2-7b_chat:sociology,validation,0.9818612989038229
201,0.45771142840385437,0.572139322757721,0.6234044674910252,0.050800661839062844,mmlu_offline:llama2-7b_chat:sociology,test,7.951829445082694
11,0.5454545617103577,0.5454545617103577,0.5,0.022017045454545414,mmlu_offline:llama2-7b_chat:us_foreign_policy,validation,0.74302300484851
100,0.550000011920929,0.5699999928474426,0.5163636363636364,0.04703123092651362,mmlu_offline:llama2-7b_chat:us_foreign_policy,test,4.0946535132825375
18,0.2777777910232544,0.5555555820465088,0.6461538461538462,0.030381951067182777,mmlu_offline:llama2-7b_chat:virology,validation,1.0892590102739632
166,0.34337350726127625,0.46987950801849365,0.5488491871881539,0.05348737986691029,mmlu_offline:llama2-7b_chat:virology,test,6.373196207918227
19,0.6315789222717285,0.5263158082962036,0.4523809523809524,0.009046058905752052,mmlu_offline:llama2-7b_chat:world_religions,validation,0.9522622390650213
171,0.5380116701126099,0.5614035129547119,0.5704457897633461,0.04031890212443834,mmlu_offline:llama2-7b_chat:world_religions,test,6.030130091123283
