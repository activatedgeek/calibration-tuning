N,acc,unc_acc,unc_auroc,unc_ece,dataset,split,ts
11,0.09090909361839294,0.09090909361839294,0.5,0.49893465909090906,mmlu_offline:llama2-13b_chat:abstract_algebra,validation,4.354298800230026
100,0.3499999940395355,0.3499999940395355,0.5,0.23984375000000002,mmlu_offline:llama2-13b_chat:abstract_algebra,test,5.171812757849693
14,0.2857142984867096,0.2857142984867096,0.5,0.3041294642857143,mmlu_offline:llama2-13b_chat:anatomy,validation,0.6870920788496733
135,0.42222222685813904,0.42222222685813904,0.5,0.16762152777777778,mmlu_offline:llama2-13b_chat:anatomy,test,5.295810821931809
16,0.5,0.5,0.5,0.08984375,mmlu_offline:llama2-13b_chat:astronomy,validation,1.010951121803373
152,0.5263158082962036,0.5263158082962036,0.5,0.06352796052631582,mmlu_offline:llama2-13b_chat:astronomy,test,8.277350667864084
11,0.5454545617103577,0.5454545617103577,0.5,0.044389204545454586,mmlu_offline:llama2-13b_chat:business_ethics,validation,0.936327276751399
100,0.3199999928474426,0.3199999928474426,0.5,0.26984375,mmlu_offline:llama2-13b_chat:business_ethics,test,7.315204142127186
29,0.24137930572032928,0.24137930572032928,0.5,0.3484644396551724,mmlu_offline:llama2-13b_chat:clinical_knowledge,validation,1.4752010982483625
265,0.2981131970882416,0.2981131970882416,0.5,0.2917305424528302,mmlu_offline:llama2-13b_chat:clinical_knowledge,test,12.253798226360232
16,0.25,0.25,0.5,0.33984375,mmlu_offline:llama2-13b_chat:college_biology,validation,1.0917721642181277
144,0.3958333432674408,0.3958333432674408,0.5,0.19401041666666669,mmlu_offline:llama2-13b_chat:college_biology,test,8.806096167769283
8,0.0,0.0,,0.58984375,mmlu_offline:llama2-13b_chat:college_chemistry,validation,0.9191682678647339
100,0.17000000178813934,0.17000000178813934,0.5,0.41984374999999996,mmlu_offline:llama2-13b_chat:college_chemistry,test,6.9547927202656865
11,0.27272728085517883,0.27272728085517883,0.5,0.3171164772727273,mmlu_offline:llama2-13b_chat:college_computer_science,validation,1.533119305036962
100,0.23999999463558197,0.23999999463558197,0.5,0.34984375,mmlu_offline:llama2-13b_chat:college_computer_science,test,12.912065973971039
11,0.0,0.0,,0.58984375,mmlu_offline:llama2-13b_chat:college_mathematics,validation,1.1754080993123353
100,0.09000000357627869,0.09000000357627869,0.5,0.49984375000000003,mmlu_offline:llama2-13b_chat:college_mathematics,test,8.991119042038918
22,0.5,0.5,0.5,0.08984375,mmlu_offline:llama2-13b_chat:college_medicine,validation,1.5771277570165694
173,0.3988439440727234,0.3988439440727234,0.5,0.19099981936416183,mmlu_offline:llama2-13b_chat:college_medicine,test,16.691050608176738
11,0.27272728085517883,0.27272728085517883,0.5,0.3171164772727273,mmlu_offline:llama2-13b_chat:college_physics,validation,0.9950812011957169
102,0.1568627506494522,0.1568627506494522,0.5,0.4329810049019608,mmlu_offline:llama2-13b_chat:college_physics,test,7.657601763959974
11,0.5454545617103577,0.5454545617103577,0.5,0.044389204545454586,mmlu_offline:llama2-13b_chat:computer_security,validation,0.8400091957300901
100,0.550000011920929,0.550000011920929,0.5,0.039843749999999956,mmlu_offline:llama2-13b_chat:computer_security,test,4.999451947864145
26,0.3461538553237915,0.3461538553237915,0.5,0.24368990384615385,mmlu_offline:llama2-13b_chat:conceptual_physics,validation,1.2311594267375767
235,0.4553191363811493,0.4553191363811493,0.5,0.1345246010638298,mmlu_offline:llama2-13b_chat:conceptual_physics,test,9.389444690663368
12,0.25,0.25,0.5,0.33984375,mmlu_offline:llama2-13b_chat:econometrics,validation,1.2669994123280048
114,0.1315789520740509,0.1315789520740509,0.5,0.458264802631579,mmlu_offline:llama2-13b_chat:econometrics,test,10.3534175879322
16,0.25,0.25,0.5,0.33984375,mmlu_offline:llama2-13b_chat:electrical_engineering,validation,1.117035377305001
145,0.24827586114406586,0.24827586114406586,0.5,0.3415678879310345,mmlu_offline:llama2-13b_chat:electrical_engineering,test,8.704197647050023
41,0.26829269528388977,0.26829269528388977,0.5,0.3215510670731707,mmlu_offline:llama2-13b_chat:elementary_mathematics,validation,2.8887756392359734
378,0.3174603283405304,0.3174603283405304,0.5,0.27238343253968256,mmlu_offline:llama2-13b_chat:elementary_mathematics,test,24.440435624215752
14,0.2142857164144516,0.2142857164144516,0.5,0.3755580357142857,mmlu_offline:llama2-13b_chat:formal_logic,validation,1.1803745129145682
126,0.261904776096344,0.261904776096344,0.5,0.3279389880952381,mmlu_offline:llama2-13b_chat:formal_logic,test,9.471378089860082
10,0.20000000298023224,0.20000000298023224,0.5,0.38984375,mmlu_offline:llama2-13b_chat:global_facts,validation,0.717299111187458
100,0.12999999523162842,0.12999999523162842,0.5,0.45984375,mmlu_offline:llama2-13b_chat:global_facts,test,5.399013138841838
32,0.3125,0.3125,0.5,0.27734375,mmlu_offline:llama2-13b_chat:high_school_biology,validation,2.0491579310037196
310,0.42258065938949585,0.42258065938949585,0.5,0.16726310483870965,mmlu_offline:llama2-13b_chat:high_school_biology,test,18.81650639511645
22,0.1818181872367859,0.1818181872367859,0.5,0.4080255681818182,mmlu_offline:llama2-13b_chat:high_school_chemistry,validation,1.6763604758307338
203,0.1822660118341446,0.1822660118341446,0.5,0.40757774014778325,mmlu_offline:llama2-13b_chat:high_school_chemistry,test,13.962480916175991
9,0.6666666865348816,0.6666666865348816,0.5,0.07682291666666663,mmlu_offline:llama2-13b_chat:high_school_computer_science,validation,1.1705565983429551
100,0.4300000071525574,0.4300000071525574,0.5,0.15984375,mmlu_offline:llama2-13b_chat:high_school_computer_science,test,11.506239550653845
18,0.7777777910232544,0.7777777910232544,0.5,0.1879340277777778,mmlu_offline:llama2-13b_chat:high_school_european_history,validation,9.320340636651963
165,0.7575757503509521,0.7575757503509521,0.5,0.16773200757575757,mmlu_offline:llama2-13b_chat:high_school_european_history,test,86.1909898002632
22,0.5,0.5,0.5,0.08984375,mmlu_offline:llama2-13b_chat:high_school_geography,validation,1.1408686898648739
198,0.39393940567970276,0.39393940567970276,0.5,0.19590435606060608,mmlu_offline:llama2-13b_chat:high_school_geography,test,8.394170462153852
21,0.523809552192688,0.523809552192688,0.5,0.06603422619047616,mmlu_offline:llama2-13b_chat:high_school_government_and_politics,validation,1.2637932249344885
193,0.6321243643760681,0.6321243643760681,0.5,0.04228060233160624,mmlu_offline:llama2-13b_chat:high_school_government_and_politics,test,9.870531293097883
43,0.4883720874786377,0.4883720874786377,0.5,0.1014716569767442,mmlu_offline:llama2-13b_chat:high_school_macroeconomics,validation,2.0026318673044443
390,0.3692307770252228,0.3692307770252228,0.5,0.22061298076923075,mmlu_offline:llama2-13b_chat:high_school_macroeconomics,test,17.159592870157212
29,0.06896551698446274,0.06896551698446274,0.5,0.5208782327586207,mmlu_offline:llama2-13b_chat:high_school_mathematics,validation,2.342612295411527
270,0.10000000149011612,0.10000000149011612,0.5,0.48984375,mmlu_offline:llama2-13b_chat:high_school_mathematics,test,20.965207612141967
26,0.3076923191547394,0.3076923191547394,0.5,0.2821514423076923,mmlu_offline:llama2-13b_chat:high_school_microeconomics,validation,1.266010475344956
238,0.36554622650146484,0.36554622650146484,0.5,0.22429753151260506,mmlu_offline:llama2-13b_chat:high_school_microeconomics,test,10.534239209722728
17,0.29411765933036804,0.29411765933036804,0.5,0.29572610294117646,mmlu_offline:llama2-13b_chat:high_school_physics,validation,1.5143108102492988
151,0.17218543589115143,0.17218543589115143,0.5,0.41765831953642385,mmlu_offline:llama2-13b_chat:high_school_physics,test,11.340363413095474
60,0.6000000238418579,0.6000000238418579,0.5,0.010156249999999978,mmlu_offline:llama2-13b_chat:high_school_psychology,validation,3.886620081961155
545,0.5064220428466797,0.5064220428466797,0.5,0.08342173165137612,mmlu_offline:llama2-13b_chat:high_school_psychology,test,34.46265725325793
23,0.17391304671764374,0.17391304671764374,0.5,0.41593070652173914,mmlu_offline:llama2-13b_chat:high_school_statistics,validation,2.4553832351230085
216,0.25925925374031067,0.2638888955116272,0.503125,0.32792607060185186,mmlu_offline:llama2-13b_chat:high_school_statistics,test,22.112287459895015
22,0.7727272510528564,0.7727272510528564,0.5,0.1828835227272727,mmlu_offline:llama2-13b_chat:high_school_us_history,validation,8.902632584795356
204,0.6764705777168274,0.6764705777168274,0.5,0.08662683823529416,mmlu_offline:llama2-13b_chat:high_school_us_history,test,81.82719622645527
26,0.6538461446762085,0.6538461446762085,0.5,0.06400240384615385,mmlu_offline:llama2-13b_chat:high_school_world_history,validation,7.513440717943013
237,0.6033755540847778,0.6033755540847778,0.5,0.013531777426160296,mmlu_offline:llama2-13b_chat:high_school_world_history,test,61.996281331870705
23,0.3913043439388275,0.3913043439388275,0.5,0.19853940217391303,mmlu_offline:llama2-13b_chat:human_aging,validation,1.0591467646881938
223,0.3901345431804657,0.3901345431804657,0.5,0.19970922085201792,mmlu_offline:llama2-13b_chat:human_aging,test,9.067611088044941
12,0.3333333432674408,0.3333333432674408,0.5,0.2565104166666667,mmlu_offline:llama2-13b_chat:human_sexuality,validation,0.7244016067124903
131,0.5114504098892212,0.5114504098892212,0.5,0.07839336832061072,mmlu_offline:llama2-13b_chat:human_sexuality,test,6.217713608872145
13,0.6153846383094788,0.6153846383094788,0.5,0.02554086538461542,mmlu_offline:llama2-13b_chat:international_law,validation,0.9449575450271368
121,0.6280992031097412,0.6280992031097412,0.5,0.038255423553719026,mmlu_offline:llama2-13b_chat:international_law,test,7.563303685747087
11,0.1818181872367859,0.1818181872367859,0.5,0.4080255681818182,mmlu_offline:llama2-13b_chat:jurisprudence,validation,0.7255151281133294
108,0.3611111044883728,0.3611111044883728,0.5,0.2287326388888889,mmlu_offline:llama2-13b_chat:jurisprudence,test,5.457406332716346
18,0.6666666865348816,0.6666666865348816,0.5,0.07682291666666663,mmlu_offline:llama2-13b_chat:logical_fallacies,validation,1.113416335079819
163,0.48466256260871887,0.48466256260871887,0.5,0.10518117331288346,mmlu_offline:llama2-13b_chat:logical_fallacies,test,8.921345869079232
11,0.27272728085517883,0.27272728085517883,0.5,0.3171164772727273,mmlu_offline:llama2-13b_chat:machine_learning,validation,1.0216272468678653
112,0.2946428656578064,0.2946428656578064,0.5,0.29520089285714285,mmlu_offline:llama2-13b_chat:machine_learning,test,9.138341226615012
11,0.6363636255264282,0.6363636255264282,0.5,0.046519886363636354,mmlu_offline:llama2-13b_chat:management,validation,0.5842671650461853
103,0.42718446254730225,0.42718446254730225,0.5,0.16265928398058255,mmlu_offline:llama2-13b_chat:management,test,3.868396685924381
25,0.23999999463558197,0.23999999463558197,0.5,0.34984375,mmlu_offline:llama2-13b_chat:marketing,validation,1.5070577003061771
234,0.44871795177459717,0.44871795177459717,0.5,0.14112580128205127,mmlu_offline:llama2-13b_chat:marketing,test,11.735510549042374
11,0.9090909361839294,0.9090909361839294,0.5,0.31924715909090906,mmlu_offline:llama2-13b_chat:medical_genetics,validation,0.618366751819849
100,0.44999998807907104,0.44999998807907104,0.5,0.13984375,mmlu_offline:llama2-13b_chat:medical_genetics,test,3.9446418141014874
86,0.5581395626068115,0.5581395626068115,0.5,0.03170421511627908,mmlu_offline:llama2-13b_chat:miscellaneous,validation,3.403371851891279
783,0.618135392665863,0.618135392665863,0.5,0.028291626756066446,mmlu_offline:llama2-13b_chat:miscellaneous,test,31.3014517291449
38,0.42105263471603394,0.42105263471603394,0.5,0.16879111842105265,mmlu_offline:llama2-13b_chat:moral_disputes,validation,2.246085091959685
346,0.43063583970069885,0.43063583970069885,0.5,0.159207911849711,mmlu_offline:llama2-13b_chat:moral_disputes,test,18.615561795886606
100,0.4300000071525574,0.4300000071525574,0.5,0.15984375,mmlu_offline:llama2-13b_chat:moral_scenarios,validation,9.935063520912081
895,0.38100558519363403,0.38100558519363403,0.5,0.20883816340782124,mmlu_offline:llama2-13b_chat:moral_scenarios,test,87.53510340442881
33,0.3333333432674408,0.3333333432674408,0.5,0.2565104166666667,mmlu_offline:llama2-13b_chat:nutrition,validation,2.317540049087256
306,0.4542483687400818,0.4542483687400818,0.5029940119760479,0.1354166892618915,mmlu_offline:llama2-13b_chat:nutrition,test,20.053962822072208
34,0.3235294222831726,0.3235294222831726,0.5,0.2663143382352941,mmlu_offline:llama2-13b_chat:philosophy,validation,1.6502598598599434
311,0.3633440434932709,0.3633440434932709,0.5,0.22649969855305468,mmlu_offline:llama2-13b_chat:philosophy,test,13.467665502801538
35,0.37142857909202576,0.37142857909202576,0.5,0.21841517857142856,mmlu_offline:llama2-13b_chat:prehistory,validation,2.0466192732565105
324,0.4413580298423767,0.4413580298423767,0.5,0.14848572530864196,mmlu_offline:llama2-13b_chat:prehistory,test,16.767697704024613
31,0.16129031777381897,0.16129031777381897,0.5,0.42855342741935487,mmlu_offline:llama2-13b_chat:professional_accounting,validation,3.506335017737001
282,0.1879432648420334,0.1879432648420334,0.5,0.4019004875886525,mmlu_offline:llama2-13b_chat:professional_accounting,test,30.90895389718935
170,0.38235294818878174,0.38235294818878174,0.5,0.20749080882352944,mmlu_offline:llama2-13b_chat:professional_law,validation,41.985891103278846
1534,0.34810951352119446,0.34810951352119446,0.5,0.24173423239895697,mmlu_offline:llama2-13b_chat:professional_law,test,389.1839285278693
31,0.35483869910240173,0.35483869910240173,0.5,0.23500504032258063,mmlu_offline:llama2-13b_chat:professional_medicine,validation,6.18065334809944
272,0.2904411852359772,0.2904411852359772,0.5,0.29940257352941174,mmlu_offline:llama2-13b_chat:professional_medicine,test,54.11798489000648
69,0.37681159377098083,0.37681159377098083,0.5,0.21303215579710144,mmlu_offline:llama2-13b_chat:professional_psychology,validation,4.897678615059704
612,0.3464052379131317,0.3464052379131317,0.5,0.24343852124183007,mmlu_offline:llama2-13b_chat:professional_psychology,test,39.43379101296887
12,0.1666666716337204,0.1666666716337204,0.5,0.42317708333333337,mmlu_offline:llama2-13b_chat:public_relations,validation,0.8041464951820672
110,0.30909091234207153,0.30909091234207153,0.5,0.2807528409090909,mmlu_offline:llama2-13b_chat:public_relations,test,5.741335339844227
27,0.5555555820465088,0.5555555820465088,0.5,0.03428819444444442,mmlu_offline:llama2-13b_chat:security_studies,validation,2.268040655180812
245,0.6612244844436646,0.6612244844436646,0.5,0.07138073979591841,mmlu_offline:llama2-13b_chat:security_studies,test,19.732957255095243
22,0.5909090638160706,0.5909090638160706,0.5,0.0010653409090909394,mmlu_offline:llama2-13b_chat:sociology,validation,1.177910084836185
201,0.447761207818985,0.447761207818985,0.5,0.14208255597014924,mmlu_offline:llama2-13b_chat:sociology,test,9.612684376072139
11,0.7272727489471436,0.7272727489471436,0.5,0.1374289772727273,mmlu_offline:llama2-13b_chat:us_foreign_policy,validation,0.7174934288486838
100,0.5899999737739563,0.5899999737739563,0.5,0.00015624999999996891,mmlu_offline:llama2-13b_chat:us_foreign_policy,test,4.930804927833378
18,0.5555555820465088,0.5555555820465088,0.5,0.03428819444444442,mmlu_offline:llama2-13b_chat:virology,validation,1.246716202236712
166,0.34939759969711304,0.34939759969711304,0.5,0.2404461596385542,mmlu_offline:llama2-13b_chat:virology,test,7.690480432007462
19,0.6315789222717285,0.6315789222717285,0.5,0.04173519736842102,mmlu_offline:llama2-13b_chat:world_religions,validation,0.8721111817285419
171,0.6081871390342712,0.6081871390342712,0.5,0.018343384502923943,mmlu_offline:llama2-13b_chat:world_religions,test,6.203843832015991
