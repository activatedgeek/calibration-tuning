N,acc,unc_acc,unc_auroc,unc_ece,dataset,split,ts
11,0.09090909361839294,0.5454545617103577,0.8999999999999999,0.2329545562917536,mmlu_offline:llama2-13b:abstract_algebra,validation,2.2118365592323244
100,0.23000000417232513,0.4399999976158142,0.5686053077357425,0.22957031607627873,mmlu_offline:llama2-13b:abstract_algebra,test,5.158528205007315
14,0.2142857164144516,0.2142857164144516,1.0,0.5574776870863779,mmlu_offline:llama2-13b:anatomy,validation,0.6441024378873408
135,0.45185184478759766,0.4740740656852722,0.6537439078422684,0.28877314814814814,mmlu_offline:llama2-13b:anatomy,test,5.378747655078769
16,0.25,0.5625,0.9375,0.2751464881002903,mmlu_offline:llama2-13b:astronomy,validation,1.0929722781293094
152,0.5131579041481018,0.5855262875556946,0.6312370062370063,0.08665706256502555,mmlu_offline:llama2-13b:astronomy,test,8.555611666291952
11,0.4545454680919647,0.4545454680919647,0.7666666666666666,0.23615057360042227,mmlu_offline:llama2-13b:business_ethics,validation,0.9104221509769559
100,0.3199999928474426,0.3700000047683716,0.6909466911764706,0.3490624964237213,mmlu_offline:llama2-13b:business_ethics,test,7.3775134310126305
29,0.24137930572032928,0.3103448152542114,0.6266233766233765,0.34657866584843605,mmlu_offline:llama2-13b:clinical_knowledge,validation,1.4940732857212424
265,0.3207547068595886,0.44528302550315857,0.7819607843137255,0.22435140249864113,mmlu_offline:llama2-13b:clinical_knowledge,test,12.46195299597457
16,0.25,0.3125,0.53125,0.378662109375,mmlu_offline:llama2-13b:college_biology,validation,1.0724029266275465
144,0.3958333432674408,0.5069444179534912,0.6258318209316394,0.1769748259749678,mmlu_offline:llama2-13b:college_biology,test,8.775313430000097
8,0.125,0.125,1.0,0.5932617262005806,mmlu_offline:llama2-13b:college_chemistry,validation,0.6243054941296577
100,0.15000000596046448,0.27000001072883606,0.691764705882353,0.4147265565395355,mmlu_offline:llama2-13b:college_chemistry,test,7.194147311151028
11,0.1818181872367859,0.5454545617103577,0.9722222222222222,0.25639204003594135,mmlu_offline:llama2-13b:college_computer_science,validation,1.5091772410087287
100,0.2199999988079071,0.4099999964237213,0.6101398601398602,0.2721484488248825,mmlu_offline:llama2-13b:college_computer_science,test,12.926561175845563
11,0.1818181872367859,0.7272727489471436,0.8888888888888888,0.23792614178224045,mmlu_offline:llama2-13b:college_mathematics,validation,1.154349870979786
100,0.10000000149011612,0.7699999809265137,0.5638888888888889,0.11449218213558196,mmlu_offline:llama2-13b:college_mathematics,test,8.906387699767947
22,0.5,0.5909090638160706,0.8181818181818181,0.15269886363636365,mmlu_offline:llama2-13b:college_medicine,validation,1.5653109611012042
173,0.3583815097808838,0.52601158618927,0.7088782330717814,0.1402411281717995,mmlu_offline:llama2-13b:college_medicine,test,16.779592636041343
11,0.3636363744735718,0.6363636255264282,0.875,0.21342329545454547,mmlu_offline:llama2-13b:college_physics,validation,1.0773150189779699
102,0.21568627655506134,0.4215686321258545,0.6517045454545455,0.2339537377450981,mmlu_offline:llama2-13b:college_physics,test,7.923268426209688
11,0.3636363744735718,0.27272728085517883,0.3928571428571429,0.36860794912685046,mmlu_offline:llama2-13b:computer_security,validation,0.9230403052642941
100,0.46000000834465027,0.4099999964237213,0.5821256038647343,0.2844531261920929,mmlu_offline:llama2-13b:computer_security,test,5.268308850936592
26,0.3461538553237915,0.38461539149284363,0.588235294117647,0.3030348465992854,mmlu_offline:llama2-13b:conceptual_physics,validation,1.1918599302880466
235,0.44255319237709045,0.47659575939178467,0.5631605989430417,0.2014960027755575,mmlu_offline:llama2-13b:conceptual_physics,test,9.384385759010911
12,0.0833333358168602,0.1666666716337204,0.9090909090909091,0.4999999900658926,mmlu_offline:llama2-13b:econometrics,validation,1.2493575969710946
114,0.19298245012760162,0.4649122953414917,0.6534090909090908,0.16745477182823315,mmlu_offline:llama2-13b:econometrics,test,10.336684598121792
16,0.375,0.5,0.7833333333333333,0.1599121019244194,mmlu_offline:llama2-13b:electrical_engineering,validation,1.0689497157000005
145,0.2344827651977539,0.4689655303955078,0.6766030736618971,0.16675646962790652,mmlu_offline:llama2-13b:electrical_engineering,test,9.082253256812692
41,0.2926829159259796,0.5121951103210449,0.7140804597701149,0.16911204849801412,mmlu_offline:llama2-13b:elementary_mathematics,validation,3.0353637780062854
378,0.4021163880825043,0.5608465671539307,0.6987074988355845,0.12554769556989115,mmlu_offline:llama2-13b:elementary_mathematics,test,24.60696428315714
14,0.3571428656578064,0.3571428656578064,0.5555555555555556,0.27901785714285715,mmlu_offline:llama2-13b:formal_logic,validation,1.2586921518668532
126,0.190476194024086,0.2698412835597992,0.6207107843137255,0.3746279610527886,mmlu_offline:llama2-13b:formal_logic,test,9.505348230712116
10,0.30000001192092896,0.6000000238418579,0.9047619047619048,0.105859375,mmlu_offline:llama2-13b:global_facts,validation,0.6933280569501221
100,0.23999999463558197,0.36000001430511475,0.6430921052631579,0.2926171886920929,mmlu_offline:llama2-13b:global_facts,test,5.41211793711409
32,0.34375,0.3125,0.538961038961039,0.4045410081744194,mmlu_offline:llama2-13b:high_school_biology,validation,2.0409410116262734
310,0.47096773982048035,0.5193548202514648,0.5591171065820247,0.18685735541005294,mmlu_offline:llama2-13b:high_school_biology,test,18.973160708788782
22,0.04545454680919647,0.27272728085517883,0.761904761904762,0.34197443181818177,mmlu_offline:llama2-13b:high_school_chemistry,validation,1.691411407198757
203,0.19704432785511017,0.3497537076473236,0.6142638036809815,0.3160214007194405,mmlu_offline:llama2-13b:high_school_chemistry,test,14.317782732192427
9,0.2222222238779068,0.5555555820465088,0.8571428571428571,0.3901909722222222,mmlu_offline:llama2-13b:high_school_computer_science,validation,1.1654206616804004
100,0.41999998688697815,0.5600000023841858,0.6685139573070608,0.13761719703674316,mmlu_offline:llama2-13b:high_school_computer_science,test,11.467653325758874
18,0.7222222089767456,0.6666666865348816,0.4923076923076923,0.2777777844005161,mmlu_offline:llama2-13b:high_school_european_history,validation,9.317152658011764
165,0.6000000238418579,0.6121212244033813,0.5941230486685032,0.11356533332304526,mmlu_offline:llama2-13b:high_school_european_history,test,86.1702283769846
22,0.40909090638160706,0.4545454680919647,0.7350427350427351,0.20614345507188278,mmlu_offline:llama2-13b:high_school_geography,validation,1.0959414299577475
198,0.3787878751754761,0.5505050420761108,0.7478048780487805,0.11422822601867444,mmlu_offline:llama2-13b:high_school_geography,test,8.516876223031431
21,0.4761904776096344,0.4285714328289032,0.44545454545454544,0.21837799605869115,mmlu_offline:llama2-13b:high_school_government_and_politics,validation,1.2253962559625506
193,0.5544041395187378,0.5544041395187378,0.5986742012605957,0.10136009868562529,mmlu_offline:llama2-13b:high_school_government_and_politics,test,9.768010913860053
43,0.41860464215278625,0.5813953280448914,0.5722222222222223,0.21093748613845473,mmlu_offline:llama2-13b:high_school_macroeconomics,validation,2.0266121458262205
390,0.34358975291252136,0.5923076868057251,0.7104127798507462,0.08787058133345382,mmlu_offline:llama2-13b:high_school_macroeconomics,test,17.918978694360703
29,0.13793103396892548,0.7931034564971924,0.725,0.08795796591660074,mmlu_offline:llama2-13b:high_school_mathematics,validation,2.419357005972415
270,0.10000000149011612,0.7518518567085266,0.7316720012193263,0.0490162240134345,mmlu_offline:llama2-13b:high_school_mathematics,test,21.31645578239113
26,0.38461539149284363,0.3076923191547394,0.503125,0.34314905221645653,mmlu_offline:llama2-13b:high_school_microeconomics,validation,1.2680766368284822
238,0.3781512677669525,0.462184876203537,0.6504504504504506,0.21737129873588304,mmlu_offline:llama2-13b:high_school_microeconomics,test,10.556047425605357
17,0.11764705926179886,0.29411765933036804,0.7666666666666667,0.36695773110670205,mmlu_offline:llama2-13b:high_school_physics,validation,1.438112970907241
151,0.20529800653457642,0.430463582277298,0.6146505376344086,0.22829573912336337,mmlu_offline:llama2-13b:high_school_physics,test,11.366477902978659
60,0.6000000238418579,0.550000011920929,0.5052083333333334,0.1907552142937978,mmlu_offline:llama2-13b:high_school_psychology,validation,3.8883795719593763
545,0.5357798337936401,0.5981651544570923,0.6718758460122367,0.10942518722026723,mmlu_offline:llama2-13b:high_school_psychology,test,34.34580770274624
23,0.043478261679410934,0.6086956262588501,1.0,0.15947689958240674,mmlu_offline:llama2-13b:high_school_statistics,validation,2.473221848718822
216,0.25925925374031067,0.5462962985038757,0.5766183035714286,0.10170718475624364,mmlu_offline:llama2-13b:high_school_statistics,test,22.260088280774653
22,0.5909090638160706,0.5,0.23076923076923078,0.31356534090909094,mmlu_offline:llama2-13b:high_school_us_history,validation,8.862619650084525
204,0.6225489974021912,0.6617646813392639,0.7269148174659985,0.06667432364295511,mmlu_offline:llama2-13b:high_school_us_history,test,81.75890620192513
26,0.6153846383094788,0.6538461446762085,0.615625,0.08293269001520596,mmlu_offline:llama2-13b:high_school_world_history,validation,7.459925351198763
237,0.5274261832237244,0.5696202516555786,0.6102142857142858,0.09782107142959463,mmlu_offline:llama2-13b:high_school_world_history,test,62.170748765114695
23,0.3478260934352875,0.3478260934352875,0.5416666666666667,0.3496943038442861,mmlu_offline:llama2-13b:human_aging,validation,1.0512388199567795
223,0.3497757911682129,0.3856502175331116,0.7345711759504863,0.35786854926780737,mmlu_offline:llama2-13b:human_aging,test,9.196542102843523
12,0.25,0.25,0.2962962962962963,0.3567708333333333,mmlu_offline:llama2-13b:human_sexuality,validation,0.7403950523585081
131,0.442748099565506,0.5267175436019897,0.7361832782239017,0.1730975758938389,mmlu_offline:llama2-13b:human_sexuality,test,6.849620557855815
13,0.3076923191547394,0.5384615659713745,0.5416666666666667,0.33353365384615385,mmlu_offline:llama2-13b:international_law,validation,0.9075408019125462
121,0.5206611752510071,0.5537189841270447,0.5481663929939792,0.11854340764116653,mmlu_offline:llama2-13b:international_law,test,7.538920645136386
11,0.3636363744735718,0.6363636255264282,0.6607142857142857,0.27166192639957776,mmlu_offline:llama2-13b:jurisprudence,validation,0.7015059050172567
108,0.39814814925193787,0.42592594027519226,0.579427549194991,0.2894241870553405,mmlu_offline:llama2-13b:jurisprudence,test,5.567643403075635
18,0.5555555820465088,0.6111111044883728,0.7375,0.18988716271188522,mmlu_offline:llama2-13b:logical_fallacies,validation,1.099034187849611
163,0.4601227045059204,0.6012269854545593,0.6806060606060605,0.08910084611799088,mmlu_offline:llama2-13b:logical_fallacies,test,9.035224505700171
11,0.4545454680919647,0.27272728085517883,0.4,0.36257102272727276,mmlu_offline:llama2-13b:machine_learning,validation,1.012760051060468
112,0.2232142835855484,0.3392857015132904,0.5103448275862069,0.3160225892705577,mmlu_offline:llama2-13b:machine_learning,test,9.382924390025437
11,0.8181818127632141,0.7272727489471436,0.5555555555555556,0.2578125,mmlu_offline:llama2-13b:management,validation,0.5692340089008212
103,0.43689319491386414,0.5242718458175659,0.6565134099616858,0.1935679490126452,mmlu_offline:llama2-13b:management,test,3.832591475918889
25,0.36000001430511475,0.36000001430511475,0.7291666666666666,0.4620312380790711,mmlu_offline:llama2-13b:marketing,validation,1.4862243933603168
234,0.5170940160751343,0.5213675498962402,0.6095224164411613,0.279697509148182,mmlu_offline:llama2-13b:marketing,test,11.734246503561735
11,0.7272727489471436,0.6363636255264282,0.5833333333333334,0.14240058443763037,mmlu_offline:llama2-13b:medical_genetics,validation,0.5970364059321582
100,0.4699999988079071,0.5600000023841858,0.741870734644721,0.16789062500000002,mmlu_offline:llama2-13b:medical_genetics,test,4.106459470000118
86,0.604651153087616,0.6511628031730652,0.7751696832579186,0.09270530176717182,mmlu_offline:llama2-13b:miscellaneous,validation,3.526587758678943
783,0.6168582439422607,0.6679438352584839,0.797311939268461,0.0847302043422733,mmlu_offline:llama2-13b:miscellaneous,test,32.324939772021025
38,0.3947368562221527,0.44736841320991516,0.7043478260869565,0.25534539473684204,mmlu_offline:llama2-13b:moral_disputes,validation,2.2020241022109985
346,0.424855500459671,0.4826589524745941,0.6572659214439545,0.1963737327928488,mmlu_offline:llama2-13b:moral_disputes,test,18.536473601590842
100,0.47999998927116394,0.6000000238418579,0.577724358974359,0.0791406458616257,mmlu_offline:llama2-13b:moral_scenarios,validation,9.944755458272994
895,0.44134077429771423,0.6111732125282288,0.6121518987341772,0.08556213678594411,mmlu_offline:llama2-13b:moral_scenarios,test,87.8453890378587
33,0.3030303120613098,0.39393940567970276,0.6239130434782609,0.290364576108528,mmlu_offline:llama2-13b:nutrition,validation,2.285170543938875
306,0.36274510622024536,0.44771242141723633,0.6849387849387849,0.24359171413907818,mmlu_offline:llama2-13b:nutrition,test,19.795133911073208
34,0.3235294222831726,0.4117647111415863,0.642292490118577,0.35409007002325615,mmlu_offline:llama2-13b:philosophy,validation,1.7024575360119343
311,0.34405145049095154,0.3826366662979126,0.6298790544255086,0.3489122649481059,mmlu_offline:llama2-13b:philosophy,test,13.836472089868039
35,0.3142857253551483,0.4285714328289032,0.6950757575757576,0.2957589200564793,mmlu_offline:llama2-13b:prehistory,validation,2.1088247508741915
324,0.48148149251937866,0.5123456716537476,0.6795634920634921,0.21204667271655284,mmlu_offline:llama2-13b:prehistory,test,16.901958560105413
31,0.09677419066429138,0.4838709533214569,0.5654761904761905,0.15561995967741934,mmlu_offline:llama2-13b:professional_accounting,validation,3.4924053070135415
282,0.14539006352424622,0.5,0.4839591134500556,0.13623392560803302,mmlu_offline:llama2-13b:professional_accounting,test,31.133818124886602
170,0.364705890417099,0.5470588207244873,0.48021206690561524,0.07270222762051748,mmlu_offline:llama2-13b:professional_law,validation,42.131574872881174
1534,0.29400262236595154,0.5912646651268005,0.5263434288837978,0.020580381465859623,mmlu_offline:llama2-13b:professional_law,test,388.5366262481548
31,0.35483869910240173,0.6451612710952759,0.6113636363636363,0.11693547041185441,mmlu_offline:llama2-13b:professional_medicine,validation,6.094836415722966
272,0.3345588147640228,0.5367646813392639,0.5788051727278247,0.10918830313226756,mmlu_offline:llama2-13b:professional_medicine,test,54.09163077408448
69,0.3913043439388275,0.49275362491607666,0.5833333333333333,0.16021286491034686,mmlu_offline:llama2-13b:professional_psychology,validation,4.950537391938269
612,0.3006536066532135,0.4689542353153229,0.5957309020723283,0.18744895056961408,mmlu_offline:llama2-13b:professional_psychology,test,39.550717985257506
12,0.25,0.3333333432674408,0.42592592592592593,0.3538411458333334,mmlu_offline:llama2-13b:public_relations,validation,0.7944400645792484
110,0.30909091234207153,0.40909090638160706,0.7308436532507739,0.2871448760682887,mmlu_offline:llama2-13b:public_relations,test,5.8672891613096
27,0.48148149251937866,0.48148149251937866,0.5824175824175825,0.20413772044358433,mmlu_offline:llama2-13b:security_studies,validation,2.2254947046749294
245,0.5265306234359741,0.5795918107032776,0.6087610264635124,0.10443240063531058,mmlu_offline:llama2-13b:security_studies,test,19.541924589779228
22,0.5909090638160706,0.6818181872367859,0.6837606837606838,0.20490056818181818,mmlu_offline:llama2-13b:sociology,validation,1.1271884590387344
201,0.4129353165626526,0.49751242995262146,0.6556565244026955,0.1754508587851453,mmlu_offline:llama2-13b:sociology,test,9.427752862218767
11,0.6363636255264282,0.7272727489471436,0.75,0.13139202919873325,mmlu_offline:llama2-13b:us_foreign_policy,validation,0.6966906166635454
100,0.5699999928474426,0.6200000047683716,0.6615667074663403,0.07273437261581421,mmlu_offline:llama2-13b:us_foreign_policy,test,4.88783696340397
18,0.3333333432674408,0.5,0.6944444444444444,0.19921874006589255,mmlu_offline:llama2-13b:virology,validation,1.2228195942007005
166,0.33734938502311707,0.4337349534034729,0.6316558441558442,0.24169333404805285,mmlu_offline:llama2-13b:virology,test,7.723159254994243
19,0.6315789222717285,0.5789473652839661,0.6071428571428572,0.1979851910942479,mmlu_offline:llama2-13b:world_religions,validation,0.853956594131887
171,0.6432748436927795,0.6666666865348816,0.6545454545454545,0.1310306940859521,mmlu_offline:llama2-13b:world_religions,test,6.214006658177823
