N,acc,unc_acc,unc_auroc,unc_ece,dataset,split,ts
11,0.9090909361839294,0.7272727489471436,0.2,0.14434049346230252,mmlu:us_foreign_policy,validation,42.64754605665803
100,0.8100000023841858,0.7699999809265137,0.4860298895386615,0.1209840190410614,mmlu:us_foreign_policy,test,321.9849327467382
29,0.24137930572032928,0.5862069129943848,0.6915584415584416,0.21029950215898713,mmlu:high_school_mathematics,validation,87.01004047319293
270,0.1518518477678299,0.6629629731178284,0.537543934391309,0.1434307409657372,mmlu:high_school_mathematics,test,779.3802445232868
170,0.6647058725357056,0.6176470518112183,0.4895202608290638,0.3244575903696172,mmlu:professional_law,validation,531.4275845587254
1534,0.5990873575210571,0.5677965879440308,0.5492369755035962,0.38175972639504124,mmlu:professional_law,test,4670.179186809808
29,0.4482758641242981,0.6896551847457886,0.23317307692307693,0.18915202905391823,mmlu:clinical_knowledge,validation,84.38815404661
265,0.4075471758842468,0.6188679337501526,0.46602972399150744,0.21333222096821045,mmlu:clinical_knowledge,test,804.9485973417759
12,0.5,0.75,0.3888888888888889,0.13995699087778726,mmlu:human_sexuality,validation,48.879722468554974
131,0.5190839767456055,0.6870229244232178,0.3745331465919701,0.1501594831925312,mmlu:human_sexuality,test,463.2613273561001
23,0.30434784293174744,0.5652173757553101,0.6160714285714285,0.2917532221130703,mmlu:high_school_statistics,validation,71.02958885580301
216,0.3611111044883728,0.5601851940155029,0.6395856558900037,0.28458367481275837,mmlu:high_school_statistics,test,643.6139840204269
22,0.7727273106575012,0.7272727489471436,0.4764705882352941,0.20271755348552356,mmlu:sociology,validation,37.60378262773156
201,0.5572139024734497,0.641791045665741,0.3736456661316212,0.18626159963323108,mmlu:sociology,test,389.8895356319845
9,0.4444444477558136,0.3333333432674408,0.5,0.5453778107961019,mmlu:high_school_computer_science,validation,21.505143105983734
100,0.5299999713897705,0.550000011920929,0.5678442392613409,0.3679236960411072,mmlu:high_school_computer_science,test,214.54532271623611
23,0.52173912525177,0.52173912525177,0.4734848484848485,0.4381398195805757,mmlu:human_aging,validation,61.2858498133719
223,0.452914834022522,0.5605381727218628,0.5029216036357734,0.31039979211952656,mmlu:human_aging,test,584.0491095259786
14,0.5,0.6428571939468384,0.3469387755102041,0.29806757399014067,mmlu:formal_logic,validation,31.388845685869455
126,0.3650793731212616,0.4206349551677704,0.6012228260869564,0.4834580071388729,mmlu:formal_logic,test,259.5368055701256
25,0.2800000011920929,0.4399999976158142,0.626984126984127,0.39676894664764406,mmlu:marketing,validation,83.471928011626
234,0.47435900568962097,0.6239316463470459,0.4637808540247565,0.22036715501394027,mmlu:marketing,test,1005.73386836797
22,0.40909093618392944,0.40909093618392944,0.5256410256410255,0.4537168036807667,mmlu:high_school_geography,validation,54.47063208371401
198,0.4545454680919647,0.5909090638160706,0.3974279835390946,0.26068250157616357,mmlu:high_school_geography,test,519.9756089728326
14,0.3571428656578064,0.8571429252624512,0.6444444444444445,0.10454735585621425,mmlu:anatomy,validation,36.00822048820555
135,0.5333333015441895,0.6222221851348877,0.4115961199294533,0.1354626946979099,mmlu:anatomy,test,324.46879705786705
8,0.125,0.5,0.0,0.4246338680386543,mmlu:college_chemistry,validation,23.667598098516464
100,0.19999998807907104,0.7099999785423279,0.4190625,0.24156305491924288,mmlu:college_chemistry,test,279.0621824571863
11,0.3636363744735718,0.6363636255264282,0.21428571428571433,0.2635393847118725,mmlu:college_physics,validation,36.65547590889037
102,0.18627451360225677,0.6568627953529358,0.48351299936588454,0.23503479186226336,mmlu:college_physics,test,473.94929547049105
12,0.4166666865348816,0.5833333730697632,0.6428571428571429,0.33905766904354095,mmlu:econometrics,validation,38.236731411889195
114,0.25438597798347473,0.5,0.4910750507099392,0.3326950470606486,mmlu:econometrics,test,335.01265132054687
16,0.375,0.625,0.3916666666666667,0.40401098132133484,mmlu:electrical_engineering,validation,42.828515669330955
145,0.4413793087005615,0.6206896305084229,0.5411844135802469,0.2345403284862123,mmlu:electrical_engineering,test,378.2352889161557
11,0.27272728085517883,0.8181818723678589,0.75,0.1974533850496465,mmlu:college_computer_science,validation,33.50309771671891
100,0.29999998211860657,0.5899999737739563,0.5878571428571429,0.33149765312671664,mmlu:college_computer_science,test,281.0599090475589
21,0.7142857313156128,0.7142857313156128,0.23888888888888887,0.17880011172521681,mmlu:high_school_government_and_politics,validation,50.71269410289824
193,0.6113989353179932,0.5958548784255981,0.3237853107344633,0.21995570128445796,mmlu:high_school_government_and_politics,test,458.0833476949483
19,0.6315789222717285,0.6842105388641357,0.5178571428571429,0.18951428563971268,mmlu:world_religions,validation,48.921058755367994
171,0.6842105388641357,0.6900584697723389,0.5288857233301678,0.11044286217605859,mmlu:world_religions,test,408.2931751050055
43,0.44186046719551086,0.6744186282157898,0.5932017543859649,0.22186549735623737,mmlu:high_school_macroeconomics,validation,120.40022975578904
390,0.5179487466812134,0.5564102530479431,0.5278597008637035,0.2459794085759383,mmlu:high_school_macroeconomics,test,1044.7629628088325
22,0.7272727489471436,0.5909091234207153,0.17708333333333331,0.23751266707073562,mmlu:high_school_us_history,validation,55.83383845537901
204,0.7254902124404907,0.5441176891326904,0.40968870656370665,0.17209106274679592,mmlu:high_school_us_history,test,494.20640115626156
35,0.5714285969734192,0.6285714507102966,0.5633333333333335,0.22920048577444896,mmlu:prehistory,validation,86.99674457684159
324,0.5586419701576233,0.6265432238578796,0.5332843951628482,0.15828431701218643,mmlu:prehistory,test,743.6608121600002
100,0.41999998688697815,0.5199999809265137,0.5798440065681444,0.3155116832256317,mmlu:moral_scenarios,validation,283.41701735183597
895,0.4435754120349884,0.5318435430526733,0.4553781878142292,0.2772257155546263,mmlu:moral_scenarios,test,2541.5470783151686
11,0.8181818723678589,0.6363636255264282,0.2222222222222222,0.2982841730117798,mmlu:computer_security,validation,26.626973651349545
100,0.6699999570846558,0.6399999856948853,0.4228855721393035,0.27358515322208404,mmlu:computer_security,test,265.051259836182
60,0.6500000357627869,0.7833333611488342,0.5555555555555556,0.12619242370128636,mmlu:high_school_psychology,validation,138.87281158566475
545,0.60550457239151,0.673394501209259,0.5622621564482029,0.18793157382842599,mmlu:high_school_psychology,test,1281.44022609666
11,0.7272727489471436,0.5454545617103577,0.16666666666666669,0.33209631659767846,mmlu:jurisprudence,validation,40.546741899102926
108,0.6759259104728699,0.6203703880310059,0.5448140900195694,0.2345119963089625,mmlu:jurisprudence,test,326.00996662676334
11,0.7272727489471436,0.5454545617103577,0.7083333333333333,0.3742413737557151,mmlu:machine_learning,validation,41.61624089255929
112,0.4642857313156128,0.5267857313156128,0.6415064102564103,0.38622337739382473,mmlu:machine_learning,test,412.6730195097625
10,0.6000000238418579,0.6000000238418579,0.625,0.3281390130519867,mmlu:global_facts,validation,23.707915291190147
100,0.22999998927116394,0.5399999618530273,0.5719932241671373,0.3038306659460068,mmlu:global_facts,test,251.48051629029214
18,0.6666666865348816,0.6111111044883728,0.5833333333333334,0.2216857936647203,mmlu:logical_fallacies,validation,39.09455609880388
163,0.5889570713043213,0.6073619723320007,0.5388681592039801,0.19727174709179646,mmlu:logical_fallacies,test,374.0007815482095
16,0.25,0.375,0.46875,0.585978839546442,mmlu:college_biology,validation,52.20230759307742
144,0.4375,0.6388888955116272,0.4950029394473839,0.29581397109561497,mmlu:college_biology,test,383.1482681520283
22,0.27272728085517883,0.40909093618392944,0.7604166666666666,0.520291802558032,mmlu:high_school_chemistry,validation,66.88236498832703
203,0.2857142686843872,0.5073891282081604,0.6200356718192628,0.3997853345471649,mmlu:high_school_chemistry,test,579.2564176619053
31,0.32258063554763794,0.4516128897666931,0.6785714285714285,0.4134450170301621,mmlu:professional_accounting,validation,88.72959447279572
282,0.21985815465450287,0.5567375421524048,0.6339809384164221,0.2906110906009133,mmlu:professional_accounting,test,814.0826666839421
16,0.5625,0.625,0.4285714285714286,0.3399568647146225,mmlu:astronomy,validation,42.99082463607192
152,0.6513158082962036,0.7302631735801697,0.5603201829616924,0.1757193458707709,mmlu:astronomy,test,389.7538151368499
86,0.6860464811325073,0.5116279125213623,0.3625235404896422,0.2749370592971181,mmlu:miscellaneous,validation,185.8894737586379
783,0.6692209243774414,0.58620685338974,0.352169235757022,0.21603584076405186,mmlu:miscellaneous,test,1695.7503189705312
22,0.5454545617103577,0.5909091234207153,0.36250000000000004,0.32130198587070813,mmlu:college_medicine,validation,60.403066583909094
173,0.4393063485622406,0.6416184902191162,0.43170103092783507,0.22991414462899884,mmlu:college_medicine,test,435.51699085440487
12,0.3333333432674408,0.5833333730697632,0.75,0.3515466650327047,mmlu:public_relations,validation,29.00602332316339
110,0.3636363446712494,0.5909090638160706,0.39285714285714285,0.17507618015462706,mmlu:public_relations,test,239.78432607278228
31,0.5483870506286621,0.7419354915618896,0.21638655462184875,0.0919581324823441,mmlu:professional_medicine,validation,93.2824536729604
272,0.47058823704719543,0.5845588445663452,0.41582573784722215,0.1618951865855385,mmlu:professional_medicine,test,849.5621203407645
11,0.8181818723678589,0.8181818723678589,0.9722222222222222,0.1679005135189403,mmlu:medical_genetics,validation,33.35257652401924
100,0.5600000023841858,0.7799999713897705,0.39630681818181823,0.06739654064178467,mmlu:medical_genetics,test,292.40547027625144
18,0.8888888955116272,0.5555555820465088,0.34375,0.30684999624888104,mmlu:high_school_european_history,validation,32.195829816162586
165,0.8060605525970459,0.6969696879386902,0.2961701127819549,0.08537658886475999,mmlu:high_school_european_history,test,344.12073135748506
69,0.47826087474823,0.6086956858634949,0.3766835016835017,0.17094289651815445,mmlu:professional_psychology,validation,173.18524062633514
612,0.4215686321258545,0.6356209516525269,0.44652476678491665,0.13575734412358476,mmlu:professional_psychology,test,2234.389073256403
41,0.3658536374568939,0.5365853309631348,0.5025641025641026,0.2391178448025773,mmlu:elementary_mathematics,validation,101.45090076327324
378,0.4417989253997803,0.5608465671539307,0.49919119107756055,0.23336066833879582,mmlu:elementary_mathematics,test,992.2644771263003
27,0.6296296119689941,0.6666666865348816,0.6588235294117647,0.3153905846454479,mmlu:security_studies,validation,101.16612770035863
245,0.7836734056472778,0.6897959113121033,0.44894850628930816,0.22125755597134028,mmlu:security_studies,test,873.7534610033035
11,0.27272728085517883,0.5454545617103577,0.41666666666666663,0.397546567700126,mmlu:college_mathematics,validation,33.658708944916725
100,0.14000000059604645,0.6499999761581421,0.6287375415282392,0.25123209834098814,mmlu:college_mathematics,test,289.02387541066855
34,0.5,0.5588235259056091,0.5155709342560554,0.27641765335026913,mmlu:philosophy,validation,77.51712428964674
311,0.43086814880371094,0.6173633337020874,0.5281642634286196,0.19400900621506176,mmlu:philosophy,test,789.9036429282278
32,0.34375,0.65625,0.5216450216450217,0.24405238404870033,mmlu:high_school_biology,validation,87.63379455544055
310,0.5645161271095276,0.6709677577018738,0.39767195767195773,0.2011642802146173,mmlu:high_school_biology,test,750.3544961325824
13,0.6153846383094788,0.692307710647583,0.5625,0.2723954411653372,mmlu:international_law,validation,32.37546059116721
121,0.7933883666992188,0.5950412750244141,0.42208333333333337,0.25723477680821066,mmlu:international_law,test,269.62893464881927
26,0.5769230723381042,0.5769230723381042,0.5,0.2232917524301089,mmlu:high_school_world_history,validation,51.913966501131654
237,0.7172995209693909,0.7341771721839905,0.47677787532923616,0.11369183496080874,mmlu:high_school_world_history,test,516.7793210931122
38,0.5789473652839661,0.5789473652839661,0.5795454545454545,0.197825888269826,mmlu:moral_disputes,validation,94.33187957387418
346,0.5693641304969788,0.6069363951683044,0.47269444349810924,0.17174377365608434,mmlu:moral_disputes,test,879.527825572528
11,0.27272728085517883,0.5454545617103577,0.41666666666666663,0.5527740662748164,mmlu:abstract_algebra,validation,33.408016027882695
100,0.2800000011920929,0.4599999785423279,0.48487103174603174,0.4559531146287919,mmlu:abstract_algebra,test,283.49839749746025
18,0.5,0.5555555820465088,0.46913580246913583,0.3677556680308448,mmlu:virology,validation,54.4264077283442
166,0.6204819083213806,0.668674647808075,0.5525504700261982,0.22327015055231297,mmlu:virology,test,426.9892401769757
11,0.5454545617103577,0.8181818723678589,0.8333333333333333,0.19227730686014352,mmlu:business_ethics,validation,29.368643942987546
100,0.47999998927116394,0.5099999904632568,0.5286458333333333,0.38349723100662236,mmlu:business_ethics,test,239.33177673094906
17,0.11764705926179886,0.529411792755127,0.4,0.48388015522676353,mmlu:high_school_physics,validation,48.48873225226998
151,0.33774834871292114,0.49668875336647034,0.5307843137254902,0.432941427688725,mmlu:high_school_physics,test,396.7887662369758
26,0.3076923191547394,0.6538462042808533,0.3819444444444445,0.3191074728965759,mmlu:conceptual_physics,validation,70.31334572099149
235,0.42978721857070923,0.6255319118499756,0.417171567903059,0.22690415559931004,mmlu:conceptual_physics,test,636.0870258808136
11,0.6363636255264282,0.5454545617103577,0.25,0.39729872074994177,mmlu:management,validation,31.85556123405695
103,0.5339806079864502,0.6116504669189453,0.3560606060606061,0.20829263126965866,mmlu:management,test,371.5620411299169
