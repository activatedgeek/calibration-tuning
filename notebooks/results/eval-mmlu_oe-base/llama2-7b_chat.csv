N,acc,unc_acc,unc_auroc,unc_ece,dataset,split,ts
86,0.38372093439102173,0.38372093439102173,0.5228702115494569,0.5656576752662659,mmlu:miscellaneous,validation,87.06338897719979
783,0.4431672990322113,0.4431672990322113,0.6052005393543611,0.5074793276963411,mmlu:miscellaneous,test,903.9906488656998
34,0.2647058963775635,0.2647058963775635,0.731111111111111,0.6932076724136577,mmlu:philosophy,validation,68.25278198160231
311,0.34726688265800476,0.34405145049095154,0.5139345010034665,0.6174182213387689,mmlu:philosophy,test,668.3425274118781
11,0.3636363744735718,0.3636363744735718,0.6964285714285714,0.6199682463299144,mmlu:business_ethics,validation,11.24025808647275
100,0.3199999928474426,0.3199999928474426,0.6865808823529412,0.6612361460924148,mmlu:business_ethics,test,113.32374287769198
16,0.4375,0.4375,0.4603174603174603,0.4490511380136013,mmlu:astronomy,validation,26.058981653302908
152,0.43421053886413574,0.44078949093818665,0.6362755461592671,0.44468196324611964,mmlu:astronomy,test,189.73110265657306
9,0.3333333432674408,0.3333333432674408,0.6666666666666667,0.5785217881202698,mmlu:high_school_computer_science,validation,17.429811832960695
100,0.41999998688697815,0.41999998688697815,0.541256157635468,0.4908927112817765,mmlu:high_school_computer_science,test,194.0416370839812
22,0.8181818723678589,0.8181818723678589,0.5763888888888888,0.1642850583249872,mmlu:high_school_us_history,validation,35.16833790205419
204,0.6078431606292725,0.6078431606292725,0.5424899193548387,0.3728608161795373,mmlu:high_school_us_history,test,331.87066508457065
12,0.1666666716337204,0.4166666865348816,0.675,0.5338266094525654,mmlu:econometrics,validation,16.028173508122563
114,0.11403508484363556,0.38596493005752563,0.6016755521706018,0.3797800258586281,mmlu:econometrics,test,192.215360481292
43,0.4651162624359131,0.4651162624359131,0.46413043478260874,0.4250028868054235,mmlu:high_school_macroeconomics,validation,89.1581929475069
390,0.3102564215660095,0.3153846263885498,0.46631232910381276,0.5660925914079715,mmlu:high_school_macroeconomics,test,761.5628100093454
13,0.5384615659713745,0.5384615659713745,0.5119047619047619,0.3334536506579473,mmlu:international_law,validation,22.014456810429692
121,0.5785123705863953,0.5702478885650635,0.5718487394957983,0.30827296814642663,mmlu:international_law,test,173.27705248072743
17,0.0,0.0,,0.9796741604804993,mmlu:high_school_physics,validation,21.762914327904582
151,0.125827819108963,0.125827819108963,0.5833333333333334,0.8505370360336556,mmlu:high_school_physics,test,181.64776351489127
69,0.37681159377098083,0.37681159377098083,0.4695885509838998,0.5372227724047675,mmlu:professional_psychology,validation,131.69352380931377
612,0.3186274468898773,0.3186274468898773,0.47574863186373983,0.5960755729013019,mmlu:professional_psychology,test,1149.2961675710976
14,0.2142857313156128,0.2142857313156128,0.8333333333333334,0.7069927453994751,mmlu:anatomy,validation,15.566700630821288
135,0.4000000059604645,0.4000000059604645,0.6225422953818016,0.5146789798030147,mmlu:anatomy,test,136.012794168666
12,0.4166666865348816,0.4166666865348816,0.24285714285714288,0.5535290936628977,mmlu:public_relations,validation,18.769995968788862
110,0.3545454442501068,0.37272727489471436,0.5541711809317443,0.590754148093137,mmlu:public_relations,test,118.6325465478003
38,0.42105263471603394,0.42105263471603394,0.4161931818181819,0.45941914382733795,mmlu:moral_disputes,validation,88.73598594218493
346,0.46820807456970215,0.46820807456970215,0.5721618357487923,0.4160172972031412,mmlu:moral_disputes,test,756.3099492490292
11,0.27272728085517883,0.27272728085517883,0.5,0.6670043089173057,mmlu:management,validation,34.13506846502423
103,0.3786407709121704,0.3786407709121704,0.5286458333333333,0.5789348841870873,mmlu:management,test,229.84109795093536
26,0.3076923191547394,0.3076923191547394,0.5243055555555556,0.6410991893364834,mmlu:high_school_microeconomics,validation,34.01455000042915
238,0.3613445460796356,0.3613445460796356,0.5774173806609547,0.5740937224957121,mmlu:high_school_microeconomics,test,337.7737202811986
41,0.31707316637039185,0.31707316637039185,0.5357142857142857,0.6148234518562874,mmlu:elementary_mathematics,validation,40.34633016586304
378,0.2936507761478424,0.2936507761478424,0.5573438607146473,0.6455694758702839,mmlu:elementary_mathematics,test,449.5246436186135
22,0.4545454680919647,0.4545454680919647,0.38333333333333336,0.5035722093148665,mmlu:sociology,validation,63.206520453095436
201,0.4427860677242279,0.4427860677242279,0.6339285714285714,0.5179367898708553,mmlu:sociology,test,619.4804579168558
26,0.6538462042808533,0.6538462042808533,0.6111111111111112,0.3197797445150522,mmlu:high_school_world_history,validation,47.028662364929914
237,0.5738396644592285,0.5738396644592285,0.5234784507862551,0.3975189320648773,mmlu:high_school_world_history,test,418.30831809714437
16,0.1875,0.25,0.5641025641025641,0.5899425260722637,mmlu:electrical_engineering,validation,20.41110648191534
145,0.22068965435028076,0.22758620977401733,0.486587389380531,0.648050226014236,mmlu:electrical_engineering,test,183.78291827603243
29,0.03448275849223137,0.03448275849223137,0.8928571428571429,0.8736418074574965,mmlu:high_school_mathematics,validation,27.022630419000052
270,0.051851850003004074,0.051851850003004074,0.39076450892857145,0.8702063856301484,mmlu:high_school_mathematics,test,224.42259800000465
31,0.3870967626571655,0.3870967626571655,0.581140350877193,0.5683726476084802,mmlu:professional_medicine,validation,32.58074436709285
272,0.23529411852359772,0.23529411852359772,0.5029672475961539,0.7217928291681934,mmlu:professional_medicine,test,292.65781942009926
10,0.20000000298023224,0.20000000298023224,0.0625,0.6985438227653503,mmlu:global_facts,validation,15.825399791821837
100,0.07999999821186066,0.07999999821186066,0.5944293478260869,0.8288763821125031,mmlu:global_facts,test,136.87874841690063
11,0.09090909361839294,0.1818181872367859,0.95,0.776594254103574,mmlu:college_physics,validation,13.646762186661363
102,0.09803921729326248,0.09803921729326248,0.5228260869565218,0.8231687913922703,mmlu:college_physics,test,120.26855087280273
18,0.4444444477558136,0.4444444477558136,0.75,0.5136673582924737,mmlu:logical_fallacies,validation,26.790561438072473
163,0.4417177736759186,0.4417177736759186,0.5128968253968255,0.5135977809414541,mmlu:logical_fallacies,test,207.91276823403314
35,0.3142857253551483,0.3142857253551483,0.7007575757575757,0.6497188414846148,mmlu:prehistory,validation,83.6572891920805
324,0.37345680594444275,0.37345680594444275,0.5294548711476611,0.598178580219363,mmlu:prehistory,test,753.3027915414423
22,0.4545454680919647,0.4545454680919647,0.4666666666666666,0.47213820977644483,mmlu:high_school_geography,validation,34.12063476629555
198,0.4343434274196625,0.4343434274196625,0.6517338039867109,0.4762727091408739,mmlu:high_school_geography,test,258.19028028845787
25,0.23999999463558197,0.23999999463558197,0.5,0.7338564825057983,mmlu:marketing,validation,46.21260333806276
234,0.41880345344543457,0.41880345344543457,0.5601740696278511,0.5576885579488217,mmlu:marketing,test,382.25920489802957
23,0.30434784293174744,0.30434784293174744,0.5178571428571428,0.6583386530046877,mmlu:human_aging,validation,27.802769592031837
223,0.33183857798576355,0.33183857798576355,0.5487937602031562,0.6282625476341076,mmlu:human_aging,test,321.8807130660862
11,0.7272727489471436,0.7272727489471436,0.2916666666666667,0.22942265055396338,mmlu:medical_genetics,validation,33.15169273875654
100,0.38999998569488525,0.38999998569488525,0.6349306431273645,0.5540623939037324,mmlu:medical_genetics,test,249.43267532251775
26,0.26923078298568726,0.26923078298568726,0.5789473684210527,0.6679489681353936,mmlu:conceptual_physics,validation,52.545022912323475
235,0.3914893567562103,0.3914893567562103,0.5166844025539677,0.5406647745599138,mmlu:conceptual_physics,test,471.5891865044832
31,0.12903225421905518,0.12903225421905518,0.5277777777777778,0.7887292619674436,mmlu:professional_accounting,validation,30.74832976423204
282,0.1560283601284027,0.1560283601284027,0.5083556149732621,0.76561967637522,mmlu:professional_accounting,test,265.48744565062225
8,0.0,0.0,,0.9204190298914909,mmlu:college_chemistry,validation,13.396406210958958
100,0.14000000059604645,0.14000000059604645,0.5573089700996678,0.7916894602775574,mmlu:college_chemistry,test,118.51324057509191
11,0.6363636255264282,0.6363636255264282,0.6607142857142857,0.31293522227894177,mmlu:us_foreign_policy,validation,17.178317612037063
100,0.5600000023841858,0.5699999928474426,0.6542207792207793,0.3818866258859634,mmlu:us_foreign_policy,test,124.21475973539054
11,0.3636363744735718,0.3636363744735718,0.39285714285714285,0.5476699417287654,mmlu:machine_learning,validation,26.561314396560192
112,0.2142857313156128,0.196428582072258,0.5149147727272727,0.6161591910890171,mmlu:machine_learning,test,199.65214807540178
16,0.25,0.25,0.32291666666666663,0.6901961527764797,mmlu:college_biology,validation,19.33383198454976
144,0.3402777910232544,0.3402777910232544,0.6525241675617616,0.590186506923702,mmlu:college_biology,test,172.5853075813502
18,0.2777777910232544,0.2777777910232544,0.5076923076923077,0.6396999061107635,mmlu:virology,validation,27.10403359681368
166,0.3734939694404602,0.3734939694404602,0.566532258064516,0.5749907454094254,mmlu:virology,test,318.6166154742241
11,0.5454545617103577,0.5454545617103577,0.7,0.3977438157255,mmlu:computer_security,validation,17.266677908599377
100,0.5199999809265137,0.5199999809265137,0.5136217948717949,0.4442410051822663,mmlu:computer_security,test,142.97484980523586
11,0.4545454680919647,0.4545454680919647,0.6,0.4383356029337103,mmlu:jurisprudence,validation,27.052062936127186
108,0.5092592835426331,0.5092592835426331,0.4365351629502573,0.4028524392180972,mmlu:jurisprudence,test,231.72324074432254
22,0.1818181872367859,0.1818181872367859,0.513888888888889,0.7402642504735426,mmlu:high_school_chemistry,validation,27.3001298494637
203,0.1231527104973793,0.1231527104973793,0.553932584269663,0.7857412857375121,mmlu:high_school_chemistry,test,357.03102984651923
29,0.13793103396892548,0.13793103396892548,0.77,0.779079073461993,mmlu:clinical_knowledge,validation,32.750380735844374
265,0.27547168731689453,0.2792452871799469,0.5474457762557077,0.6368056403016145,mmlu:clinical_knowledge,test,419.62647501192987
27,0.6296296119689941,0.6296296119689941,0.6235294117647059,0.31417942930150916,mmlu:security_studies,validation,59.06269710510969
245,0.6734693646430969,0.6816326379776001,0.6387121212121213,0.23717926935273775,mmlu:security_studies,test,476.54153326526284
23,0.260869562625885,0.260869562625885,0.6225490196078431,0.6836638528367747,mmlu:high_school_statistics,validation,26.813693087548018
216,0.2222222238779068,0.22685185074806213,0.5471850198412698,0.7200067746970389,mmlu:high_school_statistics,test,261.3119318857789
18,0.7222222089767456,0.7222222089767456,0.7076923076923077,0.26096777783499825,mmlu:high_school_european_history,validation,51.81701163388789
165,0.7575757503509521,0.7575757503509521,0.5614,0.22675078384804004,mmlu:high_school_european_history,test,420.56732177361846
19,0.6315789222717285,0.6315789222717285,0.8214285714285714,0.34605679386540467,mmlu:world_religions,validation,20.9132661819458
171,0.5321637392044067,0.5321637392044067,0.5989697802197802,0.4469145247113635,mmlu:world_religions,test,165.58977381512523
60,0.4333333671092987,0.4333333671092987,0.5650452488687783,0.5131688038508098,mmlu:high_school_psychology,validation,76.33402697369456
545,0.4917431175708771,0.4917431175708771,0.5273384880650898,0.4557177159764351,mmlu:high_school_psychology,test,758.5894160456955
22,0.3636363744735718,0.3636363744735718,0.34821428571428564,0.5554950670762496,mmlu:college_medicine,validation,28.426983485929668
173,0.3352600932121277,0.3352600932121277,0.6053223388305847,0.4992190685575408,mmlu:college_medicine,test,223.9411986011546
14,0.4285714626312256,0.4285714626312256,0.5,0.5641699731349945,mmlu:formal_logic,validation,25.648721968755126
126,0.2301587462425232,0.2301587462425232,0.36384642730181305,0.6036285057900443,mmlu:formal_logic,test,157.53977053798735
32,0.3125,0.3125,0.42727272727272725,0.6326046194881201,mmlu:high_school_biology,validation,44.08822372741997
310,0.4193548262119293,0.4258064329624176,0.5363247863247863,0.5035225564433683,mmlu:high_school_biology,test,473.3656308837235
11,0.09090909361839294,0.09090909361839294,0.09999999999999998,0.7665706385265698,mmlu:abstract_algebra,validation,7.763960503041744
100,0.2199999988079071,0.22999998927116394,0.4361888111888112,0.6298125845193863,mmlu:abstract_algebra,test,84.33859298750758
12,0.25,0.25,0.5925925925925926,0.6829658796389898,mmlu:human_sexuality,validation,29.72898682579398
131,0.38167938590049744,0.38167938590049744,0.4664197530864198,0.5526338392541609,mmlu:human_sexuality,test,293.600218065083
33,0.39393940567970276,0.42424243688583374,0.49230769230769234,0.397688040227601,mmlu:nutrition,validation,52.22523456066847
306,0.4215686321258545,0.4346405267715454,0.5562124994525467,0.33380175727644784,mmlu:nutrition,test,468.81488202139735
11,0.27272728085517883,0.3636363744735718,0.45833333333333326,0.599883339621804,mmlu:college_computer_science,validation,18.086286820471287
100,0.17000000178813934,0.1899999976158142,0.47590361445783136,0.6969706654548645,mmlu:college_computer_science,test,121.97947299852967
11,0.0,0.0,,0.9556839466094971,mmlu:college_mathematics,validation,14.899654511362314
100,0.20999999344348907,0.22999998927116394,0.6491862567811935,0.723611016869545,mmlu:college_mathematics,test,120.99596462398767
21,0.523809552192688,0.523809552192688,0.3272727272727272,0.43445798612776254,mmlu:high_school_government_and_politics,validation,52.73901629005559
193,0.5233160257339478,0.5233160257339478,0.6128390012914335,0.4271886462992337,mmlu:high_school_government_and_politics,test,465.0614057779312
100,0.14999999105930328,0.14999999105930328,0.5968627450980392,0.8265437841415405,mmlu:moral_scenarios,validation,116.6419177595526
895,0.13184356689453125,0.13184356689453125,0.5171945553301486,0.8444241796125913,mmlu:moral_scenarios,test,967.226228011772
