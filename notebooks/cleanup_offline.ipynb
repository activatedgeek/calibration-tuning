{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "csv_file = f\"{os.environ.get('DATADIR')}/llm-calibration/llama2-13b_chat-all_20k-offline\"\n",
    "\n",
    "offline = pd.read_csv(f\"{csv_file}/raw/train/0.csv\")\n",
    "offline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "offline.source_dataset.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## arc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "offline[offline.source_dataset == \"arc\"].groupby([\"output\"])[[\"context\"]].count().reset_index().sort_values(\"context\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_answers = [\n",
    "    \"<most likely answer, as short as possible; not a complete sentence, just the answer!>.\",\n",
    "    \"<insert answer here>.\",\n",
    "    \"<your answer here>.\",\n",
    "    \"\\n\\nPlease provide your answer.\",\n",
    "    \"<insert your answer here>.\",\n",
    "]\n",
    "\n",
    "offline = offline.drop(offline[(offline.source_dataset == \"arc\") & (offline.output.isin(filter_answers))].index)\n",
    "\n",
    "offline[offline.source_dataset == \"arc\"].groupby([\"output\"])[[\"context\"]].count().reset_index().sort_values(\"context\", ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## boolq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "offline[offline.source_dataset == \"boolq\"].groupby([\"output\"])[[\"context\"]].count().reset_index().sort_values(\"context\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitbynewline(v):\n",
    "    if not isinstance(v, str):\n",
    "        return v\n",
    "    return v.split('\\n')[0].strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def truestr(v):\n",
    "    if not isinstance(v, str):\n",
    "        return v\n",
    "    \n",
    "    if v == \"\\n\\nTrue\":\n",
    "        return \"True\"\n",
    "    \n",
    "    return v\n",
    "\n",
    "offline.loc[offline.source_dataset == \"boolq\", \"output\"] = offline[offline.source_dataset == \"boolq\"][\"output\"].apply(truestr)\n",
    "\n",
    "offline.loc[offline.source_dataset == \"boolq\", \"output\"] = offline[offline.source_dataset == \"boolq\"][\"output\"].apply(splitbynewline)\n",
    "\n",
    "offline = offline.drop(offline[(offline.source_dataset == \"boolq\") & (offline.output == \"\")].index)\n",
    "\n",
    "offline[offline.source_dataset == \"boolq\"].groupby([\"output\"])[[\"context\"]].count().reset_index().sort_values(\"context\", ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## commonsense_qa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "offline[offline.source_dataset == \"commonsense_qa\"].groupby([\"output\"])[[\"context\"]].count().reset_index().sort_values(\"context\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_answers = [\n",
    "    \"<insert answer here>.\",\n",
    "    \"<most likely answer, as short as possible; not a complete sentence, just the answer!>.\"\n",
    "]\n",
    "\n",
    "offline = offline.drop(offline[(offline.source_dataset == \"commonsense_qa\") & (offline.output.isin(filter_answers))].index)\n",
    "\n",
    "offline[offline.source_dataset == \"commonsense_qa\"].groupby([\"output\"])[[\"context\"]].count().reset_index().sort_values(\"context\", ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## copa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "offline[offline.source_dataset == \"copa\"].groupby([\"output\"])[[\"context\"]].count().reset_index().sort_values(\"context\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onestr(v):\n",
    "    if not isinstance(v, str):\n",
    "        return v\n",
    "    \n",
    "    if v == \"\\n\\n1\":\n",
    "        return \"1\"\n",
    "    \n",
    "    return v\n",
    "\n",
    "offline.loc[offline.source_dataset == \"copa\", \"output\"] = offline[offline.source_dataset == \"copa\"][\"output\"].apply(onestr)\n",
    "\n",
    "offline.loc[offline.source_dataset == \"copa\", \"output\"] = offline[offline.source_dataset == \"copa\"][\"output\"].apply(splitbynewline)\n",
    "\n",
    "offline = offline.drop(offline[(offline.source_dataset == \"copa\") & (offline.output == \"\")].index)\n",
    "\n",
    "offline[offline.source_dataset == \"copa\"].groupby([\"output\"])[[\"context\"]].count().reset_index().sort_values(\"context\", ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cosmos_qa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "offline[offline.source_dataset == \"cosmos_qa\"].groupby([\"output\"])[[\"context\"]].count().reset_index().sort_values(\"context\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_answers = [\n",
    "    \"<most likely answer, as short as possible>.\",\n",
    "    \"<insert answer here>\"\n",
    "]\n",
    "\n",
    "offline = offline.drop(offline[(offline.source_dataset == \"cosmos_qa\") & (offline.output.isin(filter_answers))].index)\n",
    "\n",
    "offline[offline.source_dataset == \"cosmos_qa\"].groupby([\"output\"])[[\"context\"]].count().reset_index().sort_values(\"context\", ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## math_qa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "offline[offline.source_dataset == \"math_qa\"].groupby([\"output\"])[[\"context\"]].count().reset_index().sort_values(\"context\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_answers = [\n",
    "    \"<insert answer here>.\",\n",
    "    \"\\n\\nPlease provide your answer for the above problem.\",\n",
    "    \"<your answer here>.\",\n",
    "]\n",
    "\n",
    "offline = offline.drop(offline[(offline.source_dataset == \"math_qa\") & (offline.output.isin(filter_answers))].index)\n",
    "\n",
    "offline[offline.source_dataset == \"math_qa\"].groupby([\"output\"])[[\"context\"]].count().reset_index().sort_values(\"context\", ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## obqa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "offline[offline.source_dataset == \"obqa\"].groupby([\"output\"])[[\"context\"]].count().reset_index().sort_values(\"context\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_answers = [\n",
    "    \"<insert answer here>.\",\n",
    "    \"<your answer here>.\",\n",
    "    \"<most likely answer, as short as possible; not a complete sentence, just the answer!>.\",\n",
    "    \"<your answer here>\",\n",
    "    \"<insert answer here>\",\n",
    "]\n",
    "\n",
    "offline = offline.drop(offline[(offline.source_dataset == \"obqa\") & (offline.output.isin(filter_answers))].index)\n",
    "\n",
    "offline[offline.source_dataset == \"obqa\"].groupby([\"output\"])[[\"context\"]].count().reset_index().sort_values(\"context\", ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## piqa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "offline[offline.source_dataset == \"piqa\"].groupby([\"output\"])[[\"context\"]].count().reset_index().sort_values(\"context\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "offline.loc[offline.source_dataset == \"piqa\", \"output\"] = offline[offline.source_dataset == \"piqa\"][\"output\"].apply(splitbynewline)\n",
    "\n",
    "offline[offline.source_dataset == \"piqa\"].groupby([\"output\"])[[\"context\"]].count().reset_index().sort_values(\"context\", ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sciq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "offline[offline.source_dataset == \"sciq\"].groupby([\"output\"])[[\"context\"]].count().reset_index().sort_values(\"context\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_answers = [\n",
    "    \"<most likely answer, as short as possible>.\",\n",
    "    \"________________.\",\n",
    "    \"_______________\",\n",
    "    \"\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\",\n",
    "]\n",
    "\n",
    "offline = offline.drop(offline[(offline.source_dataset == \"sciq\") & (offline.output.isin(filter_answers))].index)\n",
    "\n",
    "offline[offline.source_dataset == \"sciq\"].groupby([\"output\"])[[\"context\"]].count().reset_index().sort_values(\"context\", ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## siqa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "offline[offline.source_dataset == \"siqa\"].groupby([\"output\"])[[\"context\"]].count().reset_index().sort_values(\"context\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_answers = [\"<most likely answer, as short as possible>.\"]\n",
    "\n",
    "offline = offline.drop(offline[(offline.source_dataset == \"siqa\") & (offline.output.isin(filter_answers))].index)\n",
    "\n",
    "offline[offline.source_dataset == \"siqa\"].groupby([\"output\"])[[\"context\"]].count().reset_index().sort_values(\"context\", ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## trec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "offline[offline.source_dataset == \"trec\"].groupby([\"output\"])[[\"context\"]].count().reset_index().sort_values(\"context\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "offline.loc[offline.source_dataset == \"trec\", \"output\"] = offline[offline.source_dataset == \"trec\"][\"output\"].apply(splitbynewline)\n",
    "\n",
    "offline[offline.source_dataset == \"trec\"].groupby([\"output\"])[[\"context\"]].count().reset_index().sort_values(\"context\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.makedirs(f\"{os.environ.get('DATADIR')}/llm-calibration/llama2-13b_chat-all_20k-offline/processed/train\")\n",
    "\n",
    "# offline.to_csv(f\"{os.environ.get('DATADIR')}/llm-calibration/llama2-13b_chat-all_20k-offline/processed/train/0.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "offline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = offline.copy()\n",
    "# df[\"output\"] = df[\"output\"].apply(lambda x: str(x).split(\"\\n\")[0])\n",
    "# df[\"output\"] = df[\"output\"].apply(lambda x: str(x).strip(\"\\n\").strip())\n",
    "# df = df[df[\"output\"] != \"\"]\n",
    "# df = df[df[\"output\"] != \"nan\"]\n",
    "\n",
    "# filter_out = [\n",
    "#     \"<most likely answer, as short as possible; not a complete sentence, just the answer!>\",\n",
    "#     \"<most likely answer, as short as possible>\",\n",
    "#     \"<insert answer here>\",\n",
    "#     \"<insert your answer here>\",\n",
    "#     \"<your answer here>\",\n",
    "#     \"<your answer here, as short as possible>\",\n",
    "#     \"<your answer here, as short as possible!>\",\n",
    "#     \"<best answer, as short as possible>\",\n",
    "#     \"_______________\",\n",
    "#     \"Note: I'll give you a hint\",\n",
    "#     \"Note: I'll provide the next question after you answer this one\"\n",
    "#     \"Please provide your answer\",\n",
    "#     \"Which of the following is true?\",\n",
    "# ]\n",
    "\n",
    "# #filter out any outputs that contain any of the above strings\n",
    "# for fo in filter_out:\n",
    "#     df = df[~df[\"output\"].str.contains(fo)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.makedirs(f\"{os.environ.get('DATADIR')}/llm-calibration/llama2-13b_chat-all_20k-offline/processed/train\")\n",
    "\n",
    "# offline.to_csv(f\"{os.environ.get('DATADIR')}/llm-calibration/llama2-13b_chat-all_20k-offline/processed/train/0.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
